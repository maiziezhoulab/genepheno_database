<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="research-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
<journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
<journal-title-group>
<journal-title>Scientific Reports</journal-title>
</journal-title-group>
<issn pub-type="epub">2045-2322</issn>
<publisher>
<publisher-name>Nature Publishing Group UK</publisher-name>
<publisher-loc>London</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">32732917</article-id>
<article-id pub-id-type="pmc">7393364</article-id>
<article-id pub-id-type="publisher-id">68858</article-id>
<article-id pub-id-type="doi">10.1038/s41598-020-68858-7</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Inferring disease subtypes from clusters in explanation space</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Schulz</surname>
<given-names>Marc-Andre</given-names>
</name>
<address>
<email>marc.schulz@rwth-aachen.de</email>
</address>
<xref ref-type="aff" rid="Aff1">1</xref>
<xref ref-type="aff" rid="Aff6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chapman-Rounds</surname>
<given-names>Matt</given-names>
</name>
<address>
<email>m.rounds@ed.ac.uk</email>
</address>
<xref ref-type="aff" rid="Aff2">2</xref>
<xref ref-type="aff" rid="Aff6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Verma</surname>
<given-names>Manisha</given-names>
</name>
<address>
<email>manishav@verizonmedia.com</email>
</address>
<xref ref-type="aff" rid="Aff3">3</xref>
<xref ref-type="aff" rid="Aff6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bzdok</surname>
<given-names>Danilo</given-names>
</name>
<address>
<email>danilo.bzdok@mcgill.ca</email>
</address>
<xref ref-type="aff" rid="Aff4">4</xref>
<xref ref-type="aff" rid="Aff5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Georgatzis</surname>
<given-names>Konstantinos</given-names>
</name>
<address>
<email>konstantinos.georgatzis@quantumblack.com</email>
</address>
<xref ref-type="aff" rid="Aff6">6</xref>
</contrib>
<aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0728 696X</institution-id><institution-id institution-id-type="GRID">grid.1957.a</institution-id><institution>Present Address: Department of Psychiatry, Psychotherapy and Psychosomatics, </institution><institution>RWTH Aachen University, </institution></institution-wrap>Aachen, Germany </aff>
<aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7988</institution-id><institution-id institution-id-type="GRID">grid.4305.2</institution-id><institution>Present Address: School of Informatics, </institution><institution>University of Edinburgh, </institution></institution-wrap>Edinburgh, UK </aff>
<aff id="Aff3"><label>3</label>Present Address: Verizon Media, Boston, MA USA </aff>
<aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8649</institution-id><institution-id institution-id-type="GRID">grid.14709.3b</institution-id><institution>Department of Biomedical Engineering, McConnell Brain Imaging Centre (BIC), Montreal Neurological Institute (MNI), Faculty of Medicine, </institution><institution>McGill University, </institution></institution-wrap>Montreal, Canada </aff>
<aff id="Aff5"><label>5</label>Mila - Quebec Artificial Intelligence Institute, Montreal, Canada </aff>
<aff id="Aff6"><label>6</label>QuantumBlack, London, UK </aff>
</contrib-group>
<pub-date pub-type="epub">
<day>30</day>
<month>7</month>
<year>2020</year>
</pub-date>
<pub-date pub-type="pmc-release">
<day>30</day>
<month>7</month>
<year>2020</year>
</pub-date>
<pub-date pub-type="collection">
<year>2020</year>
</pub-date>
<volume>10</volume>
<elocation-id>12900</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>1</month>
<year>2020</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>6</month>
<year>2020</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2020</copyright-statement>
<license license-type="OpenAccess">
<license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
</license>
</permissions>
<abstract id="Abs1">
<p id="Par1">Identification of disease subtypes and corresponding biomarkers can substantially improve clinical diagnosis and treatment selection. Discovering these subtypes in noisy, high dimensional biomedical data is often impossible for humans and challenging for machines. We introduce a new approach to facilitate the discovery of disease subtypes: Instead of analyzing the original data, we train a diagnostic classifier (healthy vs. diseased) and extract instance-wise explanations for the classifier’s decisions. The distribution of instances in the <italic>explanation space</italic> of our diagnostic classifier amplifies the <italic>different reasons for belonging to the same class</italic>–resulting in a representation that is uniquely useful for discovering latent subtypes. We compare our ability to recover subtypes via cluster analysis on model explanations to classical cluster analysis on the original data. In multiple datasets with known ground-truth subclasses, particularly on UK Biobank brain imaging data and transcriptome data from the Cancer Genome Atlas, we show that cluster analysis on model explanations substantially outperforms the classical approach. While we believe clustering in explanation space to be particularly valuable for inferring disease subtypes, the method is more general and applicable to any kind of sub-type identification.</p>
</abstract>
<kwd-group kwd-group-type="npg-subject">
<title>Subject terms</title>
<kwd>Biomarkers</kwd>
<kwd>Medical research</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta>
<meta-name>issue-copyright-statement</meta-name>
<meta-value>© The Author(s) 2020</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="Sec1">
<title>Introduction</title>
<p id="Par2">Many diseases manifest differently in different humans. This heterogeneity is especially pronounced in fields like psychiatry or oncology where visible symptoms are far removed from the underlying pathomechanism. What appears to be a coherent collection of symptoms is often the expression of a variety of distinct disease subtypes, each with different disease progression and different treatment responses. Dealing with these subtypes is the purview of precision medicine: identified subtypes and their corresponding biomarkers are used to refine diagnoses, predict treatment responses and disease progression, and to inform further scientific research<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
<p id="Par3">The search for biologically grounded subtypes is frequently carried out by means of cluster analysis to identify groups of subjects with similar disease phenotypes. Cluster analysis is applied in either the original set of variables or feature space (e.g. voxels of a brain MRI) or in some hand-crafted aggregate measures inspired by existing external knowledge (e.g. gray matter density of predefined brain regions)<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR5">5</xref></sup>. Identifying clusters is particularly challenging when faced with high dimensional feature spaces, such as MRI or genome data, where the high dimensionality and a generally low signal to noise ratio impede the straightforward application of modern clustering algorithms. Furthermore, similarity in the original feature space (e.g. brain MRI) is not necessarily informative about the investigated disease. That is, the most obvious sources of variation useful for groupings might reflect sex or age instead of similar disease phenotypes. Thus, biomedical data in particular often needs to be transformed from the original feature space into a more informative embedding space<sup><xref ref-type="bibr" rid="CR7">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref></sup>. In this paper, we propose a novel space that we believe to be particularly useful for identifying latent subtypes: the space of explanations corresponding to a diagnostic classifier.
</p>
<p id="Par4">Recent interest in explaining the output of complex machine learning models has been characterized by a wide range of approaches<sup><xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref></sup>, most of them focused on providing an instance-wise explanation of a model’s output as either a subset of input features<sup><xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref></sup>, or a weighting of input features<sup><xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR13">13</xref></sup>. The latter, where each input feature is weighted according to its contribution to the underlying model’s output for an instance, can be thought of as specifying a transformation from <italic>feature space</italic> to an <italic>explanation space</italic>. This explanation space is conditioned on the underlying model’s output. Simply put, every sample that can be classified by a model has a corresponding explanation. This explanation consists of the contributions of each individual input feature to the classification result. The explanation thus has the same dimensionality as the original feature space. This space of feature contributions (the explanation space) can be thought of as a new feature space in its own right and can itself serve as a basis for classification. In contrast to the original feature space, the new explanation space directly relates to the classification goal. In the case of a diagnostic classifier (healthy vs. diseased), the explanation space relates to the investigated disease.</p>
<p id="Par5">We argue that the explanation space of a diagnostic classifier is an appropriate embedding space for subsequent cluster analyses aimed at the discovery of latent disease subtypes. Firstly, the explanations should collapse features that are irrelevant to the classification of a particular disease, thereby mitigating the curse of dimensionality. Secondly, we expect different disease subtypes to have structurally different explanations for belonging to a disease class. The explanation space of a diagnostic classifier is expected to amplify the <italic>different reasons for belonging to the same class</italic> - resulting in a representation that should be uniquely useful for recovering latent subtypes. The intuition here is that instance-wise explanations refer to a <italic>local</italic> part of the classifier’s decision boundary. This means that if the decision boundary is differently oriented for different parts of the space (due, for example, to multiple distinct underlying subclasses), the explanations will also differ meaningfully.</p>
<p id="Par6">For a proof of principle, we take the approach of converting multi-class classification problems into binary ones for the purposes of training the underlying model. This means that each ‘class’ has several distinct subclasses, of which the classifier is unaware - while we retain ground-truth knowledge of the respective subclasses. In four revisited datasets, we demonstrate that the clusters in explanation space recover these known subclasses.</p>
</sec>
<sec id="Sec2">
<title>Related work</title>
<p id="Par7">Examples of clustering work in the original feature space include Carey et al.<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, who use a hierarchical clustering approach to population-based distributions and clinical associations for breast-cancer subtypes, and Erro et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, who use a k-means based clustering approach to test the hypothesis that the variability in the clinical phenotype of Parkinson’s disease was caused by the existence of multiple distinct subtypes of the disease. Drysdale et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> show that patients with depression can be subdivided into four neurophysiological subtypes, by hierarchical clustering on a learned embedding space.</p>
<p id="Par8">Using instance-wise explanations for clustering has previously been discussed in Lundberg et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, under the name “supervised clustering”. These authors show that clustering on Shapley values explains more model variance than other tree-specific feature importance estimates<sup><xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref></sup>. To the best of our knowledge, no previous work has shown if, and to what extent, clusters in explanations space can be of practical relevance, nor has any work drawn the link to disease-subtyping in precision medicine. The present work bridges the gap between recent advances in <italic>explainable AI</italic> and real-world medical applications, by linking the former to a crucial biomedical problem and providing not only a proof-of-concept, but a full, clinically relevant, example - the discovery of cancer-subtypes.</p>
</sec>
<sec id="Sec3">
<title>Data</title>
<p id="Par9">We chose four datasets of increasing complexity to evaluate the efficacy of subclass recovery in explanation space. Firstly, simple synthetic data for a proof-of-concept. Secondly, Fashion-MNIST as a machine learning benchmark. Thirdly, age prediction from brain imaging data as a simple biomedical example. Lastly, cancer subtype detection as a challenging real-world biomedical problem. Brain imaging data was provided by the UK Biobank, cancer transcriptome data from the Cancer Genome Atlas. The two are among the world’s largest biomedical data collections and represent the two most likely fields of applications: precision psychiatry and oncology, with <italic>big data</italic> in both <italic>p</italic> (dimensionality) and <italic>n</italic> (sample size).</p>
<p id="Par10">The original Madelon dataset from the 2003 NIPS feature selection challenge<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> is a synthetic binary classification problem, with data points placed in clusters on the vertices of a hypercube in a subspace of the feature dimension. The scikit-learn implementation<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> of the generative algorithm was used to create a variation with 16 classes, 50 features, and data points distributed in one cluster per class on the vertices of a four-dimensional hypercube.</p>
<p id="Par11">Zalando provides Fashion-MNIST<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> as a more challenging drop-in replacement for MNIST, with the same dimensionality (782) and sample size (70,000). Instead of the 10 classes of digits (0-9) of the original MNIST dataset, Fashion-MNIST consists of grayscale images of ten classes of clothing (e.g. T-shirts, pants, dresses). We chose Fashion-MNIST, because the original MNIST has been argued to be too easy a problem for modern methods. Indeed, most classes in the dataset can already be separated in the first two principal components of the feature space, making it ill-suited for the present analysis.</p>
<p id="Par12">The UK Biobank (UKBB)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> is one of the world’s largest biomedical data collections. It provides, amongst a multitude of other phenotypes, structural (T1) brain MRI data for 10000 participants. The structural brain MRI images were preprocessed into 164 biologically motivated imaging-derived phenotypes (IDPs), prepresentin aggregated grey and white matter densities per brain region. For a biomedical proof-of-concept, we chose age as simple target variable - cut into 4 quartile “classes”.</p>
<p id="Par13">The Cancer Genome Atlas (TCGA) project<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> provides transcriptome (The transcriptome consists of all “transcripts”, i.e. copies, of DNA into RNA, that are necessary to implement DNA instructions.) data for various forms and subtypes of cancer. The data consists of 60,498 gene expression (fpkm) values for 8500 participants (after removing participants with missing values). We chose the cancer tissue’s immune model based subtype (6 classes, i.e. Wound Healing, IFN-gamma Dominant, Inflammatory, Lymphocyte Depleted, Immunologically Quiet, TGF-beta Dominant) as a complex, clinically relevant target variable.</p>
<p id="Par15">Our synthetic and Fashion-MNIST datasets have balanced classes by design. The UK Biobank brain imaging data is cut into 4 age-quartiles resulting in balanced classes. The TCGA dataset is imbalanced. For classification it was sampled inversely proportional to class frequencies.</p>
</sec>
<sec id="Sec4">
<title>Methods</title>
<p id="Par16">For each dataset, classes were split into two supersets, one containing the first three (The first and fourth class for UKBB.) classes, the other containing the remainder (The decision to split into three-vs-rest was due to ease of visualization, and simplicity of choice.). This yielded a new binary classification problem. This setup allows for evaluating approaches to subtype discovery. The binary classification data is intended to represent observable characteristics (e.g. the healthy vs. diseased distinction in medicine) while the original classes represent the hidden subtypes.</p>
<p id="Par19">A Random Forest classifier<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> was trained on the binary classification problem and SHapley Additive Explanations (SHAP)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> were used to generate instance-wise explanations for the predictions of the classifier. Approaches to model agnostic explanations tend to be highly computationally expensive, as they typically use extensive sampling procedures to estimate local approximations of decision boundaries or to find proximate counterfactual instances. The specific combination of classification model and explanation approach motivated by the fact that there exists a particularly efficient solution to the computation of Shapley values in the case of Random Forests<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. The concept of Shapley values [21] has its origins in cooperative game theory. Shapley values indicate how to distribute payoffs of a cooperative game proportional to each player’s individual contribution. SHAP interprets prediction as a cooperative game and assigns each input variable or feature its marginal contribution to the predicted outcome. Investigating the applicability of other explanation methods and classifiers to specific domains would be a natural direction for future work.</p>
<p id="Par20">Each explanation is represented as a vector of the same dimensionality as the original feature space, indicating how strongly and in what direction each feature contributed to the prediction result of a given observation. The space spanned by the model explanations was then compared to the original feature space, with respect to the distinguishability of ground-truth subtypes (the original classes of the dataset).</p>
</sec>
<sec id="Sec5">
<title>Results</title>
<p id="Par21">To compare the correspondence of clustering in the respective spaces to the underlying distribution of class labels we used three qualitatively distinct approaches. Firstly, we visually inspected the projection into the first two principal components. Distinct clusters in the data would constitute major directions of variance and should be easily visible in the PCA projection, allowing for a first sanity check (Fig. <xref ref-type="fig" rid="Fig1">1</xref>). Secondly, we evaluated standard clustering quality indices to quantify structural differences between representations. We chose the Davies-Bouldin index, defined as the average similarity between each cluster <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_i$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2020_68858_Article_IEq1.gif"></inline-graphic></alternatives></inline-formula> and its most similar other cluster <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_j$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2020_68858_Article_IEq2.gif"></inline-graphic></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, the Silhouette Coefficient, which balances the mean distance between a sample and all other points in its cluster with the mean distance between that sample and all the points in the next nearest cluster<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, and the Calinski-Harabaz Index, which is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Lastly, to ensure that transformation from feature space into explanation space really does lead to improved subclass recovery, we applied Agglomerative Clustering and report the Adjusted Mutual Information<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> between reconstructed subclasses with ground-truth labels.</p>
<p id="Par22">To compare improvements derived from transformation into explanation space with those which can be gained by dimensionality reduction alone, we apply (All run with scikit-learn default parameters.) PCA<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, Isomap<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, and T-SNE<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> to both feature space and explanation space, reducing their dimensionality down to two latent factors. These dimensionality reduction methods were selected to represent a diversity of approaches. PCA is strictly linear, while Isomap and T-SNE allow for non-linear interactions. In contrast to Isomap and T-SME, PCA imposes orthogonality on latent factors. While PCA focusses on closeness in input-space, Isomap and T-SNE optimize closeness in embedding space. We report clustering quality indices and Mutual Information for both feature space and explanation space in the original dimensionality as well as the reduced PCA, Isomap, and T-SNE spaces.<fig id="Fig1"><label>Figure 1</label><caption><p>Projection into first two principal components after PCA of feature space (top row) and explanation space (bottom row) of Synthetic, Fashion-MNIST, UK Biobank Brain-Age, and Cancer Genome Atlas datasets. Shown are the (sub-)classes of the three-class superset used for binary classification. As can be seen, clusters are well defined in explanation space; for biomedical datasets, these sub classes correspond to real subtypes. For example in TCGA (rightmost plot), the visible clusters correspond to Wound Healing (black), IFN-gamma Dominant (yellow), and Inflammatory (pink) cancer tissue immune model types.</p></caption><graphic id="MO1" xlink:href="41598_2020_68858_Fig1_HTML"></graphic></fig>
</p>
<p id="Par24">Quantitative results are shown in Table <xref ref-type="table" rid="Tab1">1</xref>. Clustering quality indices and post-clustering Mutual Information consistently improved (average AMI gain of 0.45) when moving from feature space to explanation space in all four datasets, both before and after dimensionality reduction. Dimensionality reduction improved subclass recovery in both feature space and explanation space, with PCA performing worst, T-SNE performing best, and Isomap somewhat inconsistently in-between. Notably, improvement derived from dimensionality reduction was larger when reducing explanation space than when reducing feature space, with an average AMI gain from T-SNE of 0.06 in feature space and 0.13 in explanation space.</p>
</sec>
<sec id="Sec6">
<title>Conclusion</title>
<p id="Par25">We propose <italic>explanation space</italic> as a powerful embedding to facilitate detection of latent disease subtypes, particularly by cluster analysis. In four revisited datasets, we have shown both that the distribution of data points in explanation space is sharply clustered, and that these clusters more accurately correspond to ground-truth subclasses than clusters derived from the original feature space.</p>
<p id="Par26">We also demonstrated the relevance of our approach to the real world problem of disease subtype discovery. In both the Cancer Genome Atlas and the UK Biobank brain imaging data, task-conditioned explanations proved to be highly informative - to the extent that subtypes can be easily visually identified (Fig. <xref ref-type="fig" rid="Fig1">1</xref>).</p>
<p id="Par27">Existing approaches to subtype discovery<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup> can easily be adapted to work on task conditioned explanations. Disease status labels are naturally available and can be used to train a diagnostic classifier. The classifier’s instance-wise explanations have the same dimensionality as the original data and can serve as a drop-in replacement.</p>
<p id="Par28">Transforming from feature space to explanation space should not be seen in competition to dimensionality reduction, but rather as a complimentary processing step. Both help to recover latent subtypes and the benefits from dimensionality reduction appear to be substantially amplified by first transforming into an appropriate explanation space.</p>
<p id="Par29">We hope that this work will serve as a starting point for further exploration of explanation-based methods for inferring disease subtypes. More broadly, the fact that off-the-shelf explanatory tools can be used to generate task-specific embeddings is undoubtedly a promising avenue for a variety of applications.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Cluster tightness and separation (Davies-Bouldin, Calinski-Harabaz, Silhouette), as well as subclass recovery performance (Adjusted Mutual Information after hierarchical clustering), in explanation space (e) and feature space (f), in original dimensionality and post PCA, Isomap, and T-SNE. Higher values are better (except for Davies-Bouldin, where lower is better).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">Dim. Red.</th><th align="left">Space</th><th align="left">Davies-Bouldin</th><th align="left">Calinski-Harabaz</th><th align="left">Silhouette</th><th align="left">Adjusted MI</th></tr></thead><tbody><tr><td align="left" rowspan="8">Sythetic</td><td align="left" rowspan="2">None</td><td align="left">(f)</td><td align="left">7.00</td><td align="left">6.52</td><td align="left">0.02</td><td align="left">0.08</td></tr><tr><td align="left">(e)</td><td align="left">1.11</td><td align="left">192.62</td><td align="left">0.46</td><td align="left">0.59</td></tr><tr><td align="left" rowspan="2">PCA</td><td align="left">(f)</td><td align="left">3.30</td><td align="left">38.08</td><td align="left">0.01</td><td align="left">0.14</td></tr><tr><td align="left">(e)</td><td align="left">0.78</td><td align="left">258.24</td><td align="left">0.66</td><td align="left">0.59</td></tr><tr><td align="left" rowspan="2">Isomap</td><td align="left">(f)</td><td align="left">12.90</td><td align="left">3.93</td><td align="left">-0.02</td><td align="left">0.02</td></tr><tr><td align="left">(e)</td><td align="left">0.72</td><td align="left">187.73</td><td align="left">0.71</td><td align="left">0.52</td></tr><tr><td align="left" rowspan="2">T-SNE</td><td align="left">(f)</td><td align="left">29.45</td><td align="left">0.61</td><td align="left">-0.02</td><td align="left">0.00</td></tr><tr><td align="left">(e)</td><td align="left">0.29</td><td align="left">2937.57</td><td align="left">0.79</td><td align="left">0.97</td></tr><tr><td align="left" rowspan="8">Fashion</td><td align="left" rowspan="2">None</td><td align="left">(f)</td><td align="left">11.10</td><td align="left">2.86</td><td align="left">-0.03</td><td align="left">0.00</td></tr><tr><td align="left">(e)</td><td align="left">1.32</td><td align="left">205.45</td><td align="left">0.30</td><td align="left">0.61</td></tr><tr><td align="left" rowspan="2">PCA</td><td align="left">(f)</td><td align="left">5.72</td><td align="left">2.18</td><td align="left">-0.12</td><td align="left">0.00</td></tr><tr><td align="left">(e)</td><td align="left">0.74</td><td align="left">581.87</td><td align="left">0.51</td><td align="left">0.56</td></tr><tr><td align="left" rowspan="2">Isomap</td><td align="left">(f)</td><td align="left">4.87</td><td align="left">75.60</td><td align="left">-0.07</td><td align="left">0.00</td></tr><tr><td align="left">(e)</td><td align="left">0.92</td><td align="left">398.96</td><td align="left">0.41</td><td align="left">0.56</td></tr><tr><td align="left" rowspan="2">T-SNE</td><td align="left">(f)</td><td align="left">2.30</td><td align="left">65.03</td><td align="left">0.12</td><td align="left">0.13</td></tr><tr><td align="left">(e)</td><td align="left">0.69</td><td align="left">768.13</td><td align="left">0.52</td><td align="left">0.71</td></tr><tr><td align="left" rowspan="8">UKBB</td><td align="left" rowspan="2">None</td><td align="left">(f)</td><td align="left">4.24</td><td align="left">26.38</td><td align="left">0.04</td><td align="left">0.04</td></tr><tr><td align="left">(e)</td><td align="left">2.33</td><td align="left">84.91</td><td align="left">0.12</td><td align="left">0.15</td></tr><tr><td align="left" rowspan="2">PCA</td><td align="left">(f)</td><td align="left">2.53</td><td align="left">57.65</td><td align="left">0.07</td><td align="left">0.05</td></tr><tr><td align="left">(e)</td><td align="left">1.54</td><td align="left">152.25</td><td align="left">0.21</td><td align="left">0.14</td></tr><tr><td align="left" rowspan="2">Isomap</td><td align="left">(f)</td><td align="left">2.60</td><td align="left">60.75</td><td align="left">0.08</td><td align="left">0.04</td></tr><tr><td align="left">(e)</td><td align="left">1.74</td><td align="left">122.90</td><td align="left">0.21</td><td align="left">0.14</td></tr><tr><td align="left" rowspan="2">T-SNE</td><td align="left">(f)</td><td align="left">2.64</td><td align="left">59.99</td><td align="left">0.08</td><td align="left">0.05</td></tr><tr><td align="left">(e)</td><td align="left">1.44</td><td align="left">180.59</td><td align="left">0.24</td><td align="left">0.12</td></tr><tr><td align="left" rowspan="8">TCGA</td><td align="left" rowspan="2">None</td><td align="left">(f)</td><td align="left">8.58</td><td align="left">8.54</td><td align="left">0.01</td><td align="left">0.10</td></tr><tr><td align="left">(e)</td><td align="left">2.64</td><td align="left">57.37</td><td align="left">0.13</td><td align="left">0.68</td></tr><tr><td align="left" rowspan="2">PCA</td><td align="left">(f)</td><td align="left">4.44</td><td align="left">17.81</td><td align="left">-0.01</td><td align="left">0.05</td></tr><tr><td align="left">(e)</td><td align="left">0.59</td><td align="left">929.48</td><td align="left">0.59</td><td align="left">0.67</td></tr><tr><td align="left" rowspan="2">Isomap</td><td align="left">(f)</td><td align="left">8.27</td><td align="left">70.70</td><td align="left">0.02</td><td align="left">0.16</td></tr><tr><td align="left">(e)</td><td align="left">0.51</td><td align="left">834.70</td><td align="left">0.62</td><td align="left">0.70</td></tr><tr><td align="left" rowspan="2">T-SNE</td><td align="left">(f)</td><td align="left">5.03</td><td align="left">106.36</td><td align="left">0.11</td><td align="left">0.28</td></tr><tr><td align="left">(e)</td><td align="left">0.51</td><td align="left">1561.08</td><td align="left">0.60</td><td align="left">0.73</td></tr></tbody></table></table-wrap>
</p>
</sec>
</body>
<back>
<fn-group>
<fn>
<p>
<bold>Publisher's note</bold>
</p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
</fn>
</fn-group>
<notes notes-type="author-contribution">
<title>Author contributions</title>
<p>M.A.S., M.C.R., and M.V. performed the experiments. M.A.S., M.C.R., M.V., D.B., and K.G. wrote the manuscript.</p>
</notes>
<notes id="FPar1" notes-type="COI-statement">
<title>Competing interests</title>
<p id="Par38">The authors declare no competing interests.</p>
</notes>
<ref-list id="Bib1">
<title>References</title>
<ref id="CR1">
<label>1.</label>
<mixed-citation publication-type="other">Bzdok, Danilo, &amp; Meyer-Lindenberg, Andreas. Machine learning for precision psychiatry: Opportunites and challenges. <italic>Biological Psychiatry: Cognitive Neuroscience and Neuroimaging</italic>. (2017)</mixed-citation>
</ref>
<ref id="CR2">
<label>2.</label>
<mixed-citation publication-type="other">Carey, L.A, Perou, C.M, Livasy, C.A, Dressler, L.G, Cowan, D., Conway, K., Karaca, G., Troester, M.A, Tse, C.K., Edmiston, S., et al. Race, breast cancer subtypes, and survival in the carolina breast cancer study. <italic>JAMA</italic><bold>295</bold>(21), 2492–2502 (2006).</mixed-citation>
</ref>
<ref id="CR3">
<label>3.</label>
<mixed-citation publication-type="other">Erro, R., Vitale, C., Amboni, M., Picillo, M., Moccia, M., Longo, K., Santangelo, G., De Rosa, A., Roberto A., Flavio G., et al. The heterogeneity of early parkinson’s disease: a cluster analysis on newly diagnosed untreated patients. <italic>PloS One</italic><bold>8</bold>(8), e70244 (2013).</mixed-citation>
</ref>
<ref id="CR4">
<label>4.</label>
<mixed-citation publication-type="other">Drysdale, A. T, Grosenick, L., Downar, J., Dunlop, K., Mansouri, F., Meng, Y., Fetcho, R.N, Zebley, B., Oathes, D.J, Etkin, A. et al. Resting-state connectivity biomarkers define neurophysiological subtypes of depression. <italic>Nat. Med.</italic>, <bold>23</bold>(1), 28 (2017).</mixed-citation>
</ref>
<ref id="CR5">
<label>5.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mottron</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Bzdok</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>Autism spectrum heterogeneity: fact or artifact?</article-title>
<source/>Mol. Psychiatry.
          <year>2020</year>
<pub-id pub-id-type="doi">10.1038/s41380-020-0748-y</pub-id>
<pub-id pub-id-type="pmid">32355335</pub-id>
</element-citation>
</ref>
<ref id="CR6">
<label>6.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bzdok</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Nichols</surname>
<given-names>TE</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>SM</given-names>
</name>
</person-group>
<article-title>Towards algorithmic analytics for large-scale datasets</article-title>
<source/>Nat. Mach. Intell.
          <year>2019</year>
<volume>1</volume>
<fpage>296</fpage>
<lpage>306</lpage>
<pub-id pub-id-type="doi">10.1038/s42256-019-0069-5</pub-id>
<pub-id pub-id-type="pmid">31701088</pub-id>
</element-citation>
</ref>
<ref id="CR7">
<label>7.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Hastie</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Tibshirani</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Friedman</surname>
<given-names>J</given-names>
</name>
</person-group>
<source/>The elements of statistical learning
          <year>2001</year>
<publisher-loc>Heidelberg, Germany</publisher-loc>
<publisher-name>Springer</publisher-name>
</element-citation>
</ref>
<ref id="CR8">
<label>8.</label>
<mixed-citation publication-type="other">Lipton, Z.C. The mythos of model interpretability. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1606.03490">arXiv:1606.03490</ext-link>, (2016).</mixed-citation>
</ref>
<ref id="CR9">
<label>9.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Montavon</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Samek</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Müller</surname>
<given-names>K-R</given-names>
</name>
</person-group>
<article-title>Methods for interpreting and understanding deep neural networks</article-title>
<source/>Digit. Signal Proc.
          <year>2018</year>
<volume>73</volume>
<fpage>1</fpage>
<lpage>15</lpage>
<pub-id pub-id-type="doi">10.1016/j.dsp.2017.10.011</pub-id>
</element-citation>
</ref>
<ref id="CR10">
<label>10.</label>
<mixed-citation publication-type="other">Ribeiro, M.T., Singh, S., &amp; Guestrin, C. Anchors: High-precision model-agnostic explanations. In <italic>AAAI Conference on Artificial Intelligence</italic> (2018).</mixed-citation>
</ref>
<ref id="CR11">
<label>11.</label>
<mixed-citation publication-type="other">Chen, J., Song, L., Wainwright, M.J, &amp; Jordan, M.I. Learning to explain: an information-theoretic perspective on model interpretation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.07814">arXiv:1802.07814</ext-link> (2018).</mixed-citation>
</ref>
<ref id="CR12">
<label>12.</label>
<mixed-citation publication-type="other">Ribeiro, M. T., Singh, S., &amp; Guestrin, C. Why should i trust you? Explaining the predictions of any classifier. In <italic>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, pp. 1135–1144. ACM (2016).</mixed-citation>
</ref>
<ref id="CR13">
<label>13.</label>
<mixed-citation publication-type="other">Lundberg, S.M, &amp; Lee, S.-I. A unified approach to interpreting model predictions. In <italic>Advances in Neural Information Processing Systems</italic>, pp. 4765–4774 (2017).</mixed-citation>
</ref>
<ref id="CR14">
<label>14.</label>
<mixed-citation publication-type="other">Lundberg, S.M, Erion, G. G, &amp; Lee, S.-I. Consistent individualized feature attribution for tree ensembles. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.03888">arXiv:1802.03888</ext-link> (2018).</mixed-citation>
</ref>
<ref id="CR15">
<label>15.</label>
<mixed-citation publication-type="other">Saabas, A. Interpreting random forests. <ext-link ext-link-type="uri" xlink:href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</ext-link> (2014). Accessed 24/10/2018.</mixed-citation>
</ref>
<ref id="CR16">
<label>16.</label>
<mixed-citation publication-type="other">Guyon, I., Gunn, S., Ben-Hur, A., &amp; Dror, G. Result analysis of the nips 2003 feature selection challenge. In <italic>Advances in Neural Information Processing Systems</italic>, pp. 545–552 (2005).</mixed-citation>
</ref>
<ref id="CR17">
<label>17.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pedregosa</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Varoquaux</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Gramfort</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Michel</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Thirion</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Grisel</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Blondel</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Prettenhofer</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Weiss</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Dubourg</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Vanderplas</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Passos</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Cournapeau</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Brucher</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Perrot</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Duchesnay</surname>
<given-names>E</given-names>
</name>
<etal></etal>
</person-group>
<article-title>Scikit-learn: machine learning in Python</article-title>
<source/>J. Mach. Learn. Res.
          <year>2011</year>
<volume>12</volume>
<fpage>2825</fpage>
<lpage>2830</lpage>
</element-citation>
</ref>
<ref id="CR18">
<label>18.</label>
<mixed-citation publication-type="other">Xiao, H., Rasul, K., &amp; Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1708.07747">arXiv:1708.07747</ext-link>, (2017).</mixed-citation>
</ref>
<ref id="CR19">
<label>19.</label>
<mixed-citation publication-type="other">Miller, K.L, Alfaro-Almagro, F., Bangerter, N.K, Thomas, D.L, Yacoub, E., Xu, J., Bartsch, A.J, Jbabdi, S., Sotiropoulos, S.N, Andersson, J.L.R et al. Multimodal population brain imaging in the uk biobank prospective epidemiological study. <italic>Nature Neurosci.</italic>, <bold>19</bold>(11), 1523 (2016).</mixed-citation>
</ref>
<ref id="CR20">
<label>20.</label>
<mixed-citation publication-type="other">Weinstein, J. N, Collisson, E.A, Mills, G.B, Shaw, K.R., Mills, O., Brad, A., Ellrott, K., Shmulevich, I., Sander, C., Stuart, J.M. Cancer genome atlas research network, et al. The cancer genome atlas pan-cancer analysis project. <italic>Nature Genetics</italic>, <bold>45</bold>(10), 1113 (2013).</mixed-citation>
</ref>
<ref id="CR21">
<label>21.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Breiman</surname>
<given-names>L</given-names>
</name>
</person-group>
<article-title>Random forests</article-title>
<source/>Mach. Learn.
          <year>2001</year>
<volume>45</volume>
<issue>1</issue>
<fpage>5</fpage>
<lpage>32</lpage>
<pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
</element-citation>
</ref>
<ref id="CR22">
<label>22.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Davies</surname>
<given-names>DL</given-names>
</name>
<name>
<surname>Bouldin</surname>
<given-names>DW</given-names>
</name>
</person-group>
<article-title>A cluster separation measure</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>1979</year>
<volume>2</volume>
<fpage>224</fpage>
<lpage>227</lpage>
<pub-id pub-id-type="doi">10.1109/TPAMI.1979.4766909</pub-id>
</element-citation>
</ref>
<ref id="CR23">
<label>23.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rousseeuw</surname>
<given-names>PJ</given-names>
</name>
</person-group>
<article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>
<source/>J. Comput. Appl. Math.
          <year>1987</year>
<volume>20</volume>
<fpage>53</fpage>
<lpage>65</lpage>
<pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id>
</element-citation>
</ref>
<ref id="CR24">
<label>24.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Caliński</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Harabasz</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>A dendrite method for cluster analysis</article-title>
<source/>Commun. Stat. Theory Methods
          <year>1974</year>
<volume>3</volume>
<issue>1</issue>
<fpage>1</fpage>
<lpage>27</lpage>
<pub-id pub-id-type="doi">10.1080/03610927408827101</pub-id>
</element-citation>
</ref>
<ref id="CR25">
<label>25.</label>
<mixed-citation publication-type="other">Vinh, N. X., Epps, J., &amp; Bailey, J. Information theoretic measures for clusterings comparison: variants, properties, normalization and correction for chance. <italic>J. Mach. Learn. Res.</italic>, <bold>11</bold>(Oct), 2837–2854 (2010).</mixed-citation>
</ref>
<ref id="CR26">
<label>26.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tenenbaum</surname>
<given-names>JB</given-names>
</name>
<name>
<surname>De Silva</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Langford</surname>
<given-names>JC</given-names>
</name>
</person-group>
<article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>
<source/>Science
          <year>2000</year>
<volume>290</volume>
<issue>5500</issue>
<fpage>2319</fpage>
<lpage>2323</lpage>
<pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id>
<pub-id pub-id-type="pmid">11125149</pub-id>
</element-citation>
</ref>
<ref id="CR27">
<label>27.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>van der Maaten</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Hinton</surname>
<given-names>G</given-names>
</name>
</person-group>
<article-title>Visualizing data using t-sne</article-title>
<source/>J. Mach. Learn. Res.
          <year>2008</year>
<volume>9</volume>
<issue>Nov</issue>
<fpage>2579</fpage>
<lpage>2605</lpage>
</element-citation>
</ref>
</ref-list>
</back>
</article>
</pmc-articleset>