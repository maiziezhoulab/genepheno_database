<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="review-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Front Hum Neurosci</journal-id>
<journal-id journal-id-type="iso-abbrev">Front Hum Neurosci</journal-id>
<journal-id journal-id-type="publisher-id">Front. Hum. Neurosci.</journal-id>
<journal-title-group>
<journal-title>Frontiers in Human Neuroscience</journal-title>
</journal-title-group>
<issn pub-type="epub">1662-5161</issn>
<publisher>
<publisher-name>Frontiers Media S.A.</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">28824397</article-id>
<article-id pub-id-type="pmc">5540883</article-id>
<article-id pub-id-type="doi">10.3389/fnhum.2017.00390</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
<subj-group>
<subject>Review</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Szucs</surname>
<given-names>Denes</given-names>
</name>
<xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref>
<xref ref-type="author-notes" rid="fn001">
<sup>*</sup>
</xref>
<uri xlink:href="http://loop.frontiersin.org/people/39702/overview" xlink:type="simple"></uri>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ioannidis</surname>
<given-names>John P. A.</given-names>
</name>
<xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref>
</contrib>
</contrib-group>
<aff id="aff1"><sup>1</sup><institution>Department of Psychology, University of Cambridge</institution>
<country>Cambridge, United Kingdom</country></aff>
<aff id="aff2"><sup>2</sup><institution>Meta-Research Innovation Center at Stanford and Department of Medicine, Department of Health Research and Policy, and Department of Statistics, Stanford University</institution>
<country>Stanford, CA, United States</country></aff>
<author-notes>
<fn fn-type="edited-by">
<p>Edited by: Satrajit S. Ghosh, Massachusetts Institute of Technology, United States</p>
</fn>
<fn fn-type="edited-by">
<p>Reviewed by: Bertrand Thirion, Institut National de Recherche en Informatique et en Automatique (INRIA), France; Cyril R. Pernet, University of Edinburgh, United Kingdom</p>
</fn>
<corresp id="fn001">*Correspondence: Denes Szucs <email xlink:type="simple">ds377@cam.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>03</day>
<month>8</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<volume>11</volume>
<elocation-id>390</elocation-id>
<history>
<date date-type="received">
<day>03</day>
<month>2</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>7</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>Copyright © 2017 Szucs and Ioannidis.</copyright-statement>
<copyright-year>2017</copyright-year>
<copyright-holder>Szucs and Ioannidis</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/">
<license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
</license>
</permissions>
<abstract>
<p>Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology, and biomedical science in general. We review these shortcomings and suggest that, after sustained negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out.</p>
</abstract>
<kwd-group>
<kwd>replication crisis</kwd>
<kwd>false positive findings</kwd>
<kwd>research methodology</kwd>
<kwd>null hypothesis significance testing</kwd>
<kwd>Bayesian methods</kwd>
</kwd-group>
<funding-group>
<award-group>
<funding-source id="cn001">James S. McDonnell Foundation<named-content content-type="fundref-id">10.13039/100000913</named-content></funding-source>
<award-id rid="cn001">220020370</award-id>
</award-group>
</funding-group>
<counts>
<fig-count count="4"></fig-count>
<table-count count="3"></table-count>
<equation-count count="3"></equation-count>
<ref-count count="163"></ref-count>
<page-count count="21"></page-count>
<word-count count="18840"></word-count>
</counts>
</article-meta>
</front>
<body>
<p><disp-quote><p>“What used to be called judgment is now called prejudice and what used to be called prejudice is now called a null hypothesis. In the social sciences, particularly, it is dangerous nonsense (dressed up as the “scientific method”) and will cause much trouble before it is widely appreciated as such.”</p><attrib>(Edwards, <xref ref-type="bibr" rid="B36">1972</xref>; p.180.)</attrib></disp-quote>
<disp-quote><p>“…the mathematical rules of probability theory are not merely rules for calculating frequencies of random variables; they are also the unique consistent rules for conducting inference (i.e., plausible reasoning)”</p><attrib>(Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; p. xxii).</attrib></disp-quote></p>
<sec id="s1">
<title>The replication crisis and null hypothesis significance testing (NHST)</title>
<p>There is increasing discontent that many areas of psychological science, cognitive neuroscience, and biomedical research (Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>; Ioannidis et al., <xref ref-type="bibr" rid="B77">2014</xref>) are in a crisis of producing too many false positive non-replicable results (Begley and Ellis, <xref ref-type="bibr" rid="B5">2012</xref>; Aarts et al., <xref ref-type="bibr" rid="B1">2015</xref>). This wastes research funding, erodes credibility and slows down scientific progress. Since more than half a century many methodologists have claimed repeatedly that this crisis may at least in part be related to problems with Null Hypothesis Significance Testing (NHST; Rozeboom, <xref ref-type="bibr" rid="B131">1960</xref>; Bakan, <xref ref-type="bibr" rid="B2">1966</xref>; Meehl, <xref ref-type="bibr" rid="B102">1978</xref>; Gigerenzer, <xref ref-type="bibr" rid="B53">1998</xref>; Nickerson, <xref ref-type="bibr" rid="B114">2000</xref>). However, most scientists (and in particular psychologists, biomedical scientists, social scientists, cognitive scientists, and neuroscientists) are still near exclusively educated in NHST, they tend to misunderstand and abuse NHST and the method is near fully dominant in scientific papers (Chavalarias et al., <xref ref-type="bibr" rid="B23">1990-2015</xref>). Here we provide an accessible critical reassessment of NHST and suggest that while it may have legitimate uses when there are precise quantitative predictions and/or as a heuristic, it should be abandoned as the <italic>cornerstone</italic> of research.</p>
<p>Our paper does not concern specifically the details of neuro-imaging methodology, many papers dealt with such details recently (Pernet and Poline, <xref ref-type="bibr" rid="B127">2015</xref>; Nichols et al., <xref ref-type="bibr" rid="B112">2016</xref>, <xref ref-type="bibr" rid="B113">2017</xref>). Rather, we take a more general view in discussing fundamental problems that can affect any scientific field, including neuroscience and neuro-imaging. In relation to this it is important to see that non-invasive neuroscience data related to behavioral tasks cannot be interpreted if task manipulations did not work and/or behavior is unclear. This is because most measured brain activity changes can be interpreted in many different ways on their own (Poldrack, <xref ref-type="bibr" rid="B128">2006</xref>; see Section 2.7 in Nichols et al., <xref ref-type="bibr" rid="B112">2016</xref>). So, as most behavioral data are analyzed by NHST statistics NHST based inference from behavioral data also plays a crucial role in interpreting brain data.</p>
</sec>
<sec id="s2">
<title>The origins of NHST as a weak heuristic and a decision rule</title>
<sec>
<title>NHST as a weak heuristic based on the <italic>p</italic>-value: Fisher</title>
<p><italic>p</italic>-values were popularized by Fisher (<xref ref-type="bibr" rid="B44">1925</xref>). In the context of the current NHST approach Fisher <italic>only</italic> relied on the concepts of the null hypothesis (H<sub>0</sub>) and the <italic>exact p-value</italic> (hereafter p will refer to the <italic>p</italic>-value and “pr” to probability; see Appendix <xref ref-type="supplementary-material" rid="SM1">1</xref> in Supplementary Material for terms). He thought that experiments should aim to reject (or “nullify”; henceforth the name “null hypothesis”) H<sub>0</sub> which assumes that the data demonstrates random variability according to some distribution around a certain value. Discrepancy from H<sub>0</sub> is measured by a test statistic whose values can be paired with one or two-tailed <italic>p</italic>-values which tell us how likely it is that we would have found our data <italic>or</italic> more extreme data if H<sub>0</sub> was really correct. Formally we will refer to the <italic>p</italic>-value as: pr(data or more extreme data|H<sub>0</sub>). It is important to realize that the <italic>p</italic>-value represents the “extremeness” of the data according to an imaginary data distribution assuming there is no bias in data sampling.</p>
<p>The late Fisher viewed the <italic>exact p</italic>-value as a <italic>heuristic piece of inductive evidence</italic> which gives an indication of the plausibility of H<sub>0</sub> together with other available evidence, like effect sizes (see Hubbard and Bayarri, <xref ref-type="bibr" rid="B68">2003</xref>; Gigerenzer et al., <xref ref-type="bibr" rid="B56">2004</xref>). Fisher recommended that H<sub>0</sub> can usually be rejected if <italic>p</italic> ≤ 0.05 but in his system there is no mathematical justification for selecting a particular <italic>p</italic>-value for the rejection of H<sub>0</sub>. Rather, this is up to the substantively informed judgment of the experimenter. Fisher thought that a hypothesis is demonstrable only when properly designed experiments “<italic>rarely fail”</italic> to give us statistically significant results (Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>, p. 96; Goodman, <xref ref-type="bibr" rid="B64">2008</xref>). Hence, a single significant result should not represent a “scientific fact” but should merely draw attention to a phenomenon which seems worthy of further investigation including replication (Goodman, <xref ref-type="bibr" rid="B64">2008</xref>). In contrast to the above, until recently replication studies have been very rare in many scientific fields; lack of replication efforts has been a particular problem in the psychological sciences (Makel et al., <xref ref-type="bibr" rid="B99">2012</xref>), but this may hopefully change now with the wide attention that replication has received (Aarts et al., <xref ref-type="bibr" rid="B1">2015</xref>).</p>
</sec>
<sec>
<title>Neyman and Pearson: a decision mechanism optimized for the long-run</title>
<p>The concepts of the alternative hypothesis (H<sub>1</sub>), α, power, β, Type I, and Type II errors were introduced by Neyman and Pearson (Neyman and Pearson, <xref ref-type="bibr" rid="B110">1933</xref>; Neyman, <xref ref-type="bibr" rid="B109">1950</xref>) who set up a formal decision procedure motivated by industrial quality control problems (Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>). Their approach aimed to minimize the false negative (Type II) error rate to an acceptable level (β) and consequently to maximize power (1-β) <italic>subject</italic> to a bound (α) on false positive (Type I) errors (Hubbard and Bayarri, <xref ref-type="bibr" rid="B68">2003</xref>). α can be set by the experimenter to an arbitrary value and Type-II error can be controlled by setting the sample size so that the required effect size can be detected (see Figure <xref ref-type="fig" rid="F1">1</xref> for illustration). In contrast to Fisher, this framework does not use the <italic>p</italic>-value as a measure of evidence. We merely determine the critical value of the test statistic associated with α and reject H<sub>0</sub> whenever the test statistic is larger than the critical value. The exact <italic>p</italic>-value is irrelevant because the sole objective of the decision framework is long-run error minimization and only the critical threshold but not the exact <italic>p</italic>-value plays any role in achieving this goal (Hubbard and Bayarri, <xref ref-type="bibr" rid="B68">2003</xref>). Neyman and Pearson rejected the idea of inductive reasoning and offered <italic>a reasoning</italic>-<italic>free inductive behavioral rule</italic> to choose between two behaviors, accepting or rejecting H<sub>0</sub>, irrespective of the researcher's belief about whether H<sub>0</sub> and H<sub>1</sub> are true or not (Neyman and Pearson, <xref ref-type="bibr" rid="B110">1933</xref>).</p>
<fig id="F1" position="float">
<label>Figure 1</label>
<caption>
<p>NHST concepts make sense in the context of a long run of studies. 3 × 10,000 studies with normally distributed data were simulated for 3 situations (<bold>A:</bold> True H<sub>0</sub> situation: Mean = 0; <italic>SD</italic> = 1; <italic>n</italic> = 16. <bold>B:</bold> Mean = 0.5; <italic>SD</italic> = 1; <italic>n</italic> = 16; Power = 0.46. <bold>C:</bold> Mean = 0.5; <italic>SD</italic> = 1; n = 32; Power = 0.78.). One sample two-tailed <italic>t-</italic>tests determined whether the sample means were zero. The red dots in the top panels show t scores for 3 × 1,000 studies (not all studies are shown for better visibility). The vertical dashed lines mark the critical rejection thresholds for H<sub>0</sub>, t(α/2) for the two-tailed test. The studies producing a t statistic more extreme than these thresholds are declared statistically significant. The middle panels show the distribution of t scores for all 3 × 10,000 studies (bins = 0.1). The bottom panels show the distribution of <italic>p</italic>-values for all 3 × 10,000 studies (bins = 0.01) and state the proportion of significant studies. The inset in the bottom right panel shows the mean absolute effect sizes in standard deviation units for situations A-C from all significant (Sig.) and non-significant (n.s.) studies with 95% bias corrected and accelerated bootstrap confidence intervals (10,000 permutations). The real effect size was 0 in situation <bold>(A)</bold> and 0.5 in situations <bold>(B,C)</bold>. Note that the less is the power the more statistically significant studies overstate the effect size. Also note that <italic>p</italic>-values are randomly distributed and the larger is power the more right skewed is the distribution of <italic>p</italic>-values. In the true H<sub>0</sub> situation the distribution of <italic>p</italic>-values is uniform between 0 and 1. See further explanation of this figure in Appendix <xref ref-type="supplementary-material" rid="SM1">2</xref> in Supplementary Material.</p>
</caption>
<graphic xlink:href="fnhum-11-00390-g0001"></graphic>
</fig>
<p>Crucially, the Neyman–Pearson approach is designed to work efficiently (Neyman and Pearson, <xref ref-type="bibr" rid="B110">1933</xref>) in the context of long-run repeated testing (exact replication). Hence, there is a major difference between the <italic>p</italic>-value which is computed for a <italic>single</italic> data set and α, β, power, Type I, and Type II error which are so called “<italic>frequentist”</italic> concepts and they make sense in the context of <italic>a long-run of many repeated experiments</italic>. If we only run a single experiment all we can claim is that if we <italic>had</italic> run a long series of experiments we <italic>would have had</italic> 100α% false positives (Type I error) had H<sub>0</sub> been true and 100β% false negatives (Type II error) had H<sub>1</sub> been true <italic>provided</italic> we got the power calculations right. Note the conditionals.</p>
<p>In the Neyman–Pearson framework optimally setting α and β assures long-term decision-making efficiency in light of our costs and benefits by committing Type I and Type II errors. However, optimizing α and β is much easier in industrial quality control than in research where often there is no reason to expect a specific effect size associated with H<sub>1</sub> (Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>). For example, if a factory has to produce screw heads with a diameter of 1 ± 0.01 cm than we know that we have to be able to detect a deviation of 0.01 cm to produce acceptable quality output. In this setting we know exactly the smallest effect size we are interested in (0.01 cm) and we can also control the sample size very efficiently because we can easily take a sample of a large number of screws from a factory producing them by the million assuring ample power. On the one hand, failing to detect too large or too small screws (Type II error) will result in our customers canceling their orders (or, in other industrial settings companies may deliver faulty cars or exploding laptops to customers exposing themselves to substantial litigation and compensation costs). On the other hand, throwing away false positives (Type I error), i.e., completely good batches of screws which we think are too small or too large, will also cost us a certain amount of money. Hence, we have a very clear scale (monetary value) to weigh the costs and benefits of both types of errors and we can settle on some rationally justified values of α and β so as to minimize our expenses and maximize our profit.</p>
<p>In contrast to such industrial settings, controlling the sample size and effect size and setting rational α and β levels is not that straightforward in most research settings where the true effect sizes being pursued are largely unknown and deciding about the requested size of a good enough effect can be very subjective. For example, what is the smallest difference of interest between two participant groups in a measure of “fMRI activity”? Or, what is the smallest difference of interest between two groups of participants when we measure their IQ or reaction time? And, even if we have some expectations about the “true effect size,” can we test enough participants to ensure a small enough β? Further, what is the cost of falsely claiming that a vaccine causes autism thereby generating press coverage that grossly misleads the public (Deer, <xref ref-type="bibr" rid="B31">2011</xref>; Godlee, <xref ref-type="bibr" rid="B60">2011</xref>)? What is the cost of running too many underpowered studies thereby wasting perhaps most research funding, boosting the number of false positive papers and complicating interpretation (Schmidt, <xref ref-type="bibr" rid="B132">1992</xref>; Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>; Button et al., <xref ref-type="bibr" rid="B16">2013</xref>)? More often than not researchers do not know the “true” size of an effect they are interested in, so they cannot assure adequate sample size and it is also hard to estimate general costs and benefits of having particular α and β values. While some “rules of thumb” exist about what are small, modest, and large effects (e.g., Cohen, <xref ref-type="bibr" rid="B25">1962</xref>, <xref ref-type="bibr" rid="B26">1988</xref>; Jaeschke et al., <xref ref-type="bibr" rid="B78">1989</xref>; Sedlmeier and Gigerenzer, <xref ref-type="bibr" rid="B135">1989</xref>), some large effects may not be actionable (e.g., a change in some biomarker that is a poor surrogate and thus bears little relationship to major, clinical outcomes), while some small effects may be important and may change our decision (e.g., most survival benefits with effective drugs are likely to be small, but still actionable).</p>
<p>Given the above ambiguity, researchers fall back to the default α = 0.05 level with usually undefined power. So, the unjustified α and β levels completely discredit the originally intended “<italic>efficiency”</italic> rationale of the creators of the Neyman–Pearson decision mechanism (Neyman and Pearson, <xref ref-type="bibr" rid="B110">1933</xref>).</p>
</sec>
<sec>
<title><italic>P</italic>-values are random variables and they correspond to standardized effect size measures</title>
<p>Contrary to the fact that in Figure <xref ref-type="fig" rid="F1">1</xref> all 10,000 true H<sub>0</sub> and 10,000 true H<sub>1</sub> samples were simulated from identical H<sub>0</sub> and H<sub>1</sub> distributions, the t scores and the associated <italic>p</italic>-values reflect a dramatic spread. That is, <italic>p</italic>-values are best viewed as random variables which can take on a range of values depending on the actual data (Sterling, <xref ref-type="bibr" rid="B147">1959</xref>; Murdoch et al., <xref ref-type="bibr" rid="B107">2008</xref>). Consequently, it is impossible to tell from the outcome of a single (published) experiment delivering a statistically significant result whether a true effect exist. The only difference between the true H<sub>0</sub> and true H<sub>1</sub> situations is that when H<sub>0</sub> is true in all experiments, the distribution of <italic>p</italic>-values is uniform between 0 and 1 whereas when H<sub>1</sub> is true in all experiments <italic>p</italic>-values are more likely to fall on the left of the 0–1 interval, that is, their distribution becomes right skewed. The larger is the effect size and power the stronger is this right skew (Figure <xref ref-type="fig" rid="F2">2</xref>). This fact led to the suggestion that comparing this skew allows us to determine the robustness of findings in some fields by studying “p curves” (Hung et al., <xref ref-type="bibr" rid="B69">1997</xref>; Simonsohn et al., <xref ref-type="bibr" rid="B139">2014a</xref>,<xref ref-type="bibr" rid="B140">b</xref>). Hence, from this perspective, replication, and unbiased publication of all results (“positive” and “negative”) is again crucial if we rely on NHST because only then can they inform us about the distribution of <italic>p</italic>-values.</p>
<fig id="F2" position="float">
<label>Figure 2</label>
<caption>
<p>The distribution of <italic>p</italic>-values if the alternative hypothesis (H<sub>1</sub>) is true. Each line depicts the distribution of <italic>p</italic>-values resulting from one-sample two-tailed <italic>t</italic>-tests testing whether the sample mean was zero. Effect sizes (ES) indicate the true sample means for normally distributed data with standard deviation 1. For each effect size one million simulations were run with 16 cases in each simulation. The distribution of the <italic>p</italic>-value is becoming increasingly right skewed with increasing effect size and power. Note that α, the Type I error rate, is fix irrespective of what <italic>p</italic>-value is found in an experiment.</p>
</caption>
<graphic xlink:href="fnhum-11-00390-g0002"></graphic>
</fig>
<p>Another point to notice is that both <italic>p</italic>-values and usual standardized effect size measures (Cohen's D, correlation values, etc.) are direct functions of NHST test statistics. Hence, for given degrees of freedom NHST test statistics, effect size measures and <italic>p</italic>-values will have non-linear correspondence as illustrated in Figure <xref ref-type="fig" rid="F3">3</xref>.</p>
<fig id="F3" position="float">
<label>Figure 3</label>
<caption>
<p>The relationship of the <italic>p</italic>-value, test statistic and effect sizes. <bold>(A)</bold> The relationship of t values, degrees of freedoms and <italic>p</italic>-values for Pearson correlation studies (df = n–2). <bold>(B)</bold> The relationship of Pearson correlation (r) values, degrees of freedoms, and <italic>p</italic>-values [r = t/sqrt(df + t<sup>2</sup>)]. <bold>(C)</bold> The relationship of <italic>r</italic>- and <italic>t</italic>-value pairs for each degree of freedom at various <italic>p</italic>-values. The bold black lines mark the usual significance level of α = 0.05. Note that typically only results which exceed the α = 0.05 threshold are reported in papers. Hence, papers mostly report exaggerated effect sizes.</p>
</caption>
<graphic xlink:href="fnhum-11-00390-g0003"></graphic>
</fig>
</sec>
<sec>
<title>NHST in its current form</title>
<p>The current NHST merged the approaches of Fisher and Neyman and Pearson and is often applied stereotypically as a “mindless null ritual” (Gigerenzer, <xref ref-type="bibr" rid="B54">2004</xref>). Researchers set H<sub>0</sub> nearly always “predicting” zero effect but do not quantitatively define H<sub>1</sub>. Hence, pre-experimental power cannot be calculated for most tests which is a crucial omission in the Neyman–Pearson framework. Researchers compute the <italic>exact p</italic>-value as Fisher did but also <italic>mechanistically</italic> reject H<sub>0</sub> and accept the undefined H<sub>1</sub> if <italic>p</italic> ≤ (α = 0.05) without flexibility following the <italic>behavioral decision rule</italic> of Neyman and Pearson. As soon as <italic>p</italic> ≤ α, findings have the supposed right to become a scientific fact defying the exact replication demands of Fisher and the belief neutral approach of Neyman and Pearson. Researchers also interpret the <italic>exact p</italic>-value and use it as a relative <italic>measure of evidence</italic> against H<sub>0</sub>, as Fisher did. A “<italic>highly significant”</italic> result with a small <italic>p</italic>-value is perceived as much stronger evidence than a weakly significant one. However, while Fisher was conscious of the weak nature of the evidence provided by the <italic>p</italic>-value (Wasserstein and Lazar, <xref ref-type="bibr" rid="B159">2016</xref>), generations of scientists encouraged by incorrect editorial interpretations (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>) started to exclusively rely on the <italic>p</italic>-value in their decisions even if this meant neglecting their substantive knowledge: scientific conclusions <italic>merged</italic> with reading the <italic>p</italic>-value (Goodman, <xref ref-type="bibr" rid="B63">1999</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Neglecting the full context of NHST leads to confusions about the <italic>p-</italic>value</title>
<p>Most textbooks illustrate NHST by partial 2 × 2 tables (see Table <xref ref-type="table" rid="T1">1</xref>) which fail to contextualize long-run conditional probabilities and fail to clearly distinguish between long-run probabilities and the <italic>p</italic>-value which is computed for a single data set (Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>). This leads to major confusions about the meaning of the <italic>p</italic>-value (see Appendix <xref ref-type="supplementary-material" rid="SM1">2</xref> in Supplementary Material).</p>
<table-wrap id="T1" position="float">
<label>Table 1</label>
<caption>
<p>“pr” stands for probability.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th colspan="1" rowspan="1"></th>
<th colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>True null effect (H<sub><italic>0</italic></sub>)</bold>
</th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>True positive effect (H<sub><italic>1</italic></sub>)</bold>
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>Pre-experiment probability of H<sub>0</sub> and H<sub>1</sub></italic>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">Long run of experiments</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>
<bold>pr(H</bold>
<sub>0</sub>
<bold>)</bold>
</italic>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>
<bold>pr(H</bold>
<sub>1</sub>
<bold>)</bold>
</italic>
</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">The conditional probability of having <italic>this data or more extreme data</italic> given that H<sub>0</sub> is true</td>
<td align="left" colspan="1" rowspan="1" valign="top">Single experiment</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>
<bold>p-</bold>
</italic>
<bold>value</bold>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">—</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">The conditional probability of having a significant test result given that H<sub>0</sub> or H<sub>1</sub> are true</td>
<td align="left" colspan="1" rowspan="1" valign="top">Long run of experiments</td>
<td align="left" colspan="1" rowspan="1" valign="top"><bold>Alpha level (</bold>α<bold>) Type I error</bold> False Positive False Alarm</td>
<td align="left" colspan="1" rowspan="1" valign="top"><bold>Power</bold> = 1 −β True Positive Hit</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">The conditional probability of <italic>not</italic> having a significant test result given that H<sub>0</sub> or H<sub>1</sub> are true</td>
<td align="left" colspan="1" rowspan="1" valign="top">Long run of experiments</td>
<td align="left" colspan="1" rowspan="1" valign="top">1 – α = <bold>Confidence level</bold> True Negative Correct Rejection</td>
<td align="left" colspan="1" rowspan="1" valign="top">β = 1 – Power <bold>Type II error</bold> False Negative Miss</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>Post-experiment probability of H<sub><italic>0</italic></sub> and H<sub><italic>1</italic></sub> given a significant test result</italic>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic>Long run of experiments</italic>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic><bold>FRP</bold> pr(H<sub>0</sub>|significant result)</italic>
</td>
<td align="left" colspan="1" rowspan="1" valign="top">
<italic><bold>TRP</bold> pr(H<sub>1</sub>|significant result)</italic>
</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<p><italic>NHST textbooks typically only present rows 3 and 4 of this table (Alpha level, Power, Confidence level and Type II error). We follow the NHST view and deal with long run probabilities only. Note that the p value does not fit this view as it does not have any long run interpretation besides that it is a random variable (Murdoch et al., <xref ref-type="bibr" rid="B107">2008</xref>). The most important variables are bolded, familiar signal detection categories are also provided. NHST does not deal with the concepts in italics</italic>.</p>
</table-wrap-foot>
</table-wrap>
<p>First, both H<sub>0</sub> and H<sub>1</sub> have some usually unknown pre-study or “prior” probabilities, pr(H<sub>0</sub>) and pr(H<sub>1</sub>). Nevertheless, these probabilities may be approximated through extensive substantive knowledge. For example, we may know about a single published study claiming to demonstrate H<sub>1</sub> by showing a difference between appropriate experimental conditions. However, in conferences we may have also heard about 9 highly powered but failed replication attempts very similar to the original study. In this case we may assume that the odds of H<sub>0</sub>:H<sub>1</sub> are 9:1, that is, pr(H<sub>1</sub>) is 1/10. Of course, these pre-study odds are usually hard to judge unless we demand to see our colleagues' “null results” hidden in their drawers because of the practice of not publishing negative findings. Current scientific practices appreciate the single published “positive” study more than the 9 unpublished negative ones perhaps because NHST logic only allows for rejecting H<sub>0</sub> but does not allow for accepting it <italic>and</italic> because researchers <italic>erroneously</italic> often think that the single published positive study has a very small, acceptable error rate of providing false positive statistically significant results which equals α, or the <italic>p</italic>-value. So, they often spuriously assume that the negative studies somehow lacked the sensitivity to show an effect while the single positive study is perceived as a well-executed sensitive experiment delivering a “conclusive” verdict rather than being a “lucky” false positive (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>). (See a note on pilot studies in Serious Underestimation of the Proportion of False Positive Findings in NHST).</p>
<p>NHST completely neglects the above mentioned pre-study information and exclusively deals with rows 2–4 of Table <xref ref-type="table" rid="T1">1</xref>. NHST computes the one or two-tailed <italic>p</italic>-value for a particular data set assuming that H<sub>0</sub> is true. Additionally, NHST logic takes long-run error probabilities (α and β) into account conditional on H<sub>0</sub> and H<sub>1</sub>. These long-run probabilities are represented in typical 2 × 2 NHST contingency tables but note that β is usually unknown in real studies.</p>
<p>As we have seen, NHST <italic>never</italic> computes the probability of H<sub>0</sub> and H<sub>1</sub> being true or false, all we have is a decision mechanism hoping for the best individual decision in view of long-run Type I and Type II error expectations. Nevertheless, following the repeated testing logic of the NHST framework, for many experiments we can denote the <italic>long-run probability</italic> of H<sub>0</sub> being true given a statistically significant result as False Report Probability (FRP), and the <italic>long-run probability</italic> of H<sub>1</sub> being true given a statistically significant result as True Report Probability (TRP). FRP and TRP are represented in row 5 of Table <xref ref-type="table" rid="T1">1</xref> and it is important to see that they refer to completely <italic>different conditional probabilities than</italic> the <italic>p</italic>-value.</p>
<p>Simply put, the <italic>p</italic>-value is pretty much the only thing that NHST computes but scientists usually would like to know the probability of their theory being true or false in light of their data (Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>; Goodman, <xref ref-type="bibr" rid="B62">1993</xref>; Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; Wagenmakers, <xref ref-type="bibr" rid="B156">2007</xref>). That is, researchers are interested in the post-experimental probability of H<sub>0</sub> and H<sub>1</sub>. Most probably, for the reason that researchers do not get what they really want to see and the only parameter NHST computes is the <italic>p</italic>-value it is well-documented (Oakes, <xref ref-type="bibr" rid="B119">1986</xref>; Gliner et al., <xref ref-type="bibr" rid="B59">2002</xref>; Castro Sotos et al., <xref ref-type="bibr" rid="B19">2007</xref>, <xref ref-type="bibr" rid="B20">2009</xref>; Wilkerson and Olson, <xref ref-type="bibr" rid="B162">2010</xref>; Hoekstra et al., <xref ref-type="bibr" rid="B67">2014</xref>) that many, if not most researchers confuse FRP with the <italic>p</italic>-value or α and they also confuse the complement of <italic>p</italic>-value (1-p) or α (1-α) with TRP (Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>; Cohen, <xref ref-type="bibr" rid="B27">1994</xref>). These confusions are of major portend because the difference between these completely different parameters is not minor, they can differ by orders of magnitude, the long-run FRP being much larger than the <italic>p</italic>-value under realistic conditions (Sellke et al., <xref ref-type="bibr" rid="B136">2001</xref>; Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>). The complete misunderstanding of the probability of producing false positive findings is most probably a key factor behind vastly inflated confidence in research findings and we suggest that this inflated confidence is an important contributor to the current replication crisis in biomedical science and psychology.</p>
<sec>
<title>Serious underestimation of the proportion of false positive findings in NHST</title>
<p>Ioannidis (<xref ref-type="bibr" rid="B76">2005</xref>) has shown that most published research findings relying on NHST are likely to be false. The modeling supporting this claim refers to the long-run FRP and TRP which we can compute by applying Bayes' theorem (see Figure <xref ref-type="fig" rid="F4">4</xref> for illustration, see computational details and further illustration in Appendix <xref ref-type="supplementary-material" rid="SM1">3</xref> in Supplementary Material). The calculations must consider α, the power (1-β) of the statistical test used, the pre-study probabilities of H<sub>0</sub> and H<sub>1</sub>, and it is also insightful to consider bias (Berger, <xref ref-type="bibr" rid="B10">1985</xref>; Berger and Delampady, <xref ref-type="bibr" rid="B11">1987</xref>; Berger and Sellke, <xref ref-type="bibr" rid="B12">1987</xref>; Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>; Lindley, <xref ref-type="bibr" rid="B93">1993</xref>; Sellke et al., <xref ref-type="bibr" rid="B136">2001</xref>; Sterne and Smith, <xref ref-type="bibr" rid="B149">2001</xref>; Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>).</p>
<fig id="F4" position="float">
<label>Figure 4</label>
<caption>
<p>Illustration of long run False Positive Probability (FRP) and True Positive Probability (TRP) of studies. Let's assume that we run 2 × 100 studies, H<sub>0</sub> is true in 100 studies and H<sub>1</sub> is true in 100 studies with α = 0.05 and Power = 1−β = 0.6. <bold>(A)</bold> Shows the outcome of true H<sub>0</sub> studies, 5 of the 100 studies coming up statistically significant. <bold>(B)</bold> Shows the outcome of true H<sub>1</sub> studies, 60 of the 100 studies coming up statistically significant [note that realistically the 60 studies would be scattered around just as in panel <bold>(A)</bold> but for better visibility they are represented in a block]. <bold>(C)</bold> Illustrates that true H<sub>0</sub> and true H<sub>1</sub> studies would be indistinguishable. That is, researchers do not know which study tested a true H<sub>0</sub> or true H<sub>1</sub> situation (i.e., they could not distinguish studies represented by black and gray squares). All they know is whether the outcome of a particular study out of the 200 studies run was statistically significant or not. FRP is the ratio of false positive (H<sub>0</sub> is true) statistically significant studies to all statistically significant studies: 5/65 = 0.0769. TRP is the ratio of truly positive (H<sub>1</sub> is true) statistically significant studies to all statistically significant studies: 60/65 = 0.9231 = 1 − FRP = 1 − 0.0769.</p>
</caption>
<graphic xlink:href="fnhum-11-00390-g0004"></graphic>
</fig>
<p>While NHST neglects the pre-study odds of H<sub>0</sub> and H<sub>1</sub>, these are crucial to take into account when calculating FRP and TRP. For example, let's assume that we run 200 experiments and in 100 studies our experimental ideas are wrong (that is, we test true H<sub>0</sub> situations) while in 100 studies our ideas are correct (that is, we test true H<sub>1</sub> situations). Let's also assume that the power (1-β) of our statistical test is 0.6 and α = 0.05. In this case in 100 studies (true H<sub>0</sub>) we will have 5% of results significant by chance alone and in the other 100 studies (true H<sub>1</sub>) 60% of studies will come up significant. FRP is the ratio of false positive studies to all studies which come up significant:</p>
<disp-formula id="E1">
<mml:math id="M1">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mi>F</mml:mi>
<mml:mi>R</mml:mi>
<mml:mi>P</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mi>F</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>e</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>p</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>v</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>A</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>l</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>y</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>f</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>t</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>r</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>5</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>100</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>5</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>100</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>60</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>100</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>5</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>5</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>60</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mtext>  </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>5</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>65</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>0769</mml:mn>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula>
<p>That is, we will have 5 false positives out of a total of 65 statistically significant outcomes which means that the proportion of false positive studies amongst all statistically significant results is 7.69%, higher than the usually assumed 5%. However, this example still assumes that we get every second hypothesis right. If we are not as lucky and only get every sixth hypothesis right then if we run 600 studies, 500 of them will have true H<sub>0</sub> true situations and 100 of them will have true H<sub>1</sub> situations. Hence, the computation will look like:</p>
<disp-formula id="E2">
<mml:math id="M2">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mi>F</mml:mi>
<mml:mi>R</mml:mi>
<mml:mi>P</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mi>F</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>e</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>p</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>v</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>A</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>l</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>y</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>f</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>t</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>r</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>5</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>500</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>5</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>500</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>60</mml:mn>
<mml:mi>%</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mi>o</mml:mi>
<mml:mi>f</mml:mi>
<mml:mtext> </mml:mtext>
<mml:mn>100</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>s</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>25</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>25</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mn>60</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mtext> </mml:mtext>
<mml:mfrac>
<mml:mrow>
<mml:mn>25</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>85</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>2941</mml:mn>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula>
<p>Hence, nearly 1/3 of all statistically significant findings will be false positives irrespective of the <italic>p</italic>-value. Note that this issue is basically the consequence of running multiple NHST tests throughout the whole literature and FRP can be considered the uncontrolled false discovery rate (FDR) across all studies run (see Section Family-Wise Error Rate (FWER) and FDR Correction in NHST).</p>
<p>Crucially, estimating pre-study odds is difficult, primarily due to the lack of publishing negative findings and to the lack of proper documentation of experimenter intentions before an experiment is run: We do not know what percent of the published statistically significant findings are lucky false positives explained <italic>post-hoc</italic> (Kerr, <xref ref-type="bibr" rid="B86">1998</xref>) when in fact researchers could not detect the originally hypothesized effect and/or worked out analyses depending on the data (Gelman and Loken, <xref ref-type="bibr" rid="B50">2014</xref>). However, it is reasonable to <italic>assume</italic> that only the most risk avoidant studies have lower H<sub>0</sub>:H<sub>1</sub> odds than 1, relatively conservative studies have low to moderate H<sub>0</sub>:H<sub>1</sub> odds (1–10) while H<sub>0</sub>:H<sub>1</sub> odds can be much higher in explorative research (50–100 or even higher; Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>).</p>
<p>The above H<sub>0</sub>:H<sub>1</sub> assumptions are reasonable, as they are supported by empirical data in many different fields. For example, half or more of the drugs tested in large, late phase III trials show higher effectiveness against older comparators (H<sub>0</sub>:H<sub>1</sub> = &lt;1; Soares et al., <xref ref-type="bibr" rid="B146">2005</xref>). Conversely, the vast majority of tested hypotheses in large-scale exploratory research reflect null effects, e.g., in the search of genetic variants associated with various diseases in the candidate gene era where investigators were asking hypotheses one or a few at a time (the same way that investigators continue to test hypotheses in most other biomedical and social science fields) yielded thousands of putative discovered associations, but only 1.2% of them were subsequently validated to be non-null when large-scale consortia with accurate measurements and rigorous analyses plans assessed them (Chanock et al., <xref ref-type="bibr" rid="B22">2007</xref>; Ioannidis et al., <xref ref-type="bibr" rid="B73">2011</xref>). Of the hundreds of thousands to many millions of variables assessed in current agnostic–omics testing, much less than 1% are likely to reflect non-null effects (H<sub>0</sub>:H<sub>1</sub>&gt;&gt;100). Lower rates of H<sub>0</sub>:H<sub>1</sub> would be incompatible with logical considerations of how many variables are needed to explain all the variance of a disease or outcome risk.</p>
<p>Besides H<sub>0</sub>:H<sub>1</sub> odds bias is another important determinant of FRP and TRP (Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>). Whenever, H<sub>0</sub> is not rejected findings have far more difficulty to be published and the researcher may feel that she wasted her efforts. Further, positive findings are more likely to get cited than negative findings (Kjaergard and Gluud, <xref ref-type="bibr" rid="B89">2002</xref>; Jannot et al., <xref ref-type="bibr" rid="B79">2013</xref>; Kivimäki et al., <xref ref-type="bibr" rid="B88">2014</xref>). Consequently, researchers may often be highly biased to reject H<sub>0</sub> and publish positive findings. Researcher bias affects FRP even if our NHST decision criteria, α and β, are formally unchanged. Ioannidis (<xref ref-type="bibr" rid="B76">2005</xref>) introduced the <italic>u</italic> bias parameter. The impact of u is that after some data tweaking and selective reporting (see Section NHST May Foster Selective Reporting and Subjectivity) u fraction of otherwise non-significant true H<sub>0</sub> results will be reported as significant and u faction of otherwise non-significant true H<sub>1</sub> results will be reported as significant. If u increases, FRP increases and TRP decreases. For example, if α = 0.05, power = 0.6, and H<sub>0</sub>:H<sub>1</sub> odds = 1 then a 10% bias (u = 0.1) will raise FRP to 18.47%. A 20% bias will raise FRP to 26.09%. If H<sub>0</sub>:H<sub>1</sub> odds = 6 then FRP will be 67.92%. Looking at these numbers the replication crisis does not seem surprising: using NHST very high FRP can be expected even with modestly high H<sub>0</sub>:H<sub>1</sub> odds and moderate bias (Etz and Vandekerckhove, <xref ref-type="bibr" rid="B39">2016</xref>). Hence, under realistic conditions FRP not only <italic>extremely rarely</italic> equals α or the <italic>p</italic>-value (and TRP extremely rarely equals 1-α and/or 1-<italic>p</italic>-value) but also, FRP is <italic>much</italic> larger than the generally assumed 5% and TRP is much lower than the generally assumed 95%. Overall, α or the <italic>p</italic>-value practically says nothing about the likelihood of our research findings being true or false.</p>
<p>At this point it is worth noting that it could be argued that unpublished pilot experiments may prompt us to run studies and hence, often H<sub>0</sub>:H<sub>1</sub> odds would be lower than 1. However, unpublished pilot data often comes from small scale underpowered studies with high FRP, undocumented initial hypotheses and analysis paths. Hence, we doubt that statistically significant pilot results inevitably mean low H<sub>0</sub>:H<sub>1</sub> odds.</p>
</sec>
<sec>
<title>The neglect of power reinterpreted</title>
<p>In contrast to the importance of power in determining FRP and TRP, NHST studies tend to ignore power and β and emphasize α and low <italic>p</italic>-values. Often, finding a statistically significant effect erroneously seems to override the importance of power. However, statistical significance does not protect us from false positives. FRP can only be minimized by keeping H<sub>0</sub>:H<sub>1</sub> odds and bias low and power high (Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>; Button et al., <xref ref-type="bibr" rid="B16">2013</xref>; Bayarri et al., <xref ref-type="bibr" rid="B4">2016</xref>). Hence, power is not only important so that we increase our chances to detect true effects but it is also crucial in keeping FRP low. While power in principle can be adjusted easily by increasing sample size, power in many/most fields of biomedical science and psychology has been notoriously low and the situation has not improved much during the past 50 years (Cohen, <xref ref-type="bibr" rid="B25">1962</xref>; Sedlmeier and Gigerenzer, <xref ref-type="bibr" rid="B135">1989</xref>; Rossi, <xref ref-type="bibr" rid="B130">1990</xref>; Hallahan and Rosenthal, <xref ref-type="bibr" rid="B66">1996</xref>; Button et al., <xref ref-type="bibr" rid="B16">2013</xref>; Szucs and Ioannidis, <xref ref-type="bibr" rid="B153">2017</xref>). Clearly, besides making sure that research funding is not wasted, minimizing FRP also provides very strong rationale for increasing the typically used sample sizes in studies.</p>
</sec>
</sec>
<sec id="s4">
<title>NHST logic is incomplete</title>
<sec>
<title>NHST misleads because it neglects pre-data probabilities</title>
<p>Besides often being subject to conceptual confusion and generating misleading inferences especially in the setting of weak power, NHST has further serious problems. NHST logic is based on the so-called <italic>modus tollens</italic> (denying the consequent) argumentation (see footnote in Appendix <xref ref-type="supplementary-material" rid="SM1">4</xref> in Supplementary Material): It sets up a H<sub>0</sub> model and assumes that if the data fits this model than the test statistic associated with the data should not take more extreme values than a certain threshold (Meehl, <xref ref-type="bibr" rid="B101">1967</xref>; Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>). If the test statistic contradicts this expectation then NHST assumes that H<sub>0</sub> can be rejected and consequently its complement, H<sub>1</sub> can be accepted. While this logic may be able to minimize Type I error in well-powered high-quality well-controlled tests (Section Neyman and Pearson: A decision Mechanism Optimized for the Long-Run), it is inadequate if we use it to decide about the truth of H<sub>1</sub> in a single experiment, because there is always space for Type I and Type II error (Falk and Greenbaum, <xref ref-type="bibr" rid="B42">1995</xref>). So, our conclusion is never certain and the only way to see how much error we have is to calculate the long-run FRP and TRP using appropriate α and power levels and prior H<sub>0</sub>:H<sub>1</sub> odds. The outcome of the calculation can easily conflict with NHST decisions (see Appendix <xref ref-type="supplementary-material" rid="SM1">4</xref> in Supplementary Material).</p>
</sec>
<sec>
<title>NHST neglects predictions under H<sub>1</sub> facilitating sloppy research</title>
<p>NHST does not require us to specify exactly what data H<sub>1</sub> would predict. Whereas, the Neyman–Pearson approach requires researchers to specify an effect size associated with H<sub>1</sub> and compute power (1-β), in practice this is easy to <italic>neglect</italic> because the NHST machinery only computes the <italic>p</italic>-value conditioned on H<sub>0</sub> and it is able to provide this result even if H<sub>1</sub> is not specified at all. A widespread <italic>misconception</italic> flowing from the fuzzy attitude of NHST to H<sub>1</sub> is that rejecting H<sub>0</sub> allows for accepting a <italic>specific</italic> H<sub>1</sub> (Nickerson, <xref ref-type="bibr" rid="B114">2000</xref>). This is what most practicing researchers do in practice when they reject H<sub>0</sub> and argue for their specific H<sub>1</sub> in turn. However, NHST only computes probabilities conditional on H<sub>0</sub> and it does not allow for the acceptance of either H<sub>0</sub>, a specific H<sub>1</sub> or a generic H<sub>1</sub>. Rather, it only allows for the rejection of H<sub>0</sub>. Hence, if we reject H<sub>0</sub> we will have no idea about how well our data fits a specific H<sub>1</sub>. This cavalier attitude to H<sub>1</sub> can easily lead us astray even when contrasting H<sub>0</sub> just with a single alternative hypothesis as illustrated by the invalid inference based on NHST logic in Table <xref ref-type="table" rid="T2">2</xref> (Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>).</p>
<table-wrap id="T2" position="float">
<label>Table 2</label>
<caption>
<p>Potential NHST style argument (based on Pollard and Richardson, <xref ref-type="bibr" rid="B129">1987</xref>).</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>H<sub>0</sub></bold>
</th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>Harold is American</bold>
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">H<sub>1</sub></td>
<td align="left" colspan="1" rowspan="1" valign="top">Harold is not American</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">Model for H<sub>0</sub></td>
<td align="left" colspan="1" rowspan="1" valign="top">If Harold is American (H<sub>0</sub>), than he is <italic>most probably not</italic> a member of congress.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">data</td>
<td align="left" colspan="1" rowspan="1" valign="top">Harold <italic>is</italic> a member of congress.</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">pr(data or more extreme data|H<sub>0</sub>)</td>
<td align="left" colspan="1" rowspan="1" valign="top">Very low</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">Inference</td>
<td align="left" colspan="1" rowspan="1" valign="top">Because pr(data or more extreme data|H<sub>0</sub>) is very low, we reject H<sub>0</sub> and accept H<sub>1</sub> and conclude: Harold is <italic>most probably not</italic> American.</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Our model says that if H<sub>0</sub> is true, it is a <italic>very rare</italic> event that Harold is a member of congress. This rare event then happens which is equivalent to finding a small <italic>p</italic>-value. Hence, we conclude that H<sub>0</sub> can be rejected and H<sub>1</sub> is accepted (i.e., Harold <italic>is</italic> a member of congress and <italic>therefore</italic> he is not American.). However, if we carefully explicate all probabilities it is easy to see that we are being mislead by NHST logic. First, because we have absolutely no idea about Harold's nationality we can set pre-data probabilities of both H<sub>1</sub> and H<sub>0</sub> to 1/2, which means that H<sub>0</sub>:H<sub>1</sub> odds are uninformative, 1:1. Then we can explicate the important conditional probabilities of the data (Harold <italic>is</italic> a member of congress) given the possible hypotheses. We can assign arbitrary but plausible probabilities:</p>
<disp-formula id="E3">
<mml:math id="M3">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mtext>pr</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mtext>data</mml:mtext>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>H</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mtext>pr</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mtext>Harold is member of congress </mml:mtext>
<mml:mo>|</mml:mo>
<mml:mtext> American</mml:mtext>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:msup>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>7</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mtext>pr</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mtext>data</mml:mtext>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>H</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mtext>pr</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mtext>Harold is member of congress</mml:mtext>
<mml:mo>|</mml:mo>
<mml:mtext>not American</mml:mtext>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula>
<p>That is, while the data is indeed rare under H<sub>0</sub>, its probability is actually zero under H<sub>1</sub> (in other words, the data is very unlikely under both the null and the alternative models). So, even if <italic>p</italic> ≈ 0.0000001, it does not make sense to reject H<sub>0</sub> and accept H<sub>1</sub> because this data just cannot happen if H<sub>1</sub> is true. If we only have these two hypotheses to choose from then it only makes sense to accept H<sub>0</sub> because the data is still possible under H<sub>0</sub> (Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>). In fact, using Bayes' theorem we can formally show that the probability of H<sub>0</sub> is actually 1 (Appendix <xref ref-type="supplementary-material" rid="SM1">5</xref> in Supplementary Material).</p>
<p>In most real world problems multiple alternative hypotheses compete to explain the data. However, by using NHST we can only reject H<sub>0</sub> and argue for <italic>some</italic> H<sub>1</sub> without any formal justification of why we prefer a particular hypothesis whereas it can be argued that it only makes sense to reject any hypothesis if another one better fits the data (Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>). We only have qualitative arguments to accept a specific H<sub>1</sub> and the exclusive focus on H<sub>0</sub> makes unjustified inference too easy. For example, if we assume that H<sub>0</sub> predicts normally distributed data with mean 0 and standard deviation 1 then we have endless options to pick H<sub>1</sub> (Hubbard and Bayarri, <xref ref-type="bibr" rid="B68">2003</xref>): Does H<sub>1</sub> imply that the data have a mean other than zero, the standard deviation other than 1 and/or does it represent non-normally distributed data? NHST allows us to consider any of these options <italic>implicitly</italic> and then accept one of them <italic>post-hoc</italic> without any quantitative justification of why we chose that particular option. Further, merging all alternative hypotheses into a single H<sub>1</sub> is not only too simplistic for most real world problems but it also poses an “inferential double standard” (Rozeboom, <xref ref-type="bibr" rid="B131">1960</xref>): The procedure pits the well-defined H<sub>0</sub> against a potentially infinite number of alternatives.</p>
<p>Vague H<sub>1</sub> definitions (the lack of quantitative predictions) enable researchers to avoid the falsification of their favorite hypotheses by intricately redefining them (especially in fields such as psychology and cognitive neuroscience where theoretical constructs are often vaguely defined) and never providing any definitive assessment of the plausibility of a favorite hypothesis in light of credible alternatives (Meehl, <xref ref-type="bibr" rid="B101">1967</xref>). This problem is reflected in papers aiming at the mere demonstration of often little motivated significant differences between conditions (Giere, <xref ref-type="bibr" rid="B52">1972</xref>) and <italic>post-hoc</italic> explanations of likely unexpected but statistically significant findings. For example, neuroimaging studies often attempt to explain why an fMRI BOLD signal “deactivation” happened instead of a potentially more reasonable looking “activation” (or, vice versa). Most such findings may be the consequence of the data randomly deviating into the wrong direction relative to zero between-condition difference. Even multiple testing correction will not help such studies as they still rely on standard NHST just with adjusted α thresholds. Similarly, patient studies often try to explain an unexpected difference between patient and control groups (e.g., the patient group is “better” on a measure or shows “more” or “less” brain activation) by some kind of “compensatory mechanism.” In such cases what happens is that “<italic>the burden of inference has been delegated to the statistical test</italic>,” indeed, and simply because <italic>p</italic> ≤ α odd looking observations and claims are to be trusted as scientific facts (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>, p. 423; Lykken, <xref ref-type="bibr" rid="B96">1968</xref>).</p>
<p>Finally, paradoxically, when real life practicing researchers achieve their “goal” and successfully reject H<sub>0</sub> they may be left in complete existential vacuum because during the rejection of H<sub>0</sub> NHST “<italic>saws off its own limb”</italic> (Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; p. 524): If we manage to reject H<sub>0</sub> then it follows that pr(data or more extreme data|H<sub>0</sub>) is useless because H<sub>0</sub> is not true. Thus, we are left with nothing to characterize the probability of our data in the real world; we will not know pr(data|H<sub>1</sub>) for example, because H<sub>1</sub> is formally undefined and NHST never tells us anything about it. In light of these problems Jaynes (<xref ref-type="bibr" rid="B80">2003</xref>) suggested that the NHST framework addresses an ill-posed problem and provides invalid responses to questions of statistical inference.</p>
<p>It is noteworthy that some may argue that Jaynes's argument is formally invalid as the NHST approach can be used to reject a low probability H<sub>0</sub>
<italic>in theory</italic>. However, recall that (1) NHST does not deliver final objective theoretical decisions, there is no theoretical justification for any α thresholds marking a boundary of informal surprise and NHST merely aims to minimize Type I error on the long run (and in fact, Neyman and Pearson (<xref ref-type="bibr" rid="B110">1933</xref>) considered their procedure a theory-free decision mechanism and Fisher considered it a heuristic). (2) NHST can only reject H<sub>0</sub> (heuristically or in a theory-free manner) and (3) cannot provide support for any H<sub>1</sub>. We could also add that many practicing biomedical and social scientists may not have clear quantitative predictions under H<sub>0</sub> besides expecting to reject a vague null effect (see Section NHST in Sciences with and without Exact Quantitative Predictions for the difference between sciences with and without exact predictions). Hence, their main (ultimate) objective of using NHST is often actually not the <italic>falsification</italic> of the <italic>exact</italic> theoretical predictions of a well-defined theory (H<sub>0</sub>). Rather, they are more interested in arguing in favor of an alternative theory. For example, with a bit of creativity fMRI “activation” in many different (perhaps <italic>post-hoc</italic> defined) ROIs can easily be “explained” by some theory when H<sub>0</sub> (“no activation”) is rejected in any of the ROIs. However, supporting a specific alternative theory is just not possible in the NHST framework and in this context Jaynes' comment is perfectly valid: NHST provides an ill-defined framework, after rejecting H<sub>0</sub> real-world researchers have no formal hypothesis test outcomes to support their “positive” arguments.</p>
</sec>
<sec>
<title>NHST is unsuitable for large datasets</title>
<p>In consequence of the recent ‘big data’ revolution access to large databases has increased dramatically potentially increasing power tremendously (though, large data sets with many variables are still relatively rare in neuroscience research). However, NHST leads to worse inference with large databases than with smaller ones (Meehl, <xref ref-type="bibr" rid="B101">1967</xref>; Khoury and Ioannidis, <xref ref-type="bibr" rid="B87">2014</xref>). This is due to how NHST tests statistics are computed, the properties of real data and to the lack of specifying data predicted by H<sub>1</sub> (Bruns and Ioannidis, <xref ref-type="bibr" rid="B15">2016</xref>).</p>
<p>Most NHST studies rely on nil null hypothesis testing (Nickerson, <xref ref-type="bibr" rid="B114">2000</xref>) which means that H<sub>0</sub> expects a true mean difference of exactly zero between conditions with some variation around this true zero mean. Further, NHST machinery guarantees that we can detect any tiny irrelevant effect sizes if sample size is large enough. This is because test statistics are typically computed as the ratio of the relevant between condition differences and associated variability of the data weighted by some function of the sample size [difference/variability × f(sample size)]. The <italic>p</italic>-value is smaller if the test statistic is larger. Thus, the larger is the difference between conditions and/or the smaller is variability and/or the larger is the sample size the larger is the test statistic and the smaller is the <italic>p</italic>-value (see Figure <xref ref-type="fig" rid="F3">3</xref> for examples). Consequently, by increasing sample size enough it is guaranteed that H<sub>0</sub> can be rejected even with miniature effect sizes (Ziliak and McCloskey, <xref ref-type="bibr" rid="B163">2008</xref>).</p>
<p>Parameters of many real data sets are much more likely to differ than to be the same for reasons completely unrelated to our hypotheses (Meehl, <xref ref-type="bibr" rid="B101">1967</xref>, <xref ref-type="bibr" rid="B103">1990</xref>; Edwards, <xref ref-type="bibr" rid="B36">1972</xref>). First, many psychological, social and biomedical phenomena are extremely complex reflecting the contribution of very large numbers of interacting (latent) factors, let it be at the level of society, personality or heavily networked brain function or other biological networks (Lykken, <xref ref-type="bibr" rid="B96">1968</xref>; Gelman, <xref ref-type="bibr" rid="B47">2015</xref>). Hence, if we select any two variables related to these complex networks most probably there will be some kind of at least remote connection between them. This phenomenon is called “crud factor” Meehl (<xref ref-type="bibr" rid="B103">1990</xref>) or “ambient correlational noise” (Lykken, <xref ref-type="bibr" rid="B96">1968</xref>) and it is unlikely to reflect a causal relationship. In fact some types of variables, such as intake of various nutrients and other environmental exposures are very frequently correlated among themselves and with various disease outcomes without this meaning that they have anything to do with causing disease outcomes (Patel and Ioannidis, <xref ref-type="bibr" rid="B121">2014a</xref>,<xref ref-type="bibr" rid="B122">b</xref>). Second, unlike in physical sciences it is near impossible to control for the relationship of all irrelevant variables which are correlated with the variable(s) of interest (Rozeboom, <xref ref-type="bibr" rid="B131">1960</xref>; Lykken, <xref ref-type="bibr" rid="B96">1968</xref>). Consequently, there can easily be a small effect linking two randomly picked variables even if their statistical connection merely communicates that they are part of a vast complex interconnected network of variables. Only a few of these tiny effects are likely to be causal and of any portend (Siontis and Ioannidis, <xref ref-type="bibr" rid="B141">2011</xref>).</p>
<p>The above issues have been demonstrated empirically and by simulations. For example, Bakan (<xref ref-type="bibr" rid="B2">1966</xref>; see also Berkson, <xref ref-type="bibr" rid="B13">1938</xref>; Nunnally, <xref ref-type="bibr" rid="B118">1960</xref>; Meehl, <xref ref-type="bibr" rid="B101">1967</xref>) subdivided the data of 60,000 persons according to completely arbitrary criteria, like living east or west of the Mississippi river, living in the north or south of the USA, etc. and found all tests coming up statistically significant. Waller (<xref ref-type="bibr" rid="B158">2004</xref>) examined the personality questionnaire data of 81,000 individuals to see how many randomly chosen directional null hypotheses can be rejected. If sample size is large enough, 50% of directional hypothesis tests should be significant irrespective of the hypothesis. As expected, nearly half (46%) of Waller's (<xref ref-type="bibr" rid="B158">2004</xref>) results were significant. Simulations suggest that in the presence of even tiny residual confounding (e.g., some omitted variable bias) or other bias, large observational studies of null effects will generate results that may be mistaken as revealing thousands of true relationships (Bruns and Ioannidis, <xref ref-type="bibr" rid="B15">2016</xref>). Experimental studies may also suffer the same problem, if they have even minimal biases.</p>
</sec>
<sec>
<title>NHST in sciences with and without exact quantitative predictions</title>
<p>Due to the combination of the above properties of real-world data sets and statistical machinery theory testing radically differs in sciences with exact and non-exact quantitative predictions (Meehl, <xref ref-type="bibr" rid="B101">1967</xref>). In physical sciences increased measurement precision and increased amounts of data increase the difficulties a theory must pass before it is accepted. This is because theoretical predictions are well-defined, numerically precise and it is also easier to control measurements (Lykken, <xref ref-type="bibr" rid="B96">1968</xref>). Hence, NHST may be used to aim to falsify exact theoretical predictions. For example, a theory may predict that a quantity should be let's say 8 and the experimental setup can assure that really only very few factors influence measurements—these factors can then be taken into account during analysis. Hence, increased measurement precision will make it easier to demonstrate a departure from numerically exact predictions. So, a “five sigma” deviation rule may make good sense in physics where precise models are giving precise predictions about variables.</p>
<p>In sciences using NHST without clear numerical predictions the situation is the opposite of the above, because NHST does not demand the exact specification of H<sub>1</sub>, so theories typically only predict a fairly vague “<italic>difference”</italic> between groups or experimental conditions rather than an exact numerical discrepancy between measures of groups or conditions. However, as noted, groups are actually likely to differ and if sample size increases and variability in data decreases it will become easier and easier to reject any kind of H<sub>0</sub> when following the NHST approach. In fact, with precise enough measurements, large enough sample size and repeated “falsification” attempts H<sub>0</sub> is guaranteed to be rejected on the long run (see Section The Rejection of H<sub>0</sub> is Guaranteed on the Long-Run) even if the underlying processes generating the data in two experimental conditions are exactly the same. Hence, ultimately any H<sub>1</sub> can be accepted, claiming support for any kind of theory. For example, in an amusing demonstration Carver (<xref ref-type="bibr" rid="B18">1993</xref>) used Analysis of Variance to re-analyze the data of Michaelson and Morley (<xref ref-type="bibr" rid="B104">1887</xref>) who came up with a “dreaded” null finding and based on this they suggested that the speed of light was constant (H<sub>0</sub>) thereby providing empirical support for Einstein's theory of relativity. Carver (<xref ref-type="bibr" rid="B18">1993</xref>) found that that the speed of light was actually not constant at <italic>p</italic> &lt; 0.001. The catch? The effect size as measured by Eta<sup>2</sup> was 0.005. While some may feel that Einstein's theory has now been falsified, perhaps it is also worth considering that here the statistically significant result is essentially insignificant. This example also highlights the fact we are not arguing against the Popperian view of scientific progress by falsifying theories. Rather, we discuss why NHST is a very imperfect method for this falsification (see further arguments as well).</p>
<p>A typical defense of NHST may be that we actually may not want to increase power endlessly, just as much as we still think that it allows us to detect reasonable effect sizes (Giere, <xref ref-type="bibr" rid="B52">1972</xref>). For example, equivalence testing may be used to reject the hypothesis that a meaningfully large effect exist (e.g., Wellek, <xref ref-type="bibr" rid="B160">2010</xref>) or researchers may check for the sign of expected effects (Gelman and Tuerlinckx, <xref ref-type="bibr" rid="B51">2000</xref>). However, because typically only statistically significant data is published, published studies most probably exaggerate effect sizes. So, estimating true (expected) effect sizes is very difficult. A more reasoned approach may be to consider explicitly what the consequences (“costs”) are of a false-positive, true-positive, false-negative, and true-negative result. Explicit modeling can suggest that the optimal combination of Type 1 error and power may need to be different depending on what these assumed costs are (Djulbegovic et al., <xref ref-type="bibr" rid="B34">2014</xref>). Different fields may need to operate at different optimal ratios of false-positives to false-negatives (Ioannidis et al., <xref ref-type="bibr" rid="B73">2011</xref>).</p>
</sec>
<sec>
<title>NHST may foster selective reporting and subjectivity</title>
<p>Because NHST never evaluates H<sub>1</sub> formally and it is fairly biased toward the rejection of H<sub>0</sub>, reporting bias against H<sub>0</sub> can easily infiltrate the literature even if formal NHST parameters are fixed (see Section Serious Underestimation of the Proportion of False Positive Findings in NHST about the “u” bias parameter). Overall, a long series of exploratory tools and questionable research practices are utilized in search for statistical significance (Ioannidis and Trikalinos, <xref ref-type="bibr" rid="B74">2007</xref>; John et al., <xref ref-type="bibr" rid="B82">2012</xref>). Researchers can influence their data during undocumented analysis and pre-processing steps and by the mere choice of structuring the data (constituting <italic>researcher degrees of freedom</italic>; Simmons et al., <xref ref-type="bibr" rid="B138">2011</xref>). This is particularly a problem in neuroimaging where the complexity and idiosyncrasy of analyses is such that it is usually impossible to replicate exactly what happened and why during data analysis (Kriegeskorte et al., <xref ref-type="bibr" rid="B91">2009</xref>; Vul et al., <xref ref-type="bibr" rid="B155">2009</xref>; Carp, <xref ref-type="bibr" rid="B17">2012</xref>). Another term that has been used to describe the impact of diverse analytical choices is “vibration of effects” (Ioannidis, <xref ref-type="bibr" rid="B71">2008</xref>). Different analytical options, e.g., choice of adjusting covariates in a regression model can result in a cloud of results, instead of a single result, and this may entice investigators to select a specific result that is formally significant, while most analytical options would give non-significant results or even results with effects in the opposite direction (“Janus effect”; Patel et al., <xref ref-type="bibr" rid="B123">2015</xref>). Another common mechanism that may generate biased results with NHST is when investigators continue data collection and re-analyse the accumulated data sequentially without accounting for the penalty induced by this repeated testing (DeMets and Lan, <xref ref-type="bibr" rid="B32">1994</xref>; Goodman, <xref ref-type="bibr" rid="B63">1999</xref>; Szucs, <xref ref-type="bibr" rid="B152">2016</xref>). The unplanned testing is usually undocumented and researchers may not even be conscious that it exposes them to Type I error accumulation. Bias may be the key explanation why in most biomedical and social science disciplines, the vast majority of published papers with empirical data report statistically significant results (Kavvoura et al., <xref ref-type="bibr" rid="B84">2007</xref>; Fanelli, <xref ref-type="bibr" rid="B43">2010</xref>; Chavalarias et al., <xref ref-type="bibr" rid="B23">1990-2015</xref>). Overall, it is important to see that NHST can easily be infiltrated by several undocumented subjective decisions. (Bayesian methods are often blamed such subjectivity, see Section Teach Alternative Approaches Seriously.)</p>
</sec>
<sec>
<title>The rejection of H<sub>0</sub> is guaranteed on the long-run</title>
<p>If H<sub>0</sub> is true, with α = 0.05, 5% of our tests will be statistically significant on the long-run. The riskier experiments we run, the larger are H<sub>0</sub>:H<sub>1</sub> odds and bias and the larger is the long-run FRP. For example, in a large laboratory with 20 post-docs and PhD students, each person running 5 experiments a year implementing 10 significance tests in each experiment we can expect 20 × 5 × 10 × 0.05 = 50 [usually publishable] false results a year at α = 0.05 if H<sub>0</sub> is true. Coupled with the fact that a large number of unplanned tests may be run in each study (Simmons et al., <xref ref-type="bibr" rid="B138">2011</xref>; Gelman and Loken, <xref ref-type="bibr" rid="B50">2014</xref>) and that negative results and failed replications are often not published, this leads to “<italic>unchallenged fallacies”</italic> clogging up the research literature (Ioannidis, <xref ref-type="bibr" rid="B75">2012</xref>; p1; Sterling, <xref ref-type="bibr" rid="B147">1959</xref>; Bakan, <xref ref-type="bibr" rid="B2">1966</xref>; Sterling et al., <xref ref-type="bibr" rid="B148">1995</xref>). Moreover, such published false positive true H<sub>0</sub> studies will also inevitably overestimate the effect size of the non-existent effects or of existent, but unimportantly tiny, effects (Schmidt, <xref ref-type="bibr" rid="B132">1992</xref>, <xref ref-type="bibr" rid="B133">1996</xref>; Sterling et al., <xref ref-type="bibr" rid="B148">1995</xref>; Ioannidis, <xref ref-type="bibr" rid="B71">2008</xref>). These effects may even be confirmed by meta-analyses, because meta-analyses typically are not able to incorporate unpublished negative results (Sterling et al., <xref ref-type="bibr" rid="B148">1995</xref>) and they cannot correct many of the biases that have infiltrated the primary studies. For example, such biases may result in substantial exaggeration of measured effect sizes in meta-analyses (see e.g., Szucs and Ioannidis, <xref ref-type="bibr" rid="B153">2017</xref>).</p>
<p>Given that the predictions of H<sub>1</sub> are rarely precise and that theoretical constructs in many scientific fields (including psychology and cognitive neuroscience) are often poorly defined (Pashler and Harris, <xref ref-type="bibr" rid="B120">2012</xref>), it is easy to claim support for a popular theory with many kinds of data falsifying H<sub>0</sub> even if the constructs measured in many papers are just very weakly linked to the original paper, or not linked at all. Overall, the literature may soon give the impression of a steady stream of replications throughout many years. Even when “negative” results appear, citation bias may still continue to distort the literature and the prevailing theory may continue to be based on the “positive” results. Hence, citation bias may maintain prevailing theories even when they are clearly false and unfounded (Greenberg, <xref ref-type="bibr" rid="B65">2009</xref>).</p>
</sec>
<sec>
<title>NHST does not facilitate systematic knowledge integration</title>
<p>Due to high FRP the contemporary research literature provides statistically significant “evidence” for nearly everything (Schoenfeld and Ioannidis, <xref ref-type="bibr" rid="B134">2012</xref>). Because NHST emphasizes all or none <italic>p</italic>-value based decisions rather than the magnitude of effects, often only <italic>p</italic>-values are reported for critical tests, effect size reports are often missing and interval estimates and confidence intervals are not reported. In an assessment of the entire biomedical literature in 1990–2015, 96% of the papers that used abstracts reported at least some <italic>p</italic>-value below 0.05, while only 4% of a random sample of papers presented consistently effect sizes with confidence intervals (Chavalarias et al., <xref ref-type="bibr" rid="B23">1990-2015</xref>). However, oddly enough, the main NHST “measure of evidence,” the <italic>p</italic>-value cannot be compared across studies. It is a frequent <italic>misconception</italic> that a lower <italic>p</italic>-value always means stronger evidence irrespective of the sample size and effect size (Oakes, <xref ref-type="bibr" rid="B119">1986</xref>; Schmidt, <xref ref-type="bibr" rid="B133">1996</xref>; Nickerson, <xref ref-type="bibr" rid="B114">2000</xref>). Besides the non-comparable <italic>p</italic>-values, NHST does not offer any <italic>formal</italic> mechanism for systematic knowledge accumulation and integration (Schmidt, <xref ref-type="bibr" rid="B133">1996</xref>) unlike Bayesian methods which can take such pre-study information into account. Hence, we end up with many fragmented studies which are most often unable to say anything formal about their favorite H<sub>1</sub>s (accepted in a qualitative manner). Methods do exist for the meta-analysis of <italic>p</italic>-values (see e.g., Cooper et al., <xref ref-type="bibr" rid="B28">2009</xref>) and these are still used in some fields. However, practically such meta-analyses still say nothing about the magnitude of the effect size of the phenomenon being addressed. These methods are potentially acceptable when the question is whether there is any non-null signal among multiple studies that have been performed, e.g., in some types of genetic associations where it is taken for granted that the effect sizes are likely to be small anyhow (Evangelou and Ioannidis, <xref ref-type="bibr" rid="B40">2013</xref>).</p>
</sec>
<sec>
<title>Family-wise error rate (FWER) and FDR correction in NHST</title>
<p>An increasingly important problem is that with the advent of large data sets researchers can use NHST to test multiple, related hypotheses. For example, this problem routinely appears in neuro-imaging where a large amount of non-independent data points are collected and then the same hypothesis test may be run on tens of thousands of observations, for example, from a brain volume, or from 256 electrodes placed on the scalp, each electrode recording voltage 500 times a second. Analysis procedures that generate different views of data (e.g., time-frequency or independent component analyses) may further boost the amount of tests to be run.</p>
<p>Regarding these multiple testing situations, a group of statistical tests which are somehow related to each other can be defined as a “family of comparisons.” The probability that a family of comparisons contains at least one false positive error is called the family wise error rate (FWER). If the repeated tests concern independent data sets where H<sub>0</sub> is true than the probability of having at least one Type I error in k independent tests, each with significance level α, is α<sub>TOTAL</sub> = 1 - (1 - α)<sup>k</sup>. For example if k = 1, 2, 3, 4, 5, and 10 than α<sub>TOTAL</sub> is 5, 9.75, 14.26, 18.55, 22.62, and 40.13%, respectively (see Curran-Everett, <xref ref-type="bibr" rid="B30">2000</xref>; Szucs, <xref ref-type="bibr" rid="B152">2016</xref> for graphical illustrations and simulations for non-independent data).</p>
<p>There are numerous procedures which can take multiple testing into account by correcting <italic>p</italic>-values. The simplest of these procedures is Bonferroni correction which computes an adjusted <italic>p</italic>-value threshold as α/n where α is the statistical significance threshold for a single test and n is the number of tests run. Hence, if we run 5 tests which can be defined as a family of tests and our original α is 0.05 then the Bonferroni corrected adjusted α level is 0.05/5 = 0.01. Any <italic>p</italic>-values above this threshold should not be considered to demonstrate statistically significant effects. Besides the Bonferroni correction there are other alternative methods of FWER correction, like the Tukey Honestly Significant Difference test, the Scheffe test, Holm's method, Sidak's method; Hochberg's method, etc. Some of these corrections also take the dependency (non-independence) of tests into account (see e.g., Shaffer, <xref ref-type="bibr" rid="B137">1995</xref>; Nichols and Hayasaka, <xref ref-type="bibr" rid="B111">2003</xref> for review).</p>
<p>FWER control is a conservative procedure in keeping Type I error rate low but it also sacrifices power increasing Type II error. An alternative to FWER control is False Discovery Rate (FDR) control which allows more Type I errors but assures higher power. Using the same logic as the computation of FRP discussed before, FDR control considers the estimated proportion of false positive statistically significant findings amongst all statistically significant findings (i.e., the proportion of erroneously rejected null hypotheses out of all rejected null hypotheses; Benjamini and Hochberg, <xref ref-type="bibr" rid="B7">1995</xref>). FDR computation is illustrated by Table <xref ref-type="table" rid="T3">3</xref>. If we run M hypothesis tests then a certain number of them are likely to test true null effects (M<sub>0</sub> in Table <xref ref-type="table" rid="T3">3</xref>) and some other number of them are likely to test non-null effects with true alternative hypotheses (M<sub>1</sub> in Table <xref ref-type="table" rid="T3">3</xref>). Depending on our α level and power (1-β), a certain number of the M<sub>0</sub> and M<sub>1</sub> tests will reject the null hypothesis (FP and TN, respectively, see Table <xref ref-type="table" rid="T3">3</xref> for abbreviations) while some other number of them will not reject the null hypothesis (TN and FN, respectively). If we know the exact numbers in Table <xref ref-type="table" rid="T3">3</xref> then the proportion of false positive statistically significant findings can be computed as the ratio of false positive results to all statistically significant results: Q = FP/(FP + TP) = FP/R (assuming that R≠ 0).</p>
<table-wrap id="T3" position="float">
<label>Table 3</label>
<caption>
<p>Illustrating the logic behind FDR computation.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th colspan="1" rowspan="1"></th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>Null hypothesis is true</bold>
</th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>Alternative hypothesis is true</bold>
</th>
<th align="left" colspan="1" rowspan="1" valign="top">
<bold>Sums</bold>
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1" valign="top">H<sub>0</sub> is rejected (statistically significant outcome)</td>
<td align="left" colspan="1" rowspan="1" valign="top">FP = False Positives<break></break>45 (if α = 0.05; 900·0.05 = 45)</td>
<td align="left" colspan="1" rowspan="1" valign="top">TP = True Positives<break></break>60 (if power = 0.6; 100·0.6 = 60)</td>
<td align="left" colspan="1" rowspan="1" valign="top">R<break></break>105</td>
</tr>
<tr style="border-top: thin solid #000000;">
<td align="left" colspan="1" rowspan="1" valign="top">H<sub>0</sub> is not rejected (statistically non-significant outcome)</td>
<td align="left" colspan="1" rowspan="1" valign="top">TN = True Negatives<break></break>855 (if α = 0.05; 900·0.95 = 855)</td>
<td align="left" colspan="1" rowspan="1" valign="top">FN = False Negatives<break></break>40 (if β = 0.4; 100·0.4 = 40)</td>
<td align="left" colspan="1" rowspan="1" valign="top">M – R<break></break>895</td>
</tr>
<tr style="border-top: thin solid #000000;">
<td align="left" colspan="1" rowspan="1" valign="top">Sums</td>
<td align="left" colspan="1" rowspan="1" valign="top">M<sub>0</sub><break></break>900</td>
<td align="left" colspan="1" rowspan="1" valign="top">M<sub>1</sub><break></break>100</td>
<td align="left" colspan="1" rowspan="1" valign="top">M 1000</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<p><italic>H<sub>0</sub> (leftmost column) stands for the null hypothesis. The proportion of false positive statistically significant test outcomes to all statistically significant test outcomes is Q = FP/(FP + TP) = FP/R (R≠0). The numbers give an example for the case when α = 0.05 (so, FWER = α = 0.05) and β = 0.4, so Power = 1 − β = 0.6. In the example we run 1,000 null hypothesis tests. We test 9 times as many true null situations than situations with true alternative hypotheses (that is, every 10th of our experimental ideas are correct). In this case Q = 45/105 = 0.4286. That is, 42.86% of statistically significant results will be false positives. FDR = Q · pr(R &gt; 0) = Q · [α · (M<sub>0</sub>/M) + Power · (M<sub>1</sub>/M)] = 0.045. If we only test true null effects then Q = false positives/all significant results = α·M / α·M = 1; and FDR = Q · pr(R&gt;0) = Q · α = α = FWER [pr(R&gt;0) = α because all significant results are coming from true null situations]. Note that in real research only R, M, and M-R are known whereas M<sub>0</sub>, M<sub>1</sub>, and Q are not known</italic>.</p>
</table-wrap-foot>
</table-wrap>
<p>Of course, in real research settings we do not know how many of our tests test true null effects and we only know how many tests we run and how many of them return statistically significant and non-significant results. So Q can be considered a random variable. However, as Q cannot be controlled directly FDR is defined as the expected value of the proportion of false positive errors: FDR = E[FP/R|R &gt; 0] · pr(R &gt; 0), a variable which can be controlled (see Benjamini and Hochberg, <xref ref-type="bibr" rid="B7">1995</xref>; Curran-Everett, <xref ref-type="bibr" rid="B30">2000</xref>; Nichols and Hayasaka, <xref ref-type="bibr" rid="B111">2003</xref>; Bennett et al., <xref ref-type="bibr" rid="B9">2009</xref>; Benjamini, <xref ref-type="bibr" rid="B6">2010</xref>; Goeman and Solari, <xref ref-type="bibr" rid="B61">2014</xref>). Some FDR estimation procedures can also factor in dependency between tests (Benjamini and Yekutieli, <xref ref-type="bibr" rid="B8">2001</xref>).</p>
<p>In contrast to FDR, using the notation in Table <xref ref-type="table" rid="T3">3</xref>, FWER can be expressed as FWER = pr(FP≥1) = 1 − pr(FP = 0) that is, the probability that there is at least one false positive Type I error in a family of observations. If the null hypothesis is true in all tests we run then FDR = FWER while if there are situations with true alternative hypotheses then FDR &lt; FWER (see Table <xref ref-type="table" rid="T3">3</xref> for example). Also, various other FDR and FWER measures can be derived (see the above cited reviews). It can be argued that controlling FDR is more useful in research where a very large number of tests are carried out routinely, like neuro-imaging or genetics but less useful in behavioral psychological and social science research where fewer hypotheses may be tested at any one time and accepting any single hypothesis as statistically significant may have large impact on inferences (Gelman et al., <xref ref-type="bibr" rid="B49">2012</xref>). This last statement is also true for behavioral data used to support the interpretation of neuro-imaging findings.</p>
<p>Most relevant to our paper, both FWER and FDR error rate corrections are based on the same NHST procedure. That is, they do not modify the procedure in any ways other than aiming to decrease Type I error toward initially expected levels when multiple NHST tests are run. That is, these methods can help in constraining the number of random findings when a single hypothesis is tested simultaneously in many data points (e.g., voxels) but do nothing to protect against many of the other problems discussed in this paper (e.g., generating a high amount of false positives across the literature; being sensitive to undocumented biasing procedures; neglecting predictions under H<sub>1</sub>; not providing probability statements for H<sub>1</sub>; neglecting pre-data probabilities; being unable to effectively integrate study results). These problems are valid even when just one single NHST test is run. In addition, empirical analyses of large fMRI data sets found that the most popular fMRI analysis software packages implemented erroneous multiple testing corrections and hence, generate much higher levels of false positive results than expected (Eklund et al., <xref ref-type="bibr" rid="B37">2012</xref>, <xref ref-type="bibr" rid="B38">2016</xref>). This casts doubts on a substantial part of the published fMRI literature. Further, Carp (<xref ref-type="bibr" rid="B17">2012</xref>) reported that about 40% of 241 relatively recent fMRI papers actually did not report having used multiple testing correction. So, a very high percentage of fMRI literature may have been exposed to high false positive rates either multiple correction was used or not (see also (Szucs and Ioannidis, <xref ref-type="bibr" rid="B153">2017</xref>) on statistical power).</p>
<p>In the NHST framework the multiple comparison problem is exacerbated by the fact that we may test a very large number of precise null hypotheses (Neath and Cavanaugh, <xref ref-type="bibr" rid="B108">2006</xref>), often without much theoretical justification (e.g., in many explorative whole brain analyses). However, as the H<sub>0</sub>:H<sub>1</sub> odds may be high the NHST mechanism may produce a very large number of falsely significant results. Similarly high numbers of false alarms are produced under realistic conditions even if some rudimentary model is used for the data (e.g., expecting positive or negative difference between conditions; Gelman and Tuerlinckx, <xref ref-type="bibr" rid="B51">2000</xref>). In contrast, while currently there is no standard way to correct for multiple comparisons with Bayesian methods, Bayesian methods have been shown to be more conservative than NHST in some situations (Gelman and Tuerlinckx, <xref ref-type="bibr" rid="B51">2000</xref>) and they offer various methods for correcting for multiple comparisons (e.g., Westfall et al., <xref ref-type="bibr" rid="B161">1997</xref>; Gelman et al., <xref ref-type="bibr" rid="B48">2014</xref>). In addition, Bayesian methods are strongly intertwined with explicit model specifications. These models can then be used to generate simulated data and to study model response behavior. This may offer a way to judge the reasonableness of analyses offering richer information than NHST accept/reject decisions (Gelman et al., <xref ref-type="bibr" rid="B48">2014</xref>). The challenge of course is the development of models. However, below we argue that efficient model development can only happen if we refocus our efforts on understanding data patters from the testing of often very vaguely defined hypotheses. In addition, Bayesian methods are also able to formally aggregate data from many experiments (e.g., adding data serially and by hierarchical models; Gelman et al., <xref ref-type="bibr" rid="B48">2014</xref>). This can further maximize large-scale joint efforts for better model specifications.</p>
</sec>
</sec>
<sec id="s5">
<title>The state of the art must change</title>
<sec>
<title>NHST is unsuitable as the cornerstone of scientific inquiry in most fields</title>
<p>In summary, NHST provides <italic>the illusion of certainty</italic> through supposedly ‘objective’ binary accept/reject decisions (Cohen, <xref ref-type="bibr" rid="B27">1994</xref>; Ioannidis, <xref ref-type="bibr" rid="B75">2012</xref>) based on practically not very useful <italic>p</italic>-values (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>). However, researchers usually never give any formal assessment of how well their theory (a specific H<sub>1</sub>) fits the facts and, instead of gradual model building (Gigerenzer, <xref ref-type="bibr" rid="B53">1998</xref>) and comparing the plausibility of theories, they can get away with destroying a strawman: they disprove an H<sub>0</sub> (which happens inevitably sooner or later) with a machinery biased to disproving it without ever going into much detail about the <italic>exact</italic> behavior of variables under <italic>exactly</italic> specified hypotheses (Kranz, <xref ref-type="bibr" rid="B90">1999</xref>; Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>). NHST also does not allow for systematic knowledge accumulation. In addition, both because of its shortcomings and because it is subject to major misunderstandings it facilitates the production of non-replicable false positive reports. Such reports ultimately erode scientific credibility and result in wasting perhaps most of the research funding in some areas (Ioannidis, <xref ref-type="bibr" rid="B76">2005</xref>; Macleod et al., <xref ref-type="bibr" rid="B98">2014</xref>; Kaplan and Irvin, <xref ref-type="bibr" rid="B83">2015</xref>; Nosek et al., <xref ref-type="bibr" rid="B115">2015</xref>).</p>
<p>NHST seems to dominate biomedical research for various reasons. First, it allows for the easy production of a large number of publishable papers (irrespective of their truth value) providing a response to publication pressure. Second, NHST seems deceptively simple: because the burden of inference (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>) has been delegated to the significance test all too often researchers' statistical world view is narrowed to checking an inequality: is <italic>p</italic> ≤ 0.05 (Cohen, <xref ref-type="bibr" rid="B27">1994</xref>)? After passing this test, an observation can become a “scientific fact” contradicting the random nature of statistical inference (Gelman, <xref ref-type="bibr" rid="B47">2015</xref>). Third, in biomedical and social science NHST is often falsely perceived as the <italic>single</italic> objective approach to scientific inference (Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>) and alternatives are simply not taught and/or understood.</p>
<p>We have now decades of negative experience with NHST which gradually achieved dominance in biomedical and social science since the 1930s (Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>). Critique of NHST started not much later (Jeffreys, <xref ref-type="bibr" rid="B81">1939, 1948, 1961</xref>) and has been forcefully present since then (Jeffreys, <xref ref-type="bibr" rid="B81">1939, 1948, 1961</xref>; Eysenck, <xref ref-type="bibr" rid="B41">1960</xref>; Nunnally, <xref ref-type="bibr" rid="B118">1960</xref>; Rozeboom, <xref ref-type="bibr" rid="B131">1960</xref>; Clark, <xref ref-type="bibr" rid="B24">1963</xref>; Bakan, <xref ref-type="bibr" rid="B2">1966</xref>; Meehl, <xref ref-type="bibr" rid="B101">1967</xref>; Lykken, <xref ref-type="bibr" rid="B96">1968</xref>) and continues to-date (Wasserstein and Lazar, <xref ref-type="bibr" rid="B159">2016</xref>). The problems are numerous, and as Edwards (<xref ref-type="bibr" rid="B36">1972</xref>, p. 179) concluded 44 years ago: “<italic>any method which invites the contemplation of a null hypothesis is open to grave misuse, or even abuse</italic>.” Time has proven this statement and that problems are unlikely to go away. We suggest that that it is <italic>really</italic> time for change now.</p>
</sec>
<sec>
<title>When and how to use NHST</title>
<p>Importantly, we do not want to ban NHST (Hunter, <xref ref-type="bibr" rid="B70">1997</xref>), we realize that it may be reasonable to use it in some well-justified cases. In all cases when NHST is used its use must be justified clearly rather than used as an automatic default and single cornerstone procedure. On the one hand, NHST can be used when very precise quantitative theoretical predictions can be tested, hence, both power and effect size can be estimated well as intended by Neyman and Pearson (<xref ref-type="bibr" rid="B110">1933</xref>). On the other hand, when theoretical predictions are not precise, reasonably powered NHST tests may be used as an initial heuristic look at the data as Fisher (<xref ref-type="bibr" rid="B44">1925</xref>) intended. However, in these cases (when well-justified theoretical predictions are lacking) if studies are not pre-registered (see below) NHST tests can only be considered preliminary (exploratory) heuristics. Hence, their findings should only be taken seriously if they are replicated, optimally within the same paper (Nosek et al., <xref ref-type="bibr" rid="B116">2013</xref>). These replications must be well powered to keep FRP low. As discussed, NHST can only reject H<sub>0</sub> and can accept neither a generic or specific H<sub>1</sub>. So, on its own NHST cannot provide evidence “for” something even if findings are replicated.</p>
<p>For example, if initially researches do not know where to expect experimental effects in a particular experimental task, they could run a whole brain, multiple-testing corrected search for statistical significance in a group of participants. Such a search would provide heuristic evidence if they identify some brain areas reacting to manipulations. In order to confirm these effects they would need to carefully study for example the BOLD signal or EEG amplitude changes in areas or over electrodes of interest, make predictions about the behavior of these variables, replicate measurements and minimally confirm the previous NHST results before the findings can be taken seriously. Much better, if researchers can also provide some model for the behavior of their variables, make model predictions and then confirm these with likelihood-based and/or Bayesian methods. Making such predictions would probably require intimate familiarity with a lot of raw data.</p>
</sec>
<sec>
<title>Ways to change</title>
<p>In most biomedical, neuroscience, psychology, and social science fields currently popular analysis methods are based on NHST. It is clear that analysis software and researcher knowledge cannot be changed overnight. Below we summarize some further recommendations which we think can minimize the negative features of NHST even if it continues to be dominant for a while. A very important practical goal would be to change the incentive structure of biomedical and social science to bring it in line with these and similar other recommendations (Wagenmakers et al., <xref ref-type="bibr" rid="B157">2011</xref>; Begley and Ellis, <xref ref-type="bibr" rid="B5">2012</xref>; Nosek et al., <xref ref-type="bibr" rid="B116">2013</xref>; Stodden et al., <xref ref-type="bibr" rid="B151">2016</xref>). Also note that we are not arguing against statistical inference which we consider the “logic of science” (Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; p. xxii.), quantitative and well justified statistical inference should be at the <italic>core</italic> of the scientific enterprise.</p>
<sec>
<title>If theory is weak, focus on raw data, estimating effect sizes, and their uncertainty</title>
<p>The currently dominant, NHST influenced approach is that instead of understanding raw data researchers often just focus on the all or nothing rejection of a vaguely defined H<sub>0</sub> and shift their attention to interpreting brain “activations” revealed by potentially highly misleading statistical parameter maps. Based on these maps then strong (qualitative) claims may be made about alternative theories whose support may in fact never be tested. So, current approaches seem to reward exuberant theory building based on small and underpowered studies (Szucs and Ioannidis, <xref ref-type="bibr" rid="B153">2017</xref>) much more than meticulous data collection and understanding and modeling extensive raw data patterns. For analogy, in astronomy theories typically built on thousands of years of sky observation data open to everyone. For example, Kepler could identify the correct laws of planetary motion because he had access to the large volume of observational data accumulated by Tycho Brache who devoted decades of his life to much more precise data collection than previously done. Similarly, the crucial tests of Einstein's theories were precise predictions about data which could be verified or falsified (Smolin, <xref ref-type="bibr" rid="B145">2006</xref>; Chaisson and McMillan, <xref ref-type="bibr" rid="B21">2017</xref>).</p>
<p>Overall, if we just consider competing “theories” without ever deeply considering extensive raw data patterns it is unlikely that major robust scientific breakthroughs will be done whereas many different plausible looking theories can be promoted. Imagine, for example, a situation where astronomers would have only published the outcomes of their NHST tests, some rejecting that the sun is in the middle of the universe while others rejecting that the earth is in the middle of the universe while publishing no actual raw data. Meta-analyses of published effect sizes would have confirmed both positions as both camps would have only published test statistics which passed the statistical significance threshold. Luckily, real astronomers recorded a lot of data and derived testable theories with precise predictions.</p>
<p>In basic biomedical and psychology research we often cannot provide very well worked out hypotheses and even a simple directional hypothesis may seem particularly enlightening. Such rudimentary state of knowledge can be respected. However, in such pre-hypothesis stage substantively blind all or nothing accept/reject decisions may be unhelpful and may maintain our ignorance rather than facilitate organizing new information into proper quantitative scientific models. It is much more meaningful to focus on assessing the magnitude of effects along with estimates of uncertainty, let these be error terms, confidence intervals or Bayesian credible intervals (Edwards, <xref ref-type="bibr" rid="B36">1972</xref>; Luce, <xref ref-type="bibr" rid="B95">1988</xref>; Schmidt, <xref ref-type="bibr" rid="B133">1996</xref>; Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; Gelman, <xref ref-type="bibr" rid="B45">2013a</xref>,<xref ref-type="bibr" rid="B46">b</xref>; see Morey et al., <xref ref-type="bibr" rid="B106">2016</xref> on the difference between classical confidence intervals and Bayesian credible intervals). These provide more direct information on the actual “empirical” behavior of our variables and/or the precision of interval estimation. Gaining enough experience with interval estimates and assuring their robustness by building replication into design (Nosek et al., <xref ref-type="bibr" rid="B116">2013</xref>) may then allow us to describe the behavior of variables by more and more precise scientific models which may provide more clear predictions (Schmidt, <xref ref-type="bibr" rid="B133">1996</xref>; Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; Gelman, <xref ref-type="bibr" rid="B45">2013a</xref>,<xref ref-type="bibr" rid="B46">b</xref>).</p>
<p>The above problem does not only concern perceived “soft areas” of science where measurement, predictions, control and quantification are thought to be less rigorous than in “hard” areas (Meehl, <xref ref-type="bibr" rid="B102">1978</xref>). In many fields, for example, in cognitive neuroscience, the measurement methods may be “hard” but theoretical predictions and analysis often may be just as “soft” as in any area of “soft” psychology: Using a state of the art fMRI scanner for data collection and novel but extremely complicated and often not well understood analysis paths will not make a badly defined theory well-defined.</p>
<p>The change of emphasis suggested here would require that instead of <italic>p</italic>-values and reporting the outcomes of all/nothing hypothesis tests studies should focus on reporting data in original units of measurement as well as providing derived effect sizes. It is important to publish data summaries (means, standard errors, nowadays extremely rarely plotted empirical data distributions) in original units of measurement as derived measures may be highly biased by some (undocumented) analysis techniques. If we have clear and pre-registered hypotheses then it is relatively straightforward to publish raw data summaries (e.g., mean BOLD signal or ERP amplitude change with standard errors) related to those hypotheses. Clear data presentation usually gets difficult when there are lots of incidental findings. Usually unlimited amount of data summaries can now be published cost free in Supplementary Materials.</p>
</sec>
<sec>
<title>Pre-registration</title>
<p>In our view one of the most important and virtually cost-free (to researchers) improvement would be to pre-register hypotheses and analysis parameters and approaches (in line with Section 5.2 in Nichols et al., <xref ref-type="bibr" rid="B112">2016</xref>; p11; Gelman and Loken, <xref ref-type="bibr" rid="B50">2014</xref>). Pre-registration can easily be done for example, at the website of the Open Science Foundation (osf.org), also in a manner that it does not immediately become public. Hence, competitors will not be able to scoop good ideas before the study is published. Considering the extreme analysis flexibility offered by high-dimensional neuroscience data (Kriegeskorte et al., <xref ref-type="bibr" rid="B91">2009</xref>; Vul et al., <xref ref-type="bibr" rid="B155">2009</xref>; Carp, <xref ref-type="bibr" rid="B17">2012</xref>) pre-registration seems a necessary pre-condition of robust hypothesis driven neuroscience research. Pre-registration would likely help to cleanse non-replicable “unchallenged fallacies” (Ioannidis, <xref ref-type="bibr" rid="B75">2012</xref>) from the literature. For example, Kaplan and Irvin (<xref ref-type="bibr" rid="B83">2015</xref>) found that pre-registering the primary hypotheses of clinical studies decreased the proportion of positive findings from 57% (17 of 30 studies) to 8% (2 out of 25 studies). Hence, another benefit of pre-registration would be to decrease publication volumes. This would require changing the incentive system motivating scientists (Nosek et al., <xref ref-type="bibr" rid="B116">2013</xref>).</p>
<p>It is to note that honesty regarding pre-registration and challenging questionable research practices (Simmons et al., <xref ref-type="bibr" rid="B138">2011</xref>) is the shared responsibility of all co-authors. Some fields in medical research have already over 10 years of experience with pre-registration. This experience shows that pre-registration needs to be thorough to be reliable. For example, many observational studies claimed to have been registered but closer scrutiny shows that registration has actually happened after the study/analysis was done (Boccia et al., <xref ref-type="bibr" rid="B14">2016</xref>). In other cases, clinical trials may be seemingly properly pre-registered before they start recruiting patients, but analyses and outcomes were still manipulated after registration (Ioannidis et al., <xref ref-type="bibr" rid="B72">2017</xref>). Hence, proper safeguards should be put in place to ensure that scientists are accountable for any misconduct regarding breaching pre-registration rules.</p>
</sec>
<sec>
<title>Publish all analysis scripts with analysis settings</title>
<p>Another cost-free improvement is to publish all analysis scripts with the ability to regenerate all figures and tables (Laine et al., <xref ref-type="bibr" rid="B92">2007</xref>; Peng, <xref ref-type="bibr" rid="B125">2009</xref>, <xref ref-type="bibr" rid="B126">2011</xref>; Diggle and Zeger, <xref ref-type="bibr" rid="B33">2010</xref>; Keiding, <xref ref-type="bibr" rid="B85">2010</xref>; Doshi et al., <xref ref-type="bibr" rid="B35">2013</xref>). This does not require large storage space and can also be done in Supplementary Material. If researchers keep this expectation in mind from the start of a project then implementing it becomes relatively straightforward. Program code will often provide information which is missing from papers. With regard to missing analysis information it is important to be conscious of the fact that seemingly innocuous and irrelevant analysis settings (e.g., slightly changing initial filtering parameters) can have major impact on final statistical outcomes at the end of a complicated processing pipeline. For example, modified initial settings may change the statistically significant/non-significant status of final important test statistics. This can be an issue if multiple settings can be justified and/or if some settings leading to significant outcomes are actually less justified than alternative settings. Publishing scripts will also provide more information on potential statistical errors (Bakker and Wicherts, <xref ref-type="bibr" rid="B3">2001</xref>; Nuijten et al., <xref ref-type="bibr" rid="B117">2016</xref>).</p>
<p>It is to note that in-house analysis scripts may provide substantial competitive advantage to researchers who are able to programme these. Hence, the unconditional release of these scripts may deprive researchers from an important competitive asset. In such cases in house scripts could be documented in brief methods papers which could be cited when the relevant scripts are used so that researchers benefit from citations. Perhaps specialized methods repository journals could be set up for this purpose. For some recent recommendations for improving computational reproducibility practices see Stodden et al. (<xref ref-type="bibr" rid="B151">2016</xref>).</p>
</sec>
<sec>
<title>Publish raw data</title>
<p>In an ideal world researchers should publish all raw data. This is easy with small volumes of behavioral data but it has serious monetary and time investment costs with large neural data volumes (see also Nichols et al., <xref ref-type="bibr" rid="B113">2017</xref>). Some repositories have already been set up and it is important that funders cover these costs and optimally provide infrastructure (see Pernet and Poline, <xref ref-type="bibr" rid="B127">2015</xref>; Nichols et al., <xref ref-type="bibr" rid="B113">2017</xref>). Incentives such as a badge system may help promote availability of more raw data (Nosek et al., <xref ref-type="bibr" rid="B115">2015</xref>). In our opinion it is important to publish unprocessed <italic>raw</italic> data because processed data may already have been distorted/biased in undocumented ways. In general, it is more and more usual to reanalyse data from large repositories, so much further development can be expected in this area (e.g., Eklund et al., <xref ref-type="bibr" rid="B37">2012</xref>).</p>
</sec>
<sec>
<title>Publish data (summaries) irrespective of statistical significance, promote building good quality datasets including large replication studies</title>
<p>It is important to publish data summaries and/or data sets, including the ones not resulting in statistically significant findings. Without these datasets true effect sizes simply cannot be determined. This will require that these datasets become citeable so that their authors can be rewarded if data is used for secondary analyses. Considering for example the above mentioned case of Tycho Brache it is clear that his data collection exercise was a necessary precondition of crowning the Copernican/Newtonian revolution of astronomy (Chaisson and McMillan, <xref ref-type="bibr" rid="B21">2017</xref>). Hence, we should be able to reward the mere collection of large volumes of good quality data: such activity can prove to be an immense service to the whole profession. Initiatives, like registered multi-lab replication studies should also be prioritized when the validity of important proposals is at stake. Funders are currently often reluctant to fund such studies. However, they should realize that the continuous seeking of new results and theories may just waste most of their resources (Ioannidis et al., <xref ref-type="bibr" rid="B77">2014</xref>; Kaplan and Irvin, <xref ref-type="bibr" rid="B83">2015</xref>).</p>
</sec>
<sec>
<title>Increase statistical power and publish pre-study power calculations</title>
<p>In real world research it is usually impossible to determine the statistical power of NHST tests exactly. However, if raw data summaries and/or raw data is published irrespective of statistical significance (Section Pre-registration, Publish Raw Data, and Publish Data (Summaries) Irrespective of Statistical Significance, Promote Building Good Quality Datasets Including Large Replication Studies) then we have a much better chance of trying to determine power. Another option is to determine power to detect pre-defined standardized effect sizes. In any case, power in psychology and neuroscience should be much higher than what it is nowadays (Szucs and Ioannidis, <xref ref-type="bibr" rid="B153">2017</xref>). We hypothesize that pre-registration would facilitate increasing power because researchers could less expect to rely on incidentally finding something statistically significant to report from their studies. Hence, they would have more interest in assuring that they are able to respond their primary, registered hypotheses.</p>
</sec>
<sec>
<title>Better training and better use of more statistical methods: from believers to thinkers</title>
<p>A core problem seems to be that the statistical subject knowledge of many researchers in biomedical and social science has been shown to be poor (Oakes, <xref ref-type="bibr" rid="B119">1986</xref>; Gliner et al., <xref ref-type="bibr" rid="B59">2002</xref>; Castro Sotos et al., <xref ref-type="bibr" rid="B19">2007</xref>, <xref ref-type="bibr" rid="B20">2009</xref>; Wilkerson and Olson, <xref ref-type="bibr" rid="B162">2010</xref>; Hoekstra et al., <xref ref-type="bibr" rid="B67">2014</xref>). NHST perfectly fits with poor understanding because of the perceived simplicity of interpreting its outcome: is <italic>p</italic> ≤ 0.05 (Cohen, <xref ref-type="bibr" rid="B27">1994</xref>)?</p>
<p>We suggest that the weak statistical understanding is probably due to inadequate “statistics lite” education. This approach does not build up appropriate mathematical fundamentals and does not provide scientifically rigorous introduction into statistics. Hence, students' knowledge may remain imprecise, patchy, and prone to serious misunderstandings. What this approach achieves, however, is providing students with false confidence of being able to use inferential tools whereas <italic>they usually only interpret the p-value provided by black box statistical software</italic>. While this educational problem remains unaddressed, poor statistical practices will prevail regardless of what procedures and measures may be favored and/or banned by editorials.</p>
<p>All too often statistical understanding is perceived as something external to the subject matter of substantive research. However, it is important to see that statistical understanding influences most decisions about substantive questions, because it underlies the <italic>thinking</italic> of researchers even if this remains <italic>implicit</italic>. While common sense “statistics” may be able to cope with simple situations, common sense is not enough to decipher scientific puzzles involving dozens, hundreds, or even thousands of interrelated variables. In such cases well justified applications of probability theory are necessary (Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>). Hence, instead of delegating their judgment to “automatized” but ultimately spurious decision mechanisms, researchers should have confidence in their own <italic>informed judgment</italic> when they make an inference. Such confidence requires deep study.</p>
<p>Understanding probability is difficult. Common sense is notoriously weak in understanding phenomena based on probabilities (Gigerenzer et al., <xref ref-type="bibr" rid="B55">2005</xref>). We cannot assume that without proper training biomedical and social science graduates would get miraculously enlightened about probability. Some of the best symbolic thinking minds of humanity devoted hundreds of years to the proper understanding of probability and statisticians still do not agree on how best to draw statistical inference (Stigler, <xref ref-type="bibr" rid="B150">1986</xref>; Gigerenzer et al., <xref ref-type="bibr" rid="B58">1989</xref>), e.g., the recent American Statistical Association statement on <italic>p</italic>-values (Wasserstein and Lazar, <xref ref-type="bibr" rid="B159">2016</xref>) was accompanied by 21 editorials from the statisticians and methodologists who participated in crafting it and who disagreed in different aspects among themselves.</p>
<p>There is no reason to assume that understanding twenty first and twenty second century science will require less mathematical and statistical understanding than before. Such as there is no royal road to mathematics, there is no royal road to statistics which is heavily based on mathematics. If statistical understanding does not improve it will not matter whether editorials enforce bootstrapping, likelihood estimation or Bayesian approaches, they will all remain opaque to the untrained mind and open to abuse such as the NHST of the twentieth century.</p>
<p>One approach would be to phase out the ‘statistics lite’ education approach for all research stream students and teach statistics rigorously. A typical research stream undergraduate training could include, for example, 3–4 semesters of calculus, one semester of introductory statistics, three more semesters of calculus based statistics, and then finally two semesters of more specialized statistics. An alternative and/or complementary approach would be to enhance the training of professional applied statisticians and to ensure that all research involves knowledgeable statisticians or equivalent methodologists. At a minimum, all scientists should be well trained in understanding evidence and statistics and being in a position to recognize that they may need help from a methodologist expert (Marusic and Marusic, <xref ref-type="bibr" rid="B100">2003</xref>; Moharari et al., <xref ref-type="bibr" rid="B105">2009</xref>; Vujaklija et al., <xref ref-type="bibr" rid="B154">2010</xref>).</p>
</sec>
<sec>
<title>Teach alternative approaches seriously</title>
<p>It is important that researchers are conscious that NHST only represents a small segment of available statistical techniques. Besides NHST, Bayesian and likelihood based approaches should also be taught, with explanation of the strengths and weaknesses of each inferential method. Hypotheses could be tested by either likelihood ratio testing, and/or Bayesian methods which usually view probability as characterizing the state of our beliefs about the world (Pearl, <xref ref-type="bibr" rid="B124">1988</xref>; Jaynes, <xref ref-type="bibr" rid="B80">2003</xref>; MacKay, <xref ref-type="bibr" rid="B97">2003</xref>; Sivia and Skilling, <xref ref-type="bibr" rid="B143">2006</xref>; Gelman et al., <xref ref-type="bibr" rid="B48">2014</xref>; for neuroscience data see e.g., Lorenz et al., <xref ref-type="bibr" rid="B94">2017</xref>). The above alternative approaches typically require model specifications about alternative hypotheses, they can give probability statements about H<sub>0</sub> and alternative hypotheses, they allow for clear model comparison, are insensitive to data collection procedures and do not suffer from problems with large samples. In addition, Bayesian methods can also factor in pre-study (prior) information into model evaluations which may be important for integrating current and previous research findings. Hence, the above alternative approaches seem more suitable for the purpose of scientific inquiry than NHST and ample literature is available on both. The problem is that usually none of these alternative approaches are taught properly in statistics courses for students in psychology, neuroscience, biomedical science and social science. For example, across 1,000 abstracts randomly selected from the biomedical literature of 1990–2015, none reported results in a Bayesian framework (Chavalarias et al., <xref ref-type="bibr" rid="B23">1990-2015</xref>).</p>
<p>It is important to note that Bayesian methods are often accused of subjectivity because they can take prior information into account. However, Bayesian methods are able to consider prior expectations formally and explicitly in their models provided that necessary information (e.g., raw data and/or extensive reporting of data parameters) from previous studies is available. In contrast, as we have discussed NHST can be latently biased by subjectivity at many points without ever revealing any of the biases. In contrast, different reasonable Bayesian priors can be implemented and their impact on outcomes can be debated explicitly and ultimately, the goodness of model predictions can be tested. Hence, we do not see the use of Bayesian priors as a drawback. Rather, explicit priors can represent a strength as they allow for formal knowledge integration from previous studies.</p>
</sec>
</sec>
<sec>
<title>There is no automatic inference: new-old dangers ahead?</title>
<p>Perhaps the most worrisome false belief about statistics is the belief in automatic statistical inference (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>; Gigerenzer and Marewski, <xref ref-type="bibr" rid="B57">1998</xref>), the illusion that plugging in some numbers into some black box algorithm will give a number (perhaps the <italic>p</italic>-value or some other metric) that conclusively proves or disproves hypotheses (Bakan, <xref ref-type="bibr" rid="B2">1966</xref>). There is no reason to assume that any kind of “new statistics” (Cumming, <xref ref-type="bibr" rid="B29">2014</xref>) will not suffer the fate of NHST if statistical understanding is inadequate. For example, it has been shown that confidence intervals are misinterpreted just as badly as <italic>p</italic>-values by undergraduates, graduates, and researchers alike and self-declared statistical experience even slightly positively correlates with the number of errors (Hoekstra et al., <xref ref-type="bibr" rid="B67">2014</xref>). Or, many times black box machine learning algorithms may be run uncritically and/or on relatively small data volumes. However, the more complex is a dataset the more chance such substantively blind search algorithms have to find some relationships where nothing worthy of mention exist. So, uncritical applications are likely to further boost the proportion of false positive findings irrespective of the sophistication of the algorithms (Skokic et al., <xref ref-type="bibr" rid="B144">2016</xref>). Similarly, the proper use of Bayesian methods may require use of advanced simulation methods and a clear understanding and justification of probability distribution models. In contrast to this, it is frequent to see a kind of “automatic” determination of Bayes factors or posterior estimates, again, provided by black box statistical packages which again, promise to take the load of thinking off the shoulders of researchers.</p>
</sec>
</sec>
<sec id="s6">
<title>Author note</title>
<p>A previous version of this manuscript was available as a pre-print (<ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/content/early/2016/12/20/095570">http://biorxiv.org/content/early/2016/12/20/095570</ext-link>). The copyright holder for this preprint is the author/funder. It is made available under a CC-BY-NC-ND 4.0 International license.</p>
</sec>
<sec id="s7">
<title>Author contributions</title>
<p>DS wrote the first draft of the manuscript. DS and JI revised successive drafts.</p>
<sec>
<title>Conflict of interest statement</title>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>DS is supported by a twenty-first Century Science Initiative in Understanding Human Cognition Scholar Award (220020370) from the James S. McDonnell Foundation. METRICS is supported from a grant from the Laura and John Arnold Foundation.</p>
</ack>
<sec id="s8" sec-type="supplementary-material">
<title>Supplementary material</title>
<p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnhum.2017.00390/full#supplementary-material">http://journal.frontiersin.org/article/10.3389/fnhum.2017.00390/full#supplementary-material</ext-link></p>
<supplementary-material content-type="local-data" id="SM1">
<media xlink:href="Presentation1.PDF">
<caption>
<p>Click here for additional data file.</p>
</caption>
</media>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="B1">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aarts</surname><given-names>A. A.</given-names></name><name><surname>Anderson</surname><given-names>J. E.</given-names></name><name><surname>Anderson</surname><given-names>C. J.</given-names></name><name><surname>Attridge</surname><given-names>P. R.</given-names></name><name><surname>Attwood</surname><given-names>A.</given-names></name><name><surname>Axt</surname><given-names>J.</given-names></name><etal></etal></person-group> (<year>2015</year>). <article-title>Estimating the reproducibility of psychological science</article-title>. <source/>Science
<volume>349</volume>, <fpage>943</fpage>
<pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id></mixed-citation>
</ref>
<ref id="B2">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakan</surname><given-names>D.</given-names></name></person-group> (<year>1966</year>). <article-title>The test of significance in psychological research</article-title>. <source/>Psychol. Bull.
<volume>66</volume>, <fpage>423</fpage>–<lpage>437</lpage>. <pub-id pub-id-type="doi">10.1037/h0020412</pub-id><pub-id pub-id-type="pmid">5974619</pub-id></mixed-citation>
</ref>
<ref id="B3">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname><given-names>M.</given-names></name><name><surname>Wicherts</surname><given-names>J. M.</given-names></name></person-group> (<year>2001</year>). <article-title>The misreporting of statistical results in psychology journals</article-title>. <source/>Behav. Res. Methods
<volume>43</volume>, <fpage>666</fpage>–<lpage>678</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-011-0089-5</pub-id></mixed-citation>
</ref>
<ref id="B4">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayarri</surname><given-names>M. J.</given-names></name><name><surname>Benjamin</surname><given-names>D. J.</given-names></name><name><surname>Berger</surname><given-names>J. O.</given-names></name><name><surname>Sellke</surname><given-names>T. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Rejection odds and rejection ratios: a proposal for statistical practice in testing hypotheses</article-title>. <source/>J. Math. Psychol.
<volume>72</volume>, <fpage>90</fpage>–<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmp.2015.12.007</pub-id></mixed-citation>
</ref>
<ref id="B5">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begley</surname><given-names>C. G.</given-names></name><name><surname>Ellis</surname><given-names>L. M.</given-names></name></person-group> (<year>2012</year>). <article-title>Raise standards for preclinical cancer research</article-title>. <source/>Nature
<volume>483</volume>, <fpage>531</fpage>–<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1038/483531a</pub-id><pub-id pub-id-type="pmid">22460880</pub-id></mixed-citation>
</ref>
<ref id="B6">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y.</given-names></name></person-group> (<year>2010</year>). <article-title>Simulataneous and selective inference: current successes and future challenges</article-title>. <source/>Biometr. J.
<volume>52</volume>, <fpage>708</fpage>–<lpage>721</lpage>. <pub-id pub-id-type="doi">10.1002/bimj.200900299</pub-id><pub-id pub-id-type="pmid">21154895</pub-id></mixed-citation>
</ref>
<ref id="B7">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y.</given-names></name><name><surname>Hochberg</surname><given-names>Y.</given-names></name></person-group> (<year>1995</year>). <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source/>R. Statist. Soc. B
<volume>57</volume>, <fpage>89</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="B8">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y.</given-names></name><name><surname>Yekutieli</surname><given-names>D.</given-names></name></person-group> (<year>2001</year>). <article-title>The control of false discovery rate in multiple testing under dependency</article-title>. <source/>Ann. Stat.
<volume>29</volume>, <fpage>1165</fpage>–<lpage>1188</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/2674075">http://www.jstor.org/stable/2674075</ext-link></mixed-citation>
</ref>
<ref id="B9">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>C. M.</given-names></name><name><surname>Wolford</surname><given-names>G. L.</given-names></name><name><surname>Miller</surname><given-names>M. B.</given-names></name></person-group> (<year>2009</year>). <article-title>The principled control of false positives in neuroimaging</article-title>. <source/>Soc. Cogn. Affect. Neurosci.
<volume>4</volume>, <fpage>417</fpage>–<lpage>442</lpage>. <pub-id pub-id-type="doi">10.1093/scan/nsp053</pub-id><pub-id pub-id-type="pmid">20042432</pub-id></mixed-citation>
</ref>
<ref id="B10">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>J. O.</given-names></name></person-group> (<year>1985</year>). <source/>Statistical Decision Theory and Bayesian Analysis, 2nd Edition. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>
<pub-id pub-id-type="doi">10.1007/978-1-4757-4286-2</pub-id></mixed-citation>
</ref>
<ref id="B11">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>J. O.</given-names></name><name><surname>Delampady</surname><given-names>M.</given-names></name></person-group> (<year>1987</year>). <article-title>Testing precise hypothesis</article-title>. <source/>Stat. Sci. <volume>2</volume>, <fpage>317</fpage>–<lpage>352</lpage>. <pub-id pub-id-type="doi">10.1214/ss/1177013238</pub-id></mixed-citation>
</ref>
<ref id="B12">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>J. O.</given-names></name><name><surname>Sellke</surname><given-names>T.</given-names></name></person-group> (<year>1987</year>). <article-title>Testing a point null hypothesis: the irreconcilability of <italic>p</italic>-values and evidence</article-title>. <source/>J. Am. Stat. Assoc.
<volume>82</volume>, <fpage>112</fpage>–<lpage>122</lpage>. <pub-id pub-id-type="doi">10.2307/2289139</pub-id></mixed-citation>
</ref>
<ref id="B13">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkson</surname><given-names>J.</given-names></name></person-group> (<year>1938</year>). <article-title>Some difficulties of interpretation encountered in the application of the chi-square test</article-title>. <source/>J. Am. Stat. Assoc. <volume>33</volume>, <fpage>526</fpage>–<lpage>542</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1938.10502329</pub-id></mixed-citation>
</ref>
<ref id="B14">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccia</surname><given-names>S.</given-names></name><name><surname>Rothman</surname><given-names>K. J.</given-names></name><name><surname>Panic</surname><given-names>N.</given-names></name><name><surname>Flacco</surname><given-names>M. E.</given-names></name><name><surname>Rosso</surname><given-names>A.</given-names></name><name><surname>Pastorino</surname><given-names>R.</given-names></name><etal></etal></person-group>. (<year>2016</year>). <article-title>Registration practices for observational studies on ClinicalTrials.gov indicated low adherence</article-title>. <source/>J Clin Epidemiol.
<volume>70</volume>, <fpage>176</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.1016/j.jclinepi.2015.09.009</pub-id><pub-id pub-id-type="pmid">26386325</pub-id></mixed-citation>
</ref>
<ref id="B15">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname><given-names>S.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2016</year>). <article-title>p-Curve and p-Hacking in observational research</article-title>. <source/>PLoS ONE
<volume>112</volume>:<fpage>e0149144</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0149144</pub-id></mixed-citation>
</ref>
<ref id="B16">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname><given-names>K. S.</given-names></name><name><surname>Ioannidis</surname><given-names>J.</given-names></name><name><surname>Mokrysz</surname><given-names>C.</given-names></name><name><surname>Nosek</surname><given-names>B. A.</given-names></name></person-group> (<year>2013</year>). <article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title>. <source/>Nat. Rev. Neurosci. <volume>14</volume>, <fpage>365</fpage>–<lpage>376</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></mixed-citation>
</ref>
<ref id="B17">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carp</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>The secret lives of experiments: methods reporting in the fMRI literature</article-title>. <source/>Neuroimage
<volume>63</volume>, <fpage>289</fpage>–<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.07.004</pub-id><pub-id pub-id-type="pmid">22796459</pub-id></mixed-citation>
</ref>
<ref id="B18">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carver</surname><given-names>R. P.</given-names></name></person-group> (<year>1993</year>). <article-title>The case against statistical significance testing, revisited</article-title>. <source/>J. Exp. Educ.
<volume>61</volume>, <fpage>287</fpage>–<lpage>292</lpage>. <pub-id pub-id-type="doi">10.1080/00220973.1993.10806591</pub-id></mixed-citation>
</ref>
<ref id="B19">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castro Sotos</surname><given-names>A. E.</given-names></name><name><surname>Vanhoof</surname><given-names>S.</given-names></name><name><surname>Van den Noortage</surname><given-names>W.</given-names></name><name><surname>Onghena</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>Students' misconceptions of statistical inference: a review of the empirical evidence from research on statistics education</article-title>. <source/>Educ. Res. Rev. <volume>2</volume>, <fpage>98</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/j.edurev.2007.04.001</pub-id></mixed-citation>
</ref>
<ref id="B20">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castro Sotos</surname><given-names>A. E.</given-names></name><name><surname>Vanhoof</surname><given-names>S.</given-names></name><name><surname>Van den Noortage</surname><given-names>W.</given-names></name><name><surname>Onghena</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>How confident are students in their misconceptions about hypothesis tests?</article-title>
<source/>J. Stat. Educ.
<fpage>17</fpage> Available online at: <ext-link ext-link-type="uri" xlink:href="https://ww2.amstat.org/publications/jse/v17n2/castrosotos.pdf">https://ww2.amstat.org/publications/jse/v17n2/castrosotos.pdf</ext-link></mixed-citation>
</ref>
<ref id="B21">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chaisson</surname><given-names>E.</given-names></name><name><surname>McMillan</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <source/>Astronomy Today. <publisher-name>Pearson</publisher-name>.</mixed-citation>
</ref>
<ref id="B22">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanock</surname><given-names>S. J.</given-names></name><name><surname>Manolio</surname><given-names>T.</given-names></name><name><surname>Boehnke</surname><given-names>M.</given-names></name><name><surname>Boerwinkle</surname><given-names>E.</given-names></name><name><surname>Hunter</surname><given-names>D. J.</given-names></name><name><surname>Thomas</surname><given-names>G.</given-names></name><etal></etal></person-group>. (<year>2007</year>). <article-title>Replicating genotype-phenotype associations</article-title>. <source/>Nature
<volume>447</volume>, <fpage>655</fpage>–<lpage>660</lpage>. <pub-id pub-id-type="doi">10.1038/447655a</pub-id><pub-id pub-id-type="pmid">17554299</pub-id></mixed-citation>
</ref>
<ref id="B23">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavalarias</surname><given-names>D.</given-names></name><name><surname>Wallach</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>A.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>1990-2015</year>). <article-title>Evolution of reporting P-values in the biomedical literature</article-title>. <source/>JAMA
<volume>315</volume>, <fpage>1141</fpage>–<lpage>1148</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2016.1952</pub-id><pub-id pub-id-type="pmid">26978209</pub-id></mixed-citation>
</ref>
<ref id="B24">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>C. A.</given-names></name></person-group> (<year>1963</year>). <article-title>Hypothesis testing in relation to statistical methodology</article-title>. <source/>Rev. Educ. Res. <volume>33</volume>, <fpage>455</fpage>–<lpage>473</lpage>. <pub-id pub-id-type="doi">10.2307/1169648</pub-id></mixed-citation>
</ref>
<ref id="B25">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J.</given-names></name></person-group> (<year>1962</year>). <article-title>The statistical power of abnormal - social psychological research: a review</article-title>. <source/>J. Abnorm. Soc. Psychol.
<volume>65</volume>, <fpage>145</fpage>–<lpage>153</lpage>. <pub-id pub-id-type="doi">10.1037/h0045186</pub-id><pub-id pub-id-type="pmid">13880271</pub-id></mixed-citation>
</ref>
<ref id="B26">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J.</given-names></name></person-group> (<year>1988</year>). <source/>Statistical Power Analysis for the Behavioural Sciences. <publisher-name>Academic Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B27">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J.</given-names></name></person-group> (<year>1994</year>). <article-title>The earth is round p &lt; 0.05</article-title>. <source/>Am. Psychol. <volume>49</volume>, <fpage>997</fpage>–<lpage>1003</lpage>. <pub-id pub-id-type="doi">10.1037/0003-066X.49.12.997</pub-id></mixed-citation>
</ref>
<ref id="B28">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>H.</given-names></name><name><surname>Hedges</surname><given-names>L. V.</given-names></name><name><surname>Valentine</surname><given-names>J. C.</given-names></name></person-group> (<year>2009</year>). <source/>The Handbook of Research Synthesis and Meta-analysis. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Sage</publisher-name>.</mixed-citation>
</ref>
<ref id="B29">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cumming</surname><given-names>G.</given-names></name></person-group> (<year>2014</year>). <article-title>The new statistics: why and how?</article-title>
<source/>Psychol. Sci.
<volume>25</volume>, <fpage>7</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1177/0956797613504966</pub-id><pub-id pub-id-type="pmid">24220629</pub-id></mixed-citation>
</ref>
<ref id="B30">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curran-Everett</surname><given-names>D.</given-names></name></person-group> (<year>2000</year>). <article-title>Multiple comparisons: philosophies and illustrations</article-title>. <source/>Am. J. Physiol. Regul. Integr. Comp. Physiol.
<volume>279</volume>, <fpage>R1</fpage>–<lpage>R8</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://ajpregu.physiology.org/content/279/1/R1">http://ajpregu.physiology.org/content/279/1/R1</ext-link>
<pub-id pub-id-type="pmid">10896857</pub-id></mixed-citation>
</ref>
<ref id="B31">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deer</surname><given-names>B.</given-names></name></person-group> (<year>2011</year>). <article-title>How the case against the MMR vaccine was fixed</article-title>. <source/>Br. Med. J. <volume>342</volume>:<fpage>c5347</fpage>. <pub-id pub-id-type="doi">10.1136/bmj.c5347</pub-id><pub-id pub-id-type="pmid">21209059</pub-id></mixed-citation>
</ref>
<ref id="B32">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeMets</surname><given-names>D.</given-names></name><name><surname>Lan</surname><given-names>K. K. G.</given-names></name></person-group> (<year>1994</year>). <article-title>Interim analysis: the alpha spending function approach</article-title>. <source/>Stat. Med.
<volume>13</volume>, <fpage>1341</fpage>–<lpage>1352</lpage>. <pub-id pub-id-type="doi">10.1002/sim.4780131308</pub-id><pub-id pub-id-type="pmid">7973215</pub-id></mixed-citation>
</ref>
<ref id="B33">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diggle</surname><given-names>P. J.</given-names></name><name><surname>Zeger</surname><given-names>S. L.</given-names></name></person-group> (<year>2010</year>). <article-title>Embracing the concept of reproducible research</article-title>. <source/>Biostatistics
<volume>11</volume>:<fpage>375</fpage>. <pub-id pub-id-type="doi">10.1093/biostatistics/kxq029</pub-id><pub-id pub-id-type="pmid">20538869</pub-id></mixed-citation>
</ref>
<ref id="B34">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Djulbegovic</surname><given-names>D.</given-names></name><name><surname>Hozo</surname><given-names>I.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2014</year>). <article-title>Improving the drug development process: more not less random trials</article-title>. <source/>JAMA
<volume>311</volume>, <fpage>355</fpage>–<lpage>356</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2013.283742</pub-id><pub-id pub-id-type="pmid">24449311</pub-id></mixed-citation>
</ref>
<ref id="B35">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doshi</surname><given-names>P.</given-names></name><name><surname>Goodman</surname><given-names>S. N.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2013</year>). <article-title>Raw data from clinical trials: within reach?</article-title>
<source/>Trends Pharmacol. Sci. <volume>34</volume>, <fpage>645</fpage>–<lpage>647</lpage>. <pub-id pub-id-type="doi">10.1016/j.tips.2013.10.006</pub-id><pub-id pub-id-type="pmid">24295825</pub-id></mixed-citation>
</ref>
<ref id="B36">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>A. W. F.</given-names></name></person-group> (<year>1972</year>). <source/>Likelihood: An Account of the Statistical Concept of Likelihood and Its Application to Scientific Inference. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B37">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eklund</surname><given-names>A.</given-names></name><name><surname>Andersson</surname><given-names>M.</given-names></name><name><surname>Josephson</surname><given-names>C.</given-names></name><name><surname>Johannesson</surname><given-names>M.</given-names></name><name><surname>Knutsson</surname><given-names>H.</given-names></name></person-group> (<year>2012</year>). <article-title>Does parametric fMRI analysis with SPM yield valid results? - An empirical study of 1484 datasets</article-title>. <source/>Neuroimage
<volume>61</volume>, <fpage>565</fpage>–<lpage>578</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.03.093</pub-id><pub-id pub-id-type="pmid">22507229</pub-id></mixed-citation>
</ref>
<ref id="B38">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eklund</surname><given-names>A.</given-names></name><name><surname>Nichols</surname><given-names>T. E.</given-names></name><name><surname>Knutson</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>). <article-title>Cluster failure: why fMRI inferences for spatial extent have inflated false-positives. <italic>Proc. Natl. Acad. Sci</italic></article-title>. <source/>U.S.A.
<volume>113</volume>, <fpage>7900</fpage>–<lpage>7905</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1602413113</pub-id></mixed-citation>
</ref>
<ref id="B39">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etz</surname><given-names>A.</given-names></name><name><surname>Vandekerckhove</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>A Bayesian perspective on the reproducibility project: psychology</article-title>. <source/>PLoS ONE
<volume>112</volume>:<fpage>e0149794</fpage>
<pub-id pub-id-type="doi">10.1371/journal/pone.0149794</pub-id></mixed-citation>
</ref>
<ref id="B40">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evangelou</surname><given-names>E.</given-names></name><name><surname>Ioannidis</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Meta-analysis methods for genome-wide association studies and beyond</article-title>. <source/>Nat. Rev. Genet.
<volume>14</volume>, <fpage>379</fpage>–<lpage>389</lpage>. <pub-id pub-id-type="doi">10.1038/nrg3472</pub-id><pub-id pub-id-type="pmid">23657481</pub-id></mixed-citation>
</ref>
<ref id="B41">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eysenck</surname><given-names>H. J.</given-names></name></person-group> (<year>1960</year>). <article-title>The concept of statistical significance and the controversy about one tailed tests</article-title>. <source/>Psychol. Rev. <volume>67</volume>, <fpage>269</fpage>–<lpage>271</lpage>. <pub-id pub-id-type="doi">10.1037/h0048412</pub-id><pub-id pub-id-type="pmid">13821144</pub-id></mixed-citation>
</ref>
<ref id="B42">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname><given-names>R.</given-names></name><name><surname>Greenbaum</surname><given-names>C. W.</given-names></name></person-group> (<year>1995</year>). <article-title>Significance tests die hard: the Amazing persistence of a probabilistic misconception</article-title>. <source/>Theory Psychol. <volume>5</volume>, <fpage>75</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1177/0959354395051004</pub-id></mixed-citation>
</ref>
<ref id="B43">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname><given-names>D.</given-names></name></person-group> (<year>2010</year>). <article-title>Do pressures to publish increase scientists' bias? An empirical support from US states data</article-title>. <source/>PLoS ONE
<volume>5</volume>:<fpage>e10271</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0010271</pub-id><pub-id pub-id-type="pmid">20422014</pub-id></mixed-citation>
</ref>
<ref id="B44">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>R.</given-names></name></person-group> (<year>1925</year>). <source/>Statistical Methods for Research Workers, First Edition.
<publisher-loc>Edinburgh</publisher-loc>: <publisher-name>Oliver and Boyd</publisher-name>.</mixed-citation>
</ref>
<ref id="B45">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name></person-group> (<year>2013a</year>). <article-title>Commentary: p-values and statistical practice</article-title>. <source/>Epidemiology
<volume>24</volume>, <fpage>69</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1097/EDE.0b013e31827886f7</pub-id><pub-id pub-id-type="pmid">23232612</pub-id></mixed-citation>
</ref>
<ref id="B46">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name></person-group> (<year>2013b</year>). <article-title>Interrogating p values</article-title>. <source/>J. Math. Psychol.
<volume>57</volume>, <fpage>188</fpage>–<lpage>189</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmp.2013.03.005</pub-id></mixed-citation>
</ref>
<ref id="B47">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name></person-group> (<year>2015</year>). <article-title>The connection between varying treatment effects and the crisis of unreplicable research: a bayesian perspective</article-title>. <source/>J. Manage.
<volume>41</volume>, <fpage>632</fpage>–<lpage>643</lpage>. <pub-id pub-id-type="doi">10.1177/0149206314525208</pub-id></mixed-citation>
</ref>
<ref id="B48">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name><name><surname>Carlin</surname><given-names>J. B.</given-names></name><name><surname>Stern</surname><given-names>H. S.</given-names></name><name><surname>Dunson</surname><given-names>D. B.</given-names></name><name><surname>Vehtari</surname><given-names>A.</given-names></name><name><surname>Rubin</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <source/>Bayesian Data Analysis. <publisher-name>CRC Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B49">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name><name><surname>Hill</surname><given-names>J.</given-names></name><name><surname>Yajima</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>). <article-title>Why we (usually) do not have to worry about multiple comparisons</article-title>. <source/>J. Res. Educ. Effect.
<volume>5</volume>, <fpage>189</fpage>–<lpage>211</lpage>. <pub-id pub-id-type="doi">10.1080/19345747.2011.618213</pub-id></mixed-citation>
</ref>
<ref id="B50">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name><name><surname>Loken</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>The statistical crisis in science. Data dependent analysis – A ‘garden of forking paths’ explains why many statistically significant comparisons don't hold up</article-title>. <source/>Am. Sci. <volume>102</volume>, <fpage>460</fpage>–<lpage>465</lpage>. <pub-id pub-id-type="doi">10.1511/2014.111.460</pub-id></mixed-citation>
</ref>
<ref id="B51">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A.</given-names></name><name><surname>Tuerlinckx</surname><given-names>F.</given-names></name></person-group> (<year>2000</year>). <article-title>Type S error rates for classical and Bayesian single and multiple comparison procedures</article-title>. <source/>Comput. Stat.
<volume>15</volume>, <fpage>373</fpage>–<lpage>390</lpage>. <pub-id pub-id-type="doi">10.1007/s001800000040</pub-id></mixed-citation>
</ref>
<ref id="B52">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giere</surname><given-names>R. N.</given-names></name></person-group> (<year>1972</year>). <article-title>The significance test controversy</article-title>. <source/>Br. J. Philos. Sci.
<volume>23</volume>, <fpage>170</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1093/bjps/23.2.170</pub-id></mixed-citation>
</ref>
<ref id="B53">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name></person-group> (<year>1998</year>). <article-title>We need statistical thinking, not statistical rituals</article-title>. <source/>Behav. Brain Sci.
<volume>21</volume>, <fpage>199</fpage>–<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X98281167</pub-id></mixed-citation>
</ref>
<ref id="B54">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name></person-group> (<year>2004</year>). <article-title>Mindless statistics</article-title>. <source/>J. Socio Econ.
<volume>33</volume>, <fpage>587</fpage>–<lpage>606</lpage>. <pub-id pub-id-type="doi">10.1016/j.socec.2004.09.033</pub-id></mixed-citation>
</ref>
<ref id="B55">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name><name><surname>Hertwig</surname><given-names>R.</given-names></name><name><surname>van den Broek</surname><given-names>E.</given-names></name><name><surname>Fasolo</surname><given-names>B.</given-names></name><name><surname>Katsikopoulos</surname><given-names>K. V.</given-names></name></person-group> (<year>2005</year>). <article-title>‘A 30% chance tomorrow’: how does the public understand probabilistic weather forecasts?</article-title>
<source/>Risk Analysis
<volume>25</volume>, <fpage>623</fpage>–<lpage>629</lpage>. <pub-id pub-id-type="doi">10.1111/j.1539-6924.2005.00608.x</pub-id><pub-id pub-id-type="pmid">16022695</pub-id></mixed-citation>
</ref>
<ref id="B56">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name><name><surname>Krauss</surname><given-names>S.</given-names></name><name><surname>Vitouch</surname><given-names>O.</given-names></name></person-group> (<year>2004</year>). <article-title>The null ritual: what you always wanted to know about significance testing but were afraid to ask</article-title>, in <source/>The Sage Handbook of Quantitative Methodology for the Social Sciences, ed <person-group person-group-type="editor"><name><surname>Kaplan</surname><given-names>D.</given-names></name></person-group> (<publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>), <fpage>391</fpage>–<lpage>408</lpage>. <pub-id pub-id-type="doi">10.4135/9781412986311.n21</pub-id></mixed-citation>
</ref>
<ref id="B57">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name><name><surname>Marewski</surname><given-names>J. N.</given-names></name></person-group> (<year>1998</year>). <article-title>Surrogate science: the idol of a universal method for scientific inference</article-title>. <source/>J. Manage. <volume>41</volume>, <fpage>421</fpage>–<lpage>400</lpage>. <pub-id pub-id-type="doi">10.1177/0149206314547522</pub-id></mixed-citation>
</ref>
<ref id="B58">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G.</given-names></name><name><surname>Swijtnik</surname><given-names>Z.</given-names></name><name><surname>Porter</surname><given-names>T.</given-names></name><name><surname>Daston</surname><given-names>L.</given-names></name><name><surname>Beatty</surname><given-names>J.</given-names></name><name><surname>Kruger</surname><given-names>L.</given-names></name></person-group> (<year>1989</year>). <source/>The Empire of Chance.
<publisher-loc>Camridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>
<pub-id pub-id-type="doi">10.1017/CBO9780511720482</pub-id></mixed-citation>
</ref>
<ref id="B59">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gliner</surname><given-names>J. A.</given-names></name><name><surname>Leech</surname><given-names>N. L.</given-names></name><name><surname>Morgan</surname><given-names>G. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Problems with null hypothesis significance testing NHST: what do the textbooks say?</article-title>
<source/>J. Exp. Educ. <volume>7</volume>, <fpage>83</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1080/00220970209602058</pub-id></mixed-citation>
</ref>
<ref id="B60">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Godlee</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>Wakefield's article linking MMR vaccine and autism was fraudulent</article-title>. <source/>Br. Med. J.
<volume>342</volume>:<fpage>c7452</fpage>. <pub-id pub-id-type="doi">10.1136/bmj.c7452</pub-id><pub-id pub-id-type="pmid">21209060</pub-id></mixed-citation>
</ref>
<ref id="B61">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goeman</surname><given-names>J. J.</given-names></name><name><surname>Solari</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Multiple hypothesis testing in genomics</article-title>. <source/>Stat. Med.
<volume>20</volume>, <fpage>1946</fpage>–<lpage>1978</lpage>. <pub-id pub-id-type="doi">10.1002/sim.6082</pub-id></mixed-citation>
</ref>
<ref id="B62">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>S. N.</given-names></name></person-group> (<year>1993</year>). <article-title>p values, hypothesis tests and likelihood: implications for epidemiology of a neglected historical debate</article-title>. <source/>Epidemiology
<volume>5</volume>, <fpage>485</fpage>–<lpage>496</lpage>. <pub-id pub-id-type="doi">10.1093/oxfordjournals.aje.a116700</pub-id></mixed-citation>
</ref>
<ref id="B63">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>S. N.</given-names></name></person-group> (<year>1999</year>). <article-title>Toward evidence-based medical statistics 1: the p value fallacy</article-title>. <source/>Ann. Intern. Med.
<volume>130</volume>, <fpage>995</fpage>–<lpage>1004</lpage>. <pub-id pub-id-type="doi">10.7326/0003-4819-130-12-199906150-00008</pub-id><pub-id pub-id-type="pmid">10383371</pub-id></mixed-citation>
</ref>
<ref id="B64">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>S. N.</given-names></name></person-group> (<year>2008</year>). <article-title>A dirty dozen: twelve p value misconceptions</article-title>. <source/>Semin. Hematol.
<volume>45</volume>, <fpage>135</fpage>–<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1053/j.seminhematol.2008.04.003</pub-id><pub-id pub-id-type="pmid">18582619</pub-id></mixed-citation>
</ref>
<ref id="B65">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>S. A.</given-names></name></person-group> (<year>2009</year>). <article-title>How citation distortions create unfounded authority: analysis of a citation network</article-title>. <source/>BMJ. <volume>339</volume>:<fpage>b2680</fpage>. <pub-id pub-id-type="doi">10.1136/bmj.b2680</pub-id><pub-id pub-id-type="pmid">19622839</pub-id></mixed-citation>
</ref>
<ref id="B66">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallahan</surname><given-names>M.</given-names></name><name><surname>Rosenthal</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>). <article-title>Statistical power: concepts, procedures and applications</article-title>. <source/>Behav. Res. Theory
<volume>34</volume>, <fpage>489</fpage>–<lpage>499</lpage>. <pub-id pub-id-type="pmid">8687371</pub-id></mixed-citation>
</ref>
<ref id="B67">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekstra</surname><given-names>R.</given-names></name><name><surname>Morey</surname><given-names>R. D.</given-names></name><name><surname>Rouder</surname><given-names>J. N.</given-names></name><name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Robust misinterpretation of confidence intervals</article-title>. <source/>Psychon. Bull. Rev. <volume>21</volume>, <fpage>1157</fpage>–<lpage>1164</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-013-0572-3</pub-id><pub-id pub-id-type="pmid">24420726</pub-id></mixed-citation>
</ref>
<ref id="B68">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>R.</given-names></name><name><surname>Bayarri</surname><given-names>M. J.</given-names></name></person-group> (<year>2003</year>). <article-title>Confusion over measures of evidence p's versus errors α's in classical statistical testing</article-title>. <source/>Am. Stat. <volume>57</volume>, <fpage>171</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.1198/0003130031856</pub-id></mixed-citation>
</ref>
<ref id="B69">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>H. M. J.</given-names></name><name><surname>O'Neill</surname><given-names>T.</given-names></name><name><surname>Bauer</surname><given-names>P.</given-names></name><name><surname>Kohne</surname><given-names>K.</given-names></name></person-group> (<year>1997</year>). <article-title>The behavior of the p value when the alternative hypothesis is true</article-title>. <source/>Biometrics
<volume>53</volume>, <fpage>11</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.2307/2533093</pub-id><pub-id pub-id-type="pmid">9147587</pub-id></mixed-citation>
</ref>
<ref id="B70">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>J. E.</given-names></name></person-group> (<year>1997</year>). <article-title>Needed: a ban on the significance test</article-title>. <source/>Psychol. Sci.
<volume>8</volume>, <fpage>3</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.1997.tb00534.x</pub-id></mixed-citation>
</ref>
<ref id="B71">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2008</year>). <article-title>Why most true discovered associations are inflated</article-title>. <source/>Epidemiology
<volume>19</volume>, <fpage>640</fpage>–<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1097/EDE.0b013e31818131e7</pub-id><pub-id pub-id-type="pmid">18633328</pub-id></mixed-citation>
</ref>
<ref id="B72">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name><name><surname>Caplan</surname><given-names>A. L.</given-names></name><name><surname>Dal-Ré</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Outcome reporting bias in clinical trials: why monitoring matters</article-title>. <source/>BMJ
<volume>356</volume>:<fpage>j408</fpage>. <pub-id pub-id-type="doi">10.1136/bmj.j408</pub-id><pub-id pub-id-type="pmid">28196819</pub-id></mixed-citation>
</ref>
<ref id="B73">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name><name><surname>Tarone</surname><given-names>R.</given-names></name><name><surname>McLaughlin</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>The false-positive to false-negative ratio in epidemiological studies</article-title>. <source/>Epidemiology
<volume>22</volume>, <fpage>450</fpage>–<lpage>456</lpage>. <pub-id pub-id-type="doi">10.1097/EDE.0b013e31821b506e</pub-id><pub-id pub-id-type="pmid">21490505</pub-id></mixed-citation>
</ref>
<ref id="B74">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name><name><surname>Trikalinos</surname><given-names>T. A.</given-names></name></person-group> (<year>2007</year>). <article-title>An exploratory test for an excess of significant findings</article-title>. <source/>Clin. Trials
<volume>4</volume>, <fpage>245</fpage>–<lpage>253</lpage>. <pub-id pub-id-type="doi">10.1177/1740774507079441</pub-id><pub-id pub-id-type="pmid">17715249</pub-id></mixed-citation>
</ref>
<ref id="B75">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Why science is not necessarily self-correcting</article-title>. <source/>Perspect. Psychol. Sci. <volume>7</volume>, <fpage>645</fpage>–<lpage>654</lpage>. <pub-id pub-id-type="doi">10.1177/1745691612464056</pub-id><pub-id pub-id-type="pmid">26168125</pub-id></mixed-citation>
</ref>
<ref id="B76">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Why most published research findings are false</article-title>. <source/>PLoS Med.
<volume>2</volume>:<fpage>e124</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id><pub-id pub-id-type="pmid">16060722</pub-id></mixed-citation>
</ref>
<ref id="B77">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name><name><surname>Greenland</surname><given-names>S.</given-names></name><name><surname>Hlatky</surname><given-names>M. A.</given-names></name><name><surname>Khoury</surname><given-names>M. J.</given-names></name><name><surname>MacLeod</surname><given-names>M. R.</given-names></name><name><surname>Moher</surname><given-names>D.</given-names></name><etal></etal></person-group>. (<year>2014</year>). <article-title>Increasing value and reducing waste and research design, conduct and analysis</article-title>. <source/>Lancet
<volume>383</volume>, <fpage>166</fpage>–<lpage>175</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(13)62227-8</pub-id><pub-id pub-id-type="pmid">24411645</pub-id></mixed-citation>
</ref>
<ref id="B78">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeschke</surname><given-names>R.</given-names></name><name><surname>Singer</surname><given-names>J.</given-names></name><name><surname>Guyatt</surname><given-names>G. H.</given-names></name></person-group> (<year>1989</year>). <article-title>Measurement of health status: ascertaining the minimal clinically important difference</article-title>. <source/>Controlled Clin. Trials
<volume>104</volume>, <fpage>407</fpage>–<lpage>415</lpage>. <pub-id pub-id-type="doi">10.1016/0197-2456(89)90005-6</pub-id></mixed-citation>
</ref>
<ref id="B79">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jannot</surname><given-names>A. S.</given-names></name><name><surname>Agoritsas</surname><given-names>T.</given-names></name><name><surname>Gayet-Ageron</surname><given-names>A.</given-names></name><name><surname>Perneger</surname><given-names>T. V.</given-names></name></person-group> (<year>2013</year>). <article-title>Citation bias favoring statistically significant studies was present in medical research</article-title>. <source/>J. Clin. Epidemiol.
<volume>66</volume>, <fpage>296</fpage>–<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1016/j.jclinepi.2012.09.015</pub-id><pub-id pub-id-type="pmid">23347853</pub-id></mixed-citation>
</ref>
<ref id="B80">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>E. T.</given-names></name></person-group> (<year>2003</year>). <source/>Probability Theory: The Logic of Science. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>
<pub-id pub-id-type="doi">10.1017/CBO9780511790423</pub-id></mixed-citation>
</ref>
<ref id="B81">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffreys</surname><given-names>H.</given-names></name></person-group> (<year>1939, 1948, 1961</year>). <source/>The Theory of Probability.
<publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B82">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>L. K.</given-names></name><name><surname>Loewenstein</surname><given-names>G.</given-names></name><name><surname>Prelec</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Measuring the prevalence of questionable research practices with incentives for truth-telling</article-title>. <source/>Psychol. Sci.
<volume>23</volume>, <fpage>524</fpage>–<lpage>532</lpage>. <pub-id pub-id-type="doi">10.1177/0956797611430953</pub-id><pub-id pub-id-type="pmid">22508865</pub-id></mixed-citation>
</ref>
<ref id="B83">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>R. M.</given-names></name><name><surname>Irvin</surname><given-names>V. L.</given-names></name></person-group> (<year>2015</year>). <article-title>Likelihood of null effects of large NHLBI clinical trials has increased over time</article-title>. <source/>PLoS ONE
<volume>10</volume>:<fpage>e0132382</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0132382</pub-id><pub-id pub-id-type="pmid">26244868</pub-id></mixed-citation>
</ref>
<ref id="B84">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kavvoura</surname><given-names>F. K.</given-names></name><name><surname>Liberopoulos</surname><given-names>G.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2007</year>). <article-title>Selection in reported epidemiological risks: an empirical assessment</article-title>. <source/>PLoS Med.
<volume>3</volume>:<fpage>e79</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pmed.0040079</pub-id></mixed-citation>
</ref>
<ref id="B85">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keiding</surname><given-names>N.</given-names></name></person-group> (<year>2010</year>). <article-title>Reproducible research and the substantive context</article-title>. <source/>Biostatistics
<volume>11</volume>, <fpage>376</fpage>–<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1093/biostatistics/kxq033</pub-id><pub-id pub-id-type="pmid">20498225</pub-id></mixed-citation>
</ref>
<ref id="B86">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerr</surname><given-names>N. L.</given-names></name></person-group> (<year>1998</year>). <article-title>HARKing: hypothesizing after the results are known</article-title>. <source/>Pers. Soc. Psychol. Rev.
<volume>2</volume>, <fpage>196</fpage>–<lpage>217</lpage>. <pub-id pub-id-type="doi">10.1207/s15327957pspr0203_4</pub-id><pub-id pub-id-type="pmid">15647155</pub-id></mixed-citation>
</ref>
<ref id="B87">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khoury</surname><given-names>M. J.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2014</year>). <article-title>Big data meets public health: human well-being could benefit from large-scale data if large-scale noise is minimized</article-title>. <source/>Science
<volume>346</volume>, <fpage>1054</fpage>–<lpage>1055</lpage>. <pub-id pub-id-type="doi">10.1126/science.aaa2709</pub-id><pub-id pub-id-type="pmid">25430753</pub-id></mixed-citation>
</ref>
<ref id="B88">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kivimäki</surname><given-names>M.</given-names></name><name><surname>Batty</surname><given-names>G. D.</given-names></name><name><surname>Kawachi</surname><given-names>I.</given-names></name><name><surname>Virtanen</surname><given-names>M.</given-names></name><name><surname>Singh-Manoux</surname><given-names>A.</given-names></name><name><surname>Brunner</surname><given-names>E. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Don't let the truth get in the way of a good story: an illustration of citation bias in epidemiologic research</article-title>. <source/>Am. J. Epidemiol.
<volume>180</volume>, <fpage>446</fpage>–<lpage>448</lpage>. <pub-id pub-id-type="doi">10.1093/aje/kwu164</pub-id><pub-id pub-id-type="pmid">24989242</pub-id></mixed-citation>
</ref>
<ref id="B89">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kjaergard</surname><given-names>L. L.</given-names></name><name><surname>Gluud</surname><given-names>C.</given-names></name></person-group> (<year>2002</year>). <article-title>Citation bias of hepato-biliary randomized clinical trials</article-title>. <source/>J. Clin. Epidemiol.
<volume>55</volume>, <fpage>407</fpage>–<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1016/S0895-4356(01)00513-3</pub-id><pub-id pub-id-type="pmid">11927210</pub-id></mixed-citation>
</ref>
<ref id="B90">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kranz</surname><given-names>D. H.</given-names></name></person-group> (<year>1999</year>). <article-title>The null hypothesis testing controversy in psychology</article-title>. <source/>J. Am. Stat. Assoc. <volume>94</volume>, <fpage>1372</fpage>–<lpage>1381</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1999.10473888</pub-id></mixed-citation>
</ref>
<ref id="B91">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name><name><surname>Simmons</surname><given-names>W. K.</given-names></name><name><surname>Bellgowan</surname><given-names>P. S. F.</given-names></name><name><surname>Baker</surname><given-names>C. I.</given-names></name></person-group> (<year>2009</year>). <article-title>Circular analysis in systems neuroscience – the dangers of double dipping</article-title>. <source/>Nat. Neurosci.
<volume>12</volume>, <fpage>535</fpage>–<lpage>540</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2303</pub-id><pub-id pub-id-type="pmid">19396166</pub-id></mixed-citation>
</ref>
<ref id="B92">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>C.</given-names></name><name><surname>Goodman</surname><given-names>S. N.</given-names></name><name><surname>Griswold</surname><given-names>M. E.</given-names></name><name><surname>Sox</surname><given-names>H. C.</given-names></name></person-group> (<year>2007</year>). <article-title>Reproducible research: moving toward research the public can really trust</article-title>. <source/>Ann. Intern. Med.
<volume>146</volume>, <fpage>450</fpage>–<lpage>453</lpage>. <pub-id pub-id-type="doi">10.7326/0003-4819-146-6-200703200-00154</pub-id><pub-id pub-id-type="pmid">17339612</pub-id></mixed-citation>
</ref>
<ref id="B93">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindley</surname><given-names>D. V.</given-names></name></person-group> (<year>1993</year>). <article-title>The analysis of experimental data: the appreciation of tea and wine</article-title>. <source/>Teach. Stat.
<volume>15</volume>, <fpage>22</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9639.1993.tb00252.x</pub-id></mixed-citation>
</ref>
<ref id="B94">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenz</surname><given-names>R.</given-names></name><name><surname>Hampshire</surname><given-names>A.</given-names></name><name><surname>Leech</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Neuroadaptive bayeisan optmizaiton and hypothesis testing</article-title>. <source/>Trends Cogn. Sci.
<volume>21</volume>, <fpage>155</fpage>–<lpage>167</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2017.01.006</pub-id><pub-id pub-id-type="pmid">28236531</pub-id></mixed-citation>
</ref>
<ref id="B95">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luce</surname><given-names>R. D.</given-names></name></person-group> (<year>1988</year>). <article-title>The tools to theory hypothesis. Review of G. Gigerenzer and D.J. Murray, ‘Cognition as intuitive statistics’</article-title>. <source/>Contemp. Psychol.
<volume>33</volume>, <fpage>582</fpage>–<lpage>583</lpage>. <pub-id pub-id-type="doi">10.1037/030460</pub-id></mixed-citation>
</ref>
<ref id="B96">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lykken</surname><given-names>D. T.</given-names></name></person-group> (<year>1968</year>). <article-title>Statistical significance in psychological research</article-title>. <source/>Psychol. Bull. <volume>70</volume>, <fpage>151</fpage>–<lpage>159</lpage>. <pub-id pub-id-type="doi">10.1037/h0026141</pub-id><pub-id pub-id-type="pmid">5681305</pub-id></mixed-citation>
</ref>
<ref id="B97">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>D. J. C.</given-names></name></person-group> (<year>2003</year>). <source/>Information Theory, Inference and Learning Algorithms.
<publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B98">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname><given-names>M. R.</given-names></name><name><surname>Michie</surname><given-names>S.</given-names></name><name><surname>Roberts</surname><given-names>I.</given-names></name><name><surname>Dirnagl</surname><given-names>U.</given-names></name><name><surname>Chalmers</surname><given-names>I.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name><etal></etal></person-group> (<year>2014</year>). <article-title>Biomedical research: increasing value, reducing waste</article-title>. <source/>Lancet
<volume>383</volume>, <fpage>101</fpage>–<lpage>104</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(13)62329-6</pub-id><pub-id pub-id-type="pmid">24411643</pub-id></mixed-citation>
</ref>
<ref id="B99">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makel</surname><given-names>M.</given-names></name><name><surname>Plucker</surname><given-names>J.</given-names></name><name><surname>Hegarty</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Replications in psychology research: how often do they really occur?</article-title>
<source/>Perspect. Psychol. Sci.
<volume>7</volume>, <fpage>537</fpage>–<lpage>542</lpage>. <pub-id pub-id-type="doi">10.1177/1745691612460688</pub-id><pub-id pub-id-type="pmid">26168110</pub-id></mixed-citation>
</ref>
<ref id="B100">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marusic</surname><given-names>A.</given-names></name><name><surname>Marusic</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Teaching students how to read and write science: a mandatory course on scientific research and communication in medicine</article-title>. <source/>Acad. Med.
<volume>78</volume>, <fpage>1235</fpage>–<lpage>1239</lpage>. <pub-id pub-id-type="doi">10.1097/00001888-200312000-00007</pub-id><pub-id pub-id-type="pmid">14660423</pub-id></mixed-citation>
</ref>
<ref id="B101">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meehl</surname><given-names>P. E.</given-names></name></person-group> (<year>1967</year>). <article-title>Theory testing in psychology and physics: a methodological paradox</article-title>. <source/>Philos. Sci.
<volume>34</volume>, <fpage>103</fpage>–<lpage>115</lpage>. <pub-id pub-id-type="doi">10.1086/288135</pub-id></mixed-citation>
</ref>
<ref id="B102">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meehl</surname><given-names>P. E.</given-names></name></person-group> (<year>1978</year>). <article-title>Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology</article-title>. <source/>J. Consult. Clin. Psychol.
<volume>46</volume>, <fpage>806</fpage>–<lpage>834</lpage>. <pub-id pub-id-type="doi">10.1037/0022-006X.46.4.806</pub-id></mixed-citation>
</ref>
<ref id="B103">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meehl</surname><given-names>P. E.</given-names></name></person-group> (<year>1990</year>). <article-title>Why summaries of research on psychological theories are often uninterpretable</article-title>. <source/>Psychol. Rep. <volume>66</volume>, <fpage>195</fpage>–<lpage>244</lpage>. <pub-id pub-id-type="doi">10.2466/pr0.1990.66.1.195</pub-id></mixed-citation>
</ref>
<ref id="B104">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaelson</surname><given-names>A. M.</given-names></name><name><surname>Morley</surname><given-names>E. W.</given-names></name></person-group> (<year>1887</year>). <article-title>On the relative motion of the earth and the luminiferous ether</article-title>. <source/>American Journal of Science. <volume>34</volume>, <fpage>333</fpage>–<lpage>345</lpage>. <pub-id pub-id-type="doi">10.2475/ajs.s3-34.203.333</pub-id></mixed-citation>
</ref>
<ref id="B105">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moharari</surname><given-names>R. S.</given-names></name><name><surname>Rahimi</surname><given-names>E. R.</given-names></name><name><surname>Najafi</surname><given-names>A.</given-names></name><etal></etal></person-group>. (<year>2009</year>). <article-title>Teaching critical appraisal and statistics in anesthesia journal club</article-title>. <source/>Q. J. Med.
<volume>102</volume>, <fpage>139</fpage>–<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1093/qjmed/hcn131</pub-id><pub-id pub-id-type="pmid">18842683</pub-id></mixed-citation>
</ref>
<ref id="B106">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morey</surname><given-names>R. D.</given-names></name><name><surname>Hoekstra</surname><given-names>R.</given-names></name><name><surname>Rouder</surname><given-names>J. N.</given-names></name><name><surname>Lee</surname><given-names>M. D.</given-names></name><name><surname>Wagenmakers</surname><given-names>E.-J.</given-names></name></person-group> (<year>2016</year>). <article-title>The fallacy of placing confidence in confidence intervals</article-title>. <source/>Psychon. Bull. Rev.
<volume>23</volume>, <fpage>103</fpage>–<lpage>123</lpage>. <pub-id pub-id-type="doi">10.3758/s13423-015-0947-8</pub-id><pub-id pub-id-type="pmid">26450628</pub-id></mixed-citation>
</ref>
<ref id="B107">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murdoch</surname><given-names>D. J.</given-names></name><name><surname>Tsai</surname><given-names>Y. L.</given-names></name><name><surname>Adcock</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>P values are random variables</article-title>. <source/>Am. Stat.
<volume>62</volume>, <fpage>242</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1198/000313008X332421</pub-id></mixed-citation>
</ref>
<ref id="B108">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neath</surname><given-names>A. A.</given-names></name><name><surname>Cavanaugh</surname><given-names>J. E.</given-names></name></person-group> (<year>2006</year>). <article-title>A Bayesian approach to the multiple comparison problem</article-title>. <source/>J. Data Sci.
<volume>4</volume>, <fpage>131</fpage>–<lpage>146</lpage>.</mixed-citation>
</ref>
<ref id="B109">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neyman</surname><given-names>J.</given-names></name></person-group> (<year>1950</year>). <source/>Probability and Statistics. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Holt</publisher-name>. </mixed-citation>
</ref>
<ref id="B110">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neyman</surname><given-names>J.</given-names></name><name><surname>Pearson</surname><given-names>E. S.</given-names></name></person-group> (<year>1933</year>). <article-title>On the problem of the most efficient tests of statistical hypotheses</article-title>. <source/>Philos. Trans. R. Soc. Lond. Ser. A
<volume>231</volume>, <fpage>289</fpage>–<lpage>337</lpage>.</mixed-citation>
</ref>
<ref id="B111">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>T.</given-names></name><name><surname>Hayasaka</surname><given-names>S.</given-names></name></person-group> (<year>2003</year>). <article-title>Controlling the familywise error rate in neuroimaging: a comparative review</article-title>. <source/>Stat. Methods Med. Res.
<volume>12</volume>, <fpage>419</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1191/0962280203sm341ra</pub-id><pub-id pub-id-type="pmid">14599004</pub-id></mixed-citation>
</ref>
<ref id="B112">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>T. E.</given-names></name><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Eickhoff</surname><given-names>S. B.</given-names></name><name><surname>Evans</surname><given-names>A. C.</given-names></name><name><surname>Glatard</surname><given-names>T.</given-names></name><name><surname>Hanke</surname><given-names>M.</given-names></name><etal></etal></person-group>. (<year>2016</year>). <article-title>Best practices in data analysis and sharing in neuroimaging using MRI</article-title>. <source/>bioRxiv.
<pub-id pub-id-type="doi">10.1101/054262</pub-id><pub-id pub-id-type="pmid">28230846</pub-id></mixed-citation>
</ref>
<ref id="B113">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>T. E.</given-names></name><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Eickhoff</surname><given-names>S. B.</given-names></name><name><surname>Evans</surname><given-names>A. C.</given-names></name><name><surname>Glatard</surname><given-names>T.</given-names></name><name><surname>Hanke</surname><given-names>M.</given-names></name><etal></etal></person-group>. (<year>2017</year>). <article-title>Best practices in data analysis and sharing in neuroimaging using MRI</article-title>. <source/>Nat. Neurosci.
<volume>20</volume>, <fpage>299</fpage>–<lpage>303</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4500</pub-id><pub-id pub-id-type="pmid">28230846</pub-id></mixed-citation>
</ref>
<ref id="B114">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickerson</surname><given-names>R. S.</given-names></name></person-group> (<year>2000</year>). <article-title>Null hypothesis significance testing: a review of an old and continuing controversy</article-title>. <source/>Psychol. Methods
<volume>5</volume>, <fpage>241</fpage>–<lpage>301</lpage>. <pub-id pub-id-type="doi">10.1037/1082-989X.5.2.241</pub-id><pub-id pub-id-type="pmid">10937333</pub-id></mixed-citation>
</ref>
<ref id="B115">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>B. A.</given-names></name><name><surname>Alter</surname><given-names>G.</given-names></name><name><surname>Banks</surname><given-names>G. C.</given-names></name><name><surname>Borsboom</surname><given-names>D.</given-names></name><name><surname>Bowman</surname><given-names>S. D.</given-names></name><name><surname>Breckler</surname><given-names>S. J.</given-names></name><etal></etal></person-group>. (<year>2015</year>). <article-title>Promoting an open research culture</article-title>. <source/>Science
<volume>348</volume>, <fpage>1422</fpage>–<lpage>1425</lpage>
<pub-id pub-id-type="doi">10.1016/j.jmp.2015.12.007</pub-id><pub-id pub-id-type="pmid">26113702</pub-id></mixed-citation>
</ref>
<ref id="B116">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname><given-names>B. A.</given-names></name><name><surname>Spies</surname><given-names>J. R.</given-names></name><name><surname>Motyl</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>Scientific utopia II: restructuring incentives and practices to promote truth over publishability</article-title>. <source/>Perspect. Psychol. Sci. <volume>7</volume>, <fpage>615</fpage>–<lpage>631</lpage>. <pub-id pub-id-type="doi">10.1177/1745691612459058</pub-id><pub-id pub-id-type="pmid">26168121</pub-id></mixed-citation>
</ref>
<ref id="B117">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nuijten</surname><given-names>M. B.</given-names></name><name><surname>Hartgerink</surname><given-names>C. H.</given-names></name><name><surname>van Assen</surname><given-names>M. A.</given-names></name><name><surname>Epskamp</surname><given-names>S.</given-names></name><name><surname>Wicherts</surname><given-names>J. M.</given-names></name></person-group> (<year>2016</year>). <article-title>The prevalence of statistical reporting errors in psychology 1985-2013</article-title>. <source/>Behav. Res. Methods. <volume>48</volume>, <fpage>1205</fpage>–<lpage>1226</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.3758%2Fs13428-015-0664-2">https://link.springer.com/article/10.3758%2Fs13428-015-0664-2</ext-link>
<pub-id pub-id-type="pmid">26497820</pub-id></mixed-citation>
</ref>
<ref id="B118">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunnally</surname><given-names>J.</given-names></name></person-group> (<year>1960</year>). <article-title>The place of statistics in psychology</article-title>. <source/>Educ. Psychol. Measur. <volume>20</volume>, <fpage>641</fpage>–<lpage>650</lpage>. <pub-id pub-id-type="doi">10.1177/001316446002000401</pub-id></mixed-citation>
</ref>
<ref id="B119">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oakes</surname><given-names>M. L.</given-names></name></person-group> (<year>1986</year>). <source/>Statistical Inference: A Commentary for the Social and Behavioural Sciences. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</mixed-citation>
</ref>
<ref id="B120">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname><given-names>H.</given-names></name><name><surname>Harris</surname><given-names>C. R.</given-names></name></person-group> (<year>2012</year>). <article-title>Is the replicability crisis overblown? Three arguments examined</article-title>. <source/>Perspect. Psychol. Sci.
<volume>7</volume>, <fpage>531</fpage>–<lpage>536</lpage>. <pub-id pub-id-type="doi">10.1177/1745691612463401</pub-id><pub-id pub-id-type="pmid">26168109</pub-id></mixed-citation>
</ref>
<ref id="B121">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>C. J.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2014a</year>). <article-title>Placing epidemiological results in the context of multiplicity and typical correlations of exposures</article-title>. <source/>J. Epidemiol. Community Health
<volume>68</volume>, <fpage>1096</fpage>–<lpage>1100</lpage>. <pub-id pub-id-type="doi">10.1136/jech-2014-204195</pub-id><pub-id pub-id-type="pmid">24923805</pub-id></mixed-citation>
</ref>
<ref id="B122">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>C. J.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2014b</year>). <article-title>Studying the elusive environment in large scale</article-title>. <source/>JAMA
<volume>311</volume>, <fpage>2173</fpage>–<lpage>2174</lpage>. <pub-id pub-id-type="doi">10.1136/jech-2014-204195</pub-id><pub-id pub-id-type="pmid">24893084</pub-id></mixed-citation>
</ref>
<ref id="B123">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>C. J.</given-names></name><name><surname>Burford</surname><given-names>B.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2015</year>). <article-title>Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations</article-title>. <source/>J. Clin. Epidemiol.
<volume>68</volume>, <fpage>1046</fpage>–<lpage>1058</lpage>. <pub-id pub-id-type="doi">10.1016/j.jclinepi.2015.05.029</pub-id><pub-id pub-id-type="pmid">26279400</pub-id></mixed-citation>
</ref>
<ref id="B124">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J.</given-names></name></person-group> (<year>1988</year>). <source/>Probabilistic Reasoning in Intelligent Systems.
<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan</publisher-name>.</mixed-citation>
</ref>
<ref id="B125">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>R. D.</given-names></name></person-group> (<year>2009</year>). <article-title>Reproducible research and biostatistics</article-title>. <source/>Biostatistics
<volume>10</volume>, <fpage>405</fpage>–<lpage>408</lpage>. <pub-id pub-id-type="doi">10.1093/biostatistics/kxp014</pub-id><pub-id pub-id-type="pmid">19535325</pub-id></mixed-citation>
</ref>
<ref id="B126">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>R. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Reproducible research in computational science</article-title>. <source/>Science
<volume>334</volume>, <fpage>1226</fpage>–<lpage>1227</lpage>. <pub-id pub-id-type="doi">10.1126/science.1213847</pub-id><pub-id pub-id-type="pmid">22144613</pub-id></mixed-citation>
</ref>
<ref id="B127">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernet</surname><given-names>C.</given-names></name><name><surname>Poline</surname><given-names>J.-B.</given-names></name></person-group> (<year>2015</year>). <article-title>Improving functional magnetic imaging reproducibility</article-title>. <source/>Gigascience
<volume>4</volume>:<fpage>15</fpage>. <pub-id pub-id-type="doi">10.1186/s13742-015-0055-8</pub-id><pub-id pub-id-type="pmid">25830019</pub-id></mixed-citation>
</ref>
<ref id="B128">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>R. A.</given-names></name></person-group> (<year>2006</year>). <article-title>Can cognitive processes be inferred from neuroimaging data?</article-title>
<source/>Trends Cogn. Sci.
<volume>10</volume>, <fpage>59</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.12.004</pub-id><pub-id pub-id-type="pmid">16406760</pub-id></mixed-citation>
</ref>
<ref id="B129">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollard</surname><given-names>P.</given-names></name><name><surname>Richardson</surname><given-names>J. T. E.</given-names></name></person-group> (<year>1987</year>). <article-title>On the probability of making Type-I errors</article-title>. <source/>Psychol. Bull.
<volume>102</volume>, <fpage>159</fpage>–<lpage>163</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.102.1.159</pub-id></mixed-citation>
</ref>
<ref id="B130">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossi</surname><given-names>J. S.</given-names></name></person-group> (<year>1990</year>). <article-title>Statistical power of psychological research: what have we gained in 20 years?</article-title>
<source/>J. Consult. Clin. Psychol. <volume>58</volume>, <fpage>646</fpage>–<lpage>656</lpage>. <pub-id pub-id-type="doi">10.1037/0022-006X.58.5.646</pub-id><pub-id pub-id-type="pmid">2254513</pub-id></mixed-citation>
</ref>
<ref id="B131">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rozeboom</surname><given-names>W. W.</given-names></name></person-group> (<year>1960</year>). <article-title>The fallacy of the null hypothesis significance test</article-title>. <source/>Psychol. Bull. <volume>57</volume>, <fpage>416</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1037/h0042040</pub-id><pub-id pub-id-type="pmid">13744252</pub-id></mixed-citation>
</ref>
<ref id="B132">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F. L.</given-names></name></person-group> (<year>1992</year>). <article-title>What do data really mean? Research findings, meta-analysis and cumulative knowledge in psychology</article-title>. <source/>Am. Psychol.
<volume>47</volume>, <fpage>1173</fpage>–<lpage>1181</lpage>. <pub-id pub-id-type="doi">10.1037/0003-066X.47.10.1173</pub-id></mixed-citation>
</ref>
<ref id="B133">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F. L.</given-names></name></person-group> (<year>1996</year>). <article-title>Statistical significance testing and cumulative knowledge in psychology: implications for training of researchers</article-title>. <source/>Psychol. Methods
<volume>1</volume>, <fpage>115</fpage>–<lpage>129</lpage>. <pub-id pub-id-type="doi">10.1037/1082-989X.1.2.115</pub-id></mixed-citation>
</ref>
<ref id="B134">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenfeld</surname><given-names>J. D.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Is everything we eat is associated with cancer? A systematic cookbook review</article-title>. <source/>Am. J. Clin. Nutri.
<volume>97</volume>, <fpage>127</fpage>–<lpage>134</lpage>. <pub-id pub-id-type="doi">10.3945/ajcn.112.047142</pub-id><pub-id pub-id-type="pmid">23193004</pub-id></mixed-citation>
</ref>
<ref id="B135">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedlmeier</surname><given-names>P.</given-names></name><name><surname>Gigerenzer</surname><given-names>G.</given-names></name></person-group> (<year>1989</year>). <article-title>Do studies of statistical power have an effect on the power of the studies?</article-title>
<source/>Psychol. Bull.
<volume>105</volume>, <fpage>309</fpage>–<lpage>316</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.105.2.309</pub-id></mixed-citation>
</ref>
<ref id="B136">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sellke</surname><given-names>T.</given-names></name><name><surname>Bayarri</surname><given-names>M. J.</given-names></name><name><surname>Berger</surname><given-names>J. O.</given-names></name></person-group> (<year>2001</year>). <article-title>Calibration of p values for testing precise null hypotheses</article-title>. <source/>Am. Stat.
<volume>55</volume>, <fpage>62</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1198/000313001300339950</pub-id></mixed-citation>
</ref>
<ref id="B137">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaffer</surname><given-names>J. P.</given-names></name></person-group> (<year>1995</year>). <article-title>Multiple hypothesis testing</article-title>. <source/>Annu. Rev. Psychol.
<volume>46</volume>, <fpage>561</fpage>–<lpage>584</lpage>
<pub-id pub-id-type="doi">10.1146/annurev.ps.46.020195.003021</pub-id></mixed-citation>
</ref>
<ref id="B138">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>J.</given-names></name><name><surname>Nelson</surname><given-names>L.</given-names></name><name><surname>Simonsohn</surname><given-names>U.</given-names></name></person-group> (<year>2011</year>). <article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allow presenting anything as significant</article-title>. <source/>Psychol. Sci.
<volume>22</volume>, <fpage>1359</fpage>–<lpage>1366</lpage>. <pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></mixed-citation>
</ref>
<ref id="B139">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname><given-names>U.</given-names></name><name><surname>Nelson</surname><given-names>L. D.</given-names></name><name><surname>Simmons</surname><given-names>J. P.</given-names></name></person-group> (<year>2014a</year>). <article-title>P-Curve: a key to the file drawer</article-title>. <source/>J. Exp. Psychol.
<volume>1432</volume>, <fpage>534</fpage>–<lpage>547</lpage>. <pub-id pub-id-type="doi">10.1037/a0033242</pub-id></mixed-citation>
</ref>
<ref id="B140">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname><given-names>U.</given-names></name><name><surname>Nelson</surname><given-names>L. D.</given-names></name><name><surname>Simmons</surname><given-names>J. P.</given-names></name></person-group> (<year>2014b</year>). <article-title><italic>p</italic>-Curve and effect size: correcting for publication bias using only significant results</article-title>. <source/>Psychol. Sci.
<volume>96</volume>, <fpage>666</fpage>–<lpage>681</lpage>. <pub-id pub-id-type="doi">10.1177/1745691614553988</pub-id></mixed-citation>
</ref>
<ref id="B141">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siontis</surname><given-names>G. C.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P.</given-names></name></person-group> (<year>2011</year>). <article-title>Risk factors and interventions with statistically significant tiny effects</article-title>. <source/>Int. J. Epidemiol.
<volume>40</volume>, <fpage>1292</fpage>–<lpage>1307</lpage>. <pub-id pub-id-type="doi">10.1093/ije/dyr099</pub-id><pub-id pub-id-type="pmid">21737403</pub-id></mixed-citation>
</ref>
<ref id="B143">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sivia</surname><given-names>D. S.</given-names></name><name><surname>Skilling</surname><given-names>J.</given-names></name></person-group> (<year>2006</year>). <source/>Data Analysis: A Bayesian Tutorial. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B144">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skokic</surname><given-names>M.</given-names></name><name><surname>Collins</surname><given-names>J.</given-names></name><name><surname>Callahan-Flintoft</surname><given-names>C.</given-names></name><name><surname>Bowman</surname><given-names>H.</given-names></name><name><surname>Wyble</surname><given-names>B.</given-names></name></person-group> (<year>2016</year>). <article-title>I tried a bunch of things: the dangers of unexpected overfitting in classification</article-title>. <source/>BioRxiv.</mixed-citation>
</ref>
<ref id="B145">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smolin</surname><given-names>L.</given-names></name></person-group> (<year>2006</year>). <source/>The Trouble with Physics. <publisher-loc>London</publisher-loc>: <publisher-name>Penguin</publisher-name>.</mixed-citation>
</ref>
<ref id="B146">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soares</surname><given-names>H. P.</given-names></name><name><surname>Kumar</surname><given-names>A.</given-names></name><name><surname>Daniels</surname><given-names>S.</given-names></name><name><surname>Swann</surname><given-names>S.</given-names></name><name><surname>Cantor</surname><given-names>A.</given-names></name><name><surname>Hozo</surname><given-names>I.</given-names></name><etal></etal></person-group>. (<year>2005</year>). <article-title>Evaluation of new treatments in radiation oncology: are they better than standard treatments?</article-title>
<source/>JAMA
<volume>293</volume>, <fpage>970</fpage>–<lpage>978</lpage>. <pub-id pub-id-type="doi">10.1001/jama.293.8.970</pub-id><pub-id pub-id-type="pmid">15728168</pub-id></mixed-citation>
</ref>
<ref id="B147">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>T. D.</given-names></name></person-group> (<year>1959</year>). <article-title>Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa</article-title>. <source/>J. Am. Stat. Assoc.
<volume>54</volume>, <fpage>30</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1959.10501497</pub-id></mixed-citation>
</ref>
<ref id="B148">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>T. D.</given-names></name><name><surname>Rosenbaum</surname><given-names>W. L.</given-names></name><name><surname>Weinkam</surname><given-names>J. J.</given-names></name></person-group> (<year>1995</year>). <article-title>Publication decisions revisited: The effect of the outcome of statistical tests on the decision to publish and vice versa</article-title>. <source/>Am. Stat.
<volume>49</volume>, <fpage>108</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1080/00031305.1995.10476125</pub-id></mixed-citation>
</ref>
<ref id="B149">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterne</surname><given-names>J. A. C.</given-names></name><name><surname>Smith</surname><given-names>G. D.</given-names></name></person-group> (<year>2001</year>). <article-title>Sifting the evidence - what's wrong with significance tests?</article-title>
<source/>Br. Med. J.
<volume>322</volume>, <fpage>226</fpage>–<lpage>231</lpage>. <pub-id pub-id-type="doi">10.1136/bmj.322.7280.226</pub-id><pub-id pub-id-type="pmid">11159626</pub-id></mixed-citation>
</ref>
<ref id="B150">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stigler</surname><given-names>S. M.</given-names></name></person-group> (<year>1986</year>). <source/>The History of Statistics. <publisher-loc>Cambridge, MA; London</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="B151">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stodden</surname><given-names>V.</given-names></name><name><surname>McNutt</surname><given-names>M.</given-names></name><name><surname>Bailey</surname><given-names>D. H.</given-names></name><name><surname>Deelman</surname><given-names>E.</given-names></name><name><surname>Gil</surname><given-names>Y.</given-names></name><name><surname>Hanson</surname><given-names>B.</given-names></name><etal></etal></person-group>. (<year>2016</year>). <article-title>Enhancing reproducibility for computational methods</article-title>. <source/>Science
<volume>354</volume>, <fpage>1240</fpage>–<lpage>1241</lpage>. <pub-id pub-id-type="doi">10.1126/science.aah6168</pub-id><pub-id pub-id-type="pmid">27940837</pub-id></mixed-citation>
</ref>
<ref id="B152">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szucs</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>A tutorial on hunting statistical significance by chasing N</article-title>. <source/>Front. Psychol.
<volume>7</volume>:<fpage>1444</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2016.01444</pub-id><pub-id pub-id-type="pmid">27713723</pub-id></mixed-citation>
</ref>
<ref id="B153">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szucs</surname><given-names>D.</given-names></name><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature</article-title>. <source/>PLoS Biol.
<volume>15</volume>:<fpage>e2000797</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.2000797</pub-id><pub-id pub-id-type="pmid">28253258</pub-id></mixed-citation>
</ref>
<ref id="B154">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vujaklija</surname><given-names>A.</given-names></name><name><surname>Hren</surname><given-names>D.</given-names></name><name><surname>Sambunjak</surname><given-names>D.</given-names></name><name><surname>Vodopivec</surname><given-names>I.</given-names></name><name><surname>Ivanis</surname><given-names>A.</given-names></name><name><surname>Marusic</surname><given-names>A.</given-names></name><etal></etal></person-group>. (<year>2010</year>). <article-title>Can teaching research methodology influence students' attitude toward science? Cohort study and nonrandomized trial in a single medical school</article-title>. <source/>J. Investig. Med.
<volume>58</volume>, <fpage>282</fpage>–<lpage>286</lpage>. <pub-id pub-id-type="doi">10.2310/JIM.0b013e3181cb42d9</pub-id><pub-id pub-id-type="pmid">20130460</pub-id></mixed-citation>
</ref>
<ref id="B155">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vul</surname><given-names>E.</given-names></name><name><surname>Harris</surname><given-names>C.</given-names></name><name><surname>Winkielman</surname><given-names>P.</given-names></name><name><surname>Pashler</surname><given-names>H.</given-names></name></person-group> (<year>2009</year>). <article-title>Puzzlingly high correlations in fMRI studies of emotion, personality and social cognition</article-title>. <source/>Perspect. Psychol. Sci. <volume>4</volume>, <fpage>274</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1111/j.1745-6924.2009.01125.x</pub-id><pub-id pub-id-type="pmid">26158964</pub-id></mixed-citation>
</ref>
<ref id="B156">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>E.-J.</given-names></name></person-group> (<year>2007</year>). <article-title>A practical solution to the pervasive problem of p values</article-title>. <source/>Psychon. Bull. Rev.
<volume>14</volume>, <fpage>779</fpage>–<lpage>804</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194105</pub-id><pub-id pub-id-type="pmid">18087943</pub-id></mixed-citation>
</ref>
<ref id="B157">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name><name><surname>Wetzels</surname><given-names>R.</given-names></name><name><surname>Borsboom</surname><given-names>D.</given-names></name><name><surname>van der Maas</surname><given-names>H. L. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Why psychologists must change the way they analyse their data: the case of psi: comment on Bem (2011)</article-title>. <source/>J. Pers. Soc. Psychol.
<volume>100</volume>, <fpage>426</fpage>–<lpage>432</lpage>. <pub-id pub-id-type="doi">10.1037/a0022790</pub-id><pub-id pub-id-type="pmid">21280965</pub-id></mixed-citation>
</ref>
<ref id="B158">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname><given-names>N. G.</given-names></name></person-group> (<year>2004</year>). <article-title>The fallacy of the null hypothesis in soft psychology</article-title>. <source/>Appl. Prevent. Psychol. <volume>11</volume>, <fpage>83</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1016/j.appsy.2004.02.015</pub-id></mixed-citation>
</ref>
<ref id="B159">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wasserstein</surname><given-names>R. L.</given-names></name><name><surname>Lazar</surname><given-names>N. A.</given-names></name></person-group> (<year>2016</year>). <article-title>The ASA statement on p values: context, process, and purpose</article-title>. <source/>Am. Stat.
<volume>70</volume>, <fpage>129</fpage>–<lpage>133</lpage>. <pub-id pub-id-type="doi">10.1080/00031305.2016.1154108</pub-id></mixed-citation>
</ref>
<ref id="B160">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wellek</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <source/>Testing Statistical Hypotheses of Equivalence and Noninferiority, 2nd Edition.
<publisher-name>Chapman and Hall/CRC</publisher-name>
<pub-id pub-id-type="doi">10.1201/EBK1439808184</pub-id></mixed-citation>
</ref>
<ref id="B161">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westfall</surname><given-names>P. H.</given-names></name><name><surname>Johnson</surname><given-names>W. O.</given-names></name><name><surname>Utts</surname><given-names>J. M.</given-names></name></person-group> (<year>1997</year>). <article-title>A Bayesian perspective on the Bonferroni adjustment</article-title>. <source/>Biometrika
<volume>84</volume>, <fpage>419</fpage>–<lpage>427</lpage>. <pub-id pub-id-type="doi">10.1093/biomet/84.2.419</pub-id></mixed-citation>
</ref>
<ref id="B162">
<mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkerson</surname><given-names>M.</given-names></name><name><surname>Olson</surname><given-names>M. R.</given-names></name></person-group> (<year>2010</year>). <article-title>Misconceptions about sample size, statistical significance and treatment effect</article-title>. <source/>J. Psychol. <volume>131</volume>, <fpage>627</fpage>–<lpage>631</lpage>. <pub-id pub-id-type="doi">10.1080/00223989709603844</pub-id></mixed-citation>
</ref>
<ref id="B163">
<mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ziliak</surname><given-names>T.</given-names></name><name><surname>McCloskey</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <source/>The Cult of Statistical Significance. <publisher-loc>Ann Arbor, MI</publisher-loc>: <publisher-name>The University of Michigan Press</publisher-name>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>
</pmc-articleset>