<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="research-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Neuroimage Clin</journal-id>
<journal-id journal-id-type="iso-abbrev">Neuroimage Clin</journal-id>
<journal-title-group>
<journal-title>NeuroImage : Clinical</journal-title>
</journal-title-group>
<issn pub-type="epub">2213-1582</issn>
<publisher>
<publisher-name>Elsevier</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">26793434</article-id>
<article-id pub-id-type="pmc">4683429</article-id>
<article-id pub-id-type="publisher-id">S2213-1582(15)30027-9</article-id>
<article-id pub-id-type="doi">10.1016/j.nicl.2015.11.010</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Regular Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Classification of autistic individuals and controls using cross-task characterization of fMRI activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chanel</surname>
<given-names>Guillaume</given-names>
</name>
<email>guillaume.chanel@unige.ch</email>
<xref ref-type="aff" rid="af0005">a</xref>
<xref ref-type="aff" rid="af0010">b</xref>
<xref ref-type="corresp" rid="cr0005">⁎</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pichon</surname>
<given-names>Swann</given-names>
</name>
<xref ref-type="aff" rid="af0005">a</xref>
<xref ref-type="aff" rid="af0015">c</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Conty</surname>
<given-names>Laurence</given-names>
</name>
<xref ref-type="aff" rid="af0020">d</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berthoz</surname>
<given-names>Sylvie</given-names>
</name>
<xref ref-type="aff" rid="af0025">e</xref>
<xref ref-type="aff" rid="af0030">f</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chevallier</surname>
<given-names>Coralie</given-names>
</name>
<xref ref-type="aff" rid="af0035">g</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Grèzes</surname>
<given-names>Julie</given-names>
</name>
<xref ref-type="aff" rid="af0035">g</xref>
<xref ref-type="aff" rid="af0040">h</xref>
</contrib>
</contrib-group>
<aff id="af0005"><label>a</label>Swiss Center for Affective Sciences, Campus Biotech, University of Geneva, Geneva, Switzerland</aff>
<aff id="af0010"><label>b</label>Computer Vision and Multimedia Laboratory, University of Geneva, Geneva, Switzerland</aff>
<aff id="af0015"><label>c</label>Faculty of Psychology and Educational Sciences, University of Geneva, Geneva, Switzerland</aff>
<aff id="af0020"><label>d</label>Laboratoire de Psychopathologie et Neuropsychologie EA 2027, Université Paris 8, France</aff>
<aff id="af0025"><label>e</label>CESP, INSERM, Univ. Paris-Sud, Univ. Paris Descartes, UVSQ, Université Paris-Saclay, Paris, France</aff>
<aff id="af0030"><label>f</label>Departement de Psychiatrie de l'Institut Mutualiste Montsouris, Paris, France</aff>
<aff id="af0035"><label>g</label>Laboratoire de Neuroscience Cognitive, INSERM U960, Ecole Normale Supérieure, Paris, France</aff>
<aff id="af0040"><label>h</label>Centre de Neuroimagerie de Recherche (CENIR), Centre de Recherche de l'Institut du Cerveau et de la Moelle épinière (CRICM), Université Pierre et Marie Curie-Paris 6 UMRS 975, Inserm U975, CNRS UMR 7225, Institut du cerveau et de la moëlle épinière (ICM), Paris 75013, France</aff>
<author-notes>
<corresp id="cr0005"><label>⁎</label>Corresponding author at: Campus Biotech, CISA, University of Geneva, Case Postale 60, 1211 Geneva 20, Switzerland.Campus BiotechCISAUniversity of GenevaCase Postale 60Geneva 201211Switzerland <email>guillaume.chanel@unige.ch</email></corresp>
</author-notes>
<pub-date pub-type="pmc-release">
<day>17</day>
<month>11</month>
<year>2015</year>
</pub-date>
<!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>17</day>
<month>11</month>
<year>2015</year>
</pub-date>
<volume>10</volume>
<fpage>78</fpage>
<lpage>88</lpage>
<history>
<date date-type="received">
<day>15</day>
<month>6</month>
<year>2015</year>
</date>
<date date-type="rev-recd">
<day>12</day>
<month>11</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>11</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-statement>© 2015 The Authors</copyright-statement>
<copyright-year>2015</copyright-year>
<license license-type="CC BY-NC-ND" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
<license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
</license>
</permissions>
<abstract>
<p>Multivariate pattern analysis (MVPA) has been applied successfully to task-based and resting-based fMRI recordings to investigate which neural markers distinguish individuals with autistic spectrum disorders (ASD) from controls. While most studies have focused on brain connectivity during resting state episodes and regions of interest approaches (ROI), a wealth of task-based fMRI datasets have been acquired in these populations in the last decade. This calls for techniques that can leverage information not only from a single dataset, but from several existing datasets that might share some common features and biomarkers. We propose a fully data-driven (voxel-based) approach that we apply to two different fMRI experiments with social stimuli (faces and bodies). The method, based on Support Vector Machines (SVMs) and Recursive Feature Elimination (RFE), is first trained for each experiment independently and each output is then combined to obtain a final classification output. Second, this RFE output is used to determine which voxels are most often selected for classification to generate maps of significant discriminative activity. Finally, to further explore the clinical validity of the approach, we correlate phenotypic information with obtained classifier scores. The results reveal good classification accuracy (range between 69% and 92.3%). Moreover, we were able to identify discriminative activity patterns pertaining to the social brain without relying on a priori ROI definitions. Finally, social motivation was the only dimension which correlated with classifier scores, suggesting that it is the main dimension captured by the classifiers. Altogether, we believe that the present RFE method proves to be efficient and may help identifying relevant biomarkers by taking advantage of acquired task-based fMRI datasets in psychiatric populations.</p>
</abstract>
<abstract abstract-type="author-highlights">
<title>Highlights</title>
<p>
<list list-type="simple">
<list-item id="u0005">
<label>•</label>
<p>A classification method to identify common neural markers across several fMRI datasets is proposed.</p>
</list-item>
<list-item id="u0010">
<label>•</label>
<p>This method is applied to classify patients with autism spectrum disorders and controls.</p>
</list-item>
<list-item id="u0015">
<label>•</label>
<p>The method classified autistic patients from controls with an accuracy ranging from 69% to 92.3%.</p>
</list-item>
<list-item id="u0020">
<label>•</label>
<p>Discriminative activity patterns were found in several regions of the social brain.</p>
</list-item>
<list-item id="u0025">
<label>•</label>
<p>Classifier outputs were correlated with social motivation.</p>
</list-item>
</list>
</p>
</abstract>
<kwd-group>
<title>Keywords</title>
<kwd>fMRI</kwd>
<kwd>Autistic spectrum disorder</kwd>
<kwd>Diagnosis</kwd>
<kwd>Pattern classification</kwd>
<kwd>SVM</kwd>
<kwd>Recursive Feature Elimination</kwd>
<kwd>Emotion</kwd>
<kwd>Anger</kwd>
<kwd>Face perception</kwd>
<kwd>Body perception</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="s0005">
<label>1</label>
<title>Introduction</title>
<p>Identifying biomarkers in psychiatry is a challenge that has been the focus of intense research in the past decade. Multiple approaches have been used to overcome this challenge, including attempts to identify biomarkers in genetics, metabolism or neuroimaging (<xref ref-type="bibr" rid="bb0120">Goldani et al., 2014</xref>). As far as functional neuroimaging (fMRI) is concerned, the recent development of multivariate pattern classification (MVPA) methods to brain imaging data appears to be a promising approach (<xref ref-type="bibr" rid="bb0090">Ecker and Murphy, 2014</xref>). One of the main advantages of these methods is that information sensitivity is much higher compared to the standard univariate approaches routinely used in neuroscience. Specifically, machine learning makes it possible to retrieve patterns of information within populations of voxels, that univariate analyses may fail to reveal (<xref ref-type="bibr" rid="bb0135">Haxby et al., 2001</xref>, <xref ref-type="bibr" rid="bb0140">Haxby et al., 2014</xref>). Applied to psychiatry, MVPA is a promising method to detect brain states that discriminate patients from controls and thus constitutes a valuable tool to identify potential biomarkers (<xref ref-type="bibr" rid="bb0190">Mourão-Miranda et al., 2005</xref>, <xref ref-type="bibr" rid="bb0220">Pereira et al., 2009</xref>). In recent years, MVPA has indeed successfully been used on fMRI data to classify patients with major depression (<xref ref-type="bibr" rid="bb0110">Fu et al., 2008</xref>) or drug addiction (<xref ref-type="bibr" rid="bb0310">Zhang et al., 2011</xref>) with accuracy rates ranging from 70% to 80%.</p>
<p>However, given the wealth of existing fMRI datasets collected in psychiatry research, there is a need for techniques that can go beyond the analysis of single datasets and that allow researchers to leverage information from multiple datasets at once. Such methods would increase biomarker sensitivity and allow us to make the most of existing data. While there are obvious benefits in reanalyzing large datasets, the absence of efficient methods to merge information across datasets makes the process quite labor-intensive. Existing methods such as non-parametric permutation tests (<xref ref-type="bibr" rid="bb0205">Nichols and Holmes, 2001</xref>) and searchlight methods (<xref ref-type="bibr" rid="bb0175">Kriegeskorte et al., 2006</xref>) cannot be directly applied to mine information from several experiments, or when the assumption of sample independence does not hold.</p>
<p>This rationale applies particularly well to Autism Spectrum Disorders (ASD), where large corpuses of brain data have been collected and, for a good fraction of them, made publicly available. Autism is a neurodevelopmental disorder characterized by a unique profile of impaired social interaction and communication which takes the form of an inadequate appreciation and modulation of behavior according to socio-emotional information (<xref ref-type="bibr" rid="bb0305">World Health Organization, 1992</xref>). ASD individuals display hypo-connectivity in brain networks engaged during rest (<xref ref-type="bibr" rid="bb0155">Kennedy et al., 2006</xref>) and aberrant activity in several nodes of the “social brain” (i.e. fusiform gyrus, superior temporal sulcus and amygdala) while they process social or emotional information (<xref ref-type="bibr" rid="bb0080">Dichter, 2012</xref>, <xref ref-type="bibr" rid="bb0215">Pelphrey et al., 2004</xref>).</p>
<p>To date, most ASD fMRI classification studies have used resting-state functional connectivity patterns (<xref ref-type="bibr" rid="bb0015">Anderson et al., 2011</xref>, <xref ref-type="bibr" rid="bb0075">Deshpande et al., 2013</xref>, <xref ref-type="bibr" rid="bb0145">Iidaka, 2014</xref>, <xref ref-type="bibr" rid="bb0195">Murdaugh et al., 2012</xref>, <xref ref-type="bibr" rid="bb0315">Zhou et al., 2014</xref>) and only two have applied task-based paradigms that tap into ASD core social and emotional deficits (<xref ref-type="bibr" rid="bb0065">Coutanche et al., 2011</xref>, <xref ref-type="bibr" rid="bb0075">Deshpande et al., 2013</xref>). Using a limited set of a priori regions of interest (ROI) pertaining to the social brain, both studies showed hypo-connectivity and hypo-activation in regions involved in face processing or theory of mind, functions that are indeed atypical in ASD participants. While restricting classification to a limited set of ROI is a laudable conservative approach, combining information from multiple fMRI datasets may help to improve the detection reliability of relevant biomarkers, in particular those having a small spatial extent. This goal requires a data-driven approach that can mine information from the entire brain at the voxel-wise level.</p>
<p>The goal of this paper is to propose a multivariate method that can combine information from several studies to detect activity patterns at the voxel-wise level which are significantly predictive of autism. We used data from two distinct experiments acquired in the same group of ASD and control participants. As in Coutanche et al.'s study (<xref ref-type="bibr" rid="bb0065">Coutanche et al., 2011</xref>), the tasks we used were initially designed for univariate analysis and were not planned with MVPA in mind. Both tasks required to process emotional stimuli under different conditions of social relevance or feature-based attention: the first experiment investigated the perception of angry or neutral faces with direct or averted gaze (adapted from <xref ref-type="bibr" rid="bb0060">Conty et al., 2012</xref>); the second experiment required participants to direct attention to or away from angry and neutral body expressions (adapted from <xref ref-type="bibr" rid="bb0235">Pichon et al., 2012</xref>). We report two analyses on these datasets. In Analysis 1, we estimated the classifier's ability to discriminate patients from controls after training the classifier on both studies. In Analysis 2, we extended this diagnosis-based approach to assess whether this classifier is correlated with individual differences in social motivation, a dimension of behavior that likely plays an important role in social deficits observed in ASD. Moreover, given that recent studies have raised the concern that head motion may introduce spurious biases in classification problems (<xref ref-type="bibr" rid="bb0070">Deen and Pelphrey, 2012</xref>, <xref ref-type="bibr" rid="bb0255">Power et al., 2012</xref>), we compared the results of our classifiers after regressing out 6 motion parameters (x, y, z, pitch, yaw, roll), which is still one standard practice in the field of BOLD imaging, with a more stringent method which includes 24 motion parameters that has been used elsewhere (<xref ref-type="bibr" rid="bb0260">Power et al., 2014</xref>, <xref ref-type="bibr" rid="bb0280">Satterthwaite et al., 2013</xref>).</p>
</sec>
<sec id="s0010">
<label>2</label>
<title>General method</title>
<sec id="s0015">
<label>2.1</label>
<title>Participants</title>
<p>All participants gave their informed written consent and the study was conducted in accordance with the Declaration of Helsinki and the local Ethics Committee. The sample comprised 29 adults, 15 with ASD and 14 Typically Developing (TD) subjects. All ASD participants had received a formal diagnosis of an ASD by licensed psychologists or psychiatrists according to standard diagnostic criteria (<xref ref-type="bibr" rid="bb0010">American Psychiatric Association, 2000</xref>) and using module 4 from the Autism Diagnostic Observational Schedule (ADOS, <xref ref-type="bibr" rid="bb0185">Lord et al., 2000</xref>). Participants were matched on age and IQ (<xref ref-type="table" rid="t0005">Table 1</xref>). As it is often the case, participants in the ASD group had higher trait anxiety scores than controls. This potential confound was taken into account in our analyses by ensuring that classification scores were uncorrelated to anxiety scores.</p>
</sec>
<sec id="s0020">
<label>2.2</label>
<title>Experimental procedures</title>
<sec id="s0025">
<label>2.2.1</label>
<title>The static faces task (Experiment 1)</title>
<p>This experiment aimed at addressing whether ASD participants automatically process anger expressions directed at themselves (compared to averted expressions) as self-relevant communicative signals. It was adapted from a previous study (<xref ref-type="bibr" rid="bb0060">Conty et al., 2012</xref>) and crossed two factors: gaze direction and emotion (see below). Participant's task was to press a button whenever a face was presented upside down. This oddball paradigm has the advantage of leaving the trials of interest uncontaminated by motor responses.</p>
<p>We used color pictures of 10 actors (5 males) for which we manipulated two factors: 1) gaze direction (direct gaze condition: head, eye-gaze directed toward the participant; averted gaze condition: head, eye-gaze averted by 30°; and 2) emotional expression (angry or neutral). An additional picture was taken of each actor, with a neutral expression, arms by their sides with an intermediate eye direction of 15°. This position is thereafter referred to as the “initial position”. The full description of the stimuli can be found in <xref ref-type="bibr" rid="bb0060">Conty et al. (2012)</xref>. In the scanner, each trial began by a uniform gray screen (800 ms) followed by a fixation area (500 ms) consisting of a central red fixation point and four red angles. We instructed participants to fixate the central point and to keep their attention inside the fixation area at the level of the central point. An apparent movement was then created by the consecutive presentation of two pictures. The first picture showed the actor in the initial position during a random time (mean duration = 450 ms, range 300–600 ms) and was immediately followed by a second picture of the same actor in one of the 4 conditions of interest (<xref ref-type="fig" rid="f0005">Fig. 1</xref>A.). This second stimulus remained on the screen for 1300 ms. A total of 230 trials were presented including, in addition to the 160 trials of interest (10 actors ∗ 4 conditions ∗ 2 directions ∗ 2 repetitions), 20 oddballs (the second picture is upside-down) and 50 null events (mean duration = 3050 ms).</p>
</sec>
<sec id="s0030">
<label>2.2.2</label>
<title>The dynamic bodies attention task (Experiment 2)</title>
<p>This experiment aimed at drawing attention toward or diverting attention from the emotional meaning of movie-clips depicting angry, fear and neutral body actions. It was adapted from a previous study (<xref ref-type="bibr" rid="bb0235">Pichon et al., 2012</xref>) and crossed two factors: attention (to emotion stimuli/to color dots) and emotion (anger/fear/neutral). Attention was manipulated using instruction screens and alternated every six trials. During the emotion-naming instruction, subjects were asked to judge whether the action expressed anger, fear or was neutral. During the color-naming instruction, subjects viewed the same video-clips but were requested to detect the color of a small dot (red, green, blue, visual angle = 0.3°) that was briefly flashed during 40 ms. To minimize shifts in spatial attention between tasks, the location of the color dot was carefully chosen so that it always fell on the actor's upper body. Colors were randomized across emotional expressions and appeared in both tasks. The experiment was divided into two successive scanning runs of 21 min each. Within each run, stimuli were blocked by task and blocks alternated between series of attention to emotion or attention to color conditions. Each block contained 6 trials (including 5-seconds null events). A total of 36 blocks per attention condition was presented (142 video-clips + 74 null events per task). At the beginning of each block, subjects were instructed by a text on the screen lasting 2 s whether they had to recognize emotions or detect colors. Stimuli and null events were randomly mixed within blocks. After each stimulus presentation, subjects were instructed by a response screen (fear/anger/neutral or red/green/blue) to push the corresponding button using a response pad placed in their right hand. Subjects had a delay of 2 s to give their answer. The order of response options on the screen was randomized between trials to avoid motor anticipation related effects.</p>
<p>We used 73 movie-clips (24 anger, 23 fear and 24 neutral) with a length of 3 s for the present experiment. The full description and validation of this set of stimuli can be found in previous studies (<xref ref-type="bibr" rid="bb0225">Pichon et al., 2008</xref>, <xref ref-type="bibr" rid="bb0230">Pichon et al., 2009</xref>, <xref ref-type="bibr" rid="bb0235">Pichon et al., 2012</xref>). Briefly, actions were performed by professional actors who were filmed opening a door while they enacted different scenarios corresponding to angry, fear or neutral situations. Actors were facing the camera and facial expressions were blurred at the post-processing stage such that only information from the body was available. In the present manuscript, we only focused our analyses on anger and neutral stimuli, thus only the four conditions of interest of Experiment 2 are presented in <xref ref-type="fig" rid="f0005">Fig. 1</xref>B.</p>
</sec>
</sec>
<sec id="s0035">
<label>2.3</label>
<title>fMRI data acquisition</title>
<p>For both experiments, gradient-echo T2*-weighted transverse echo-planar images (EPI) with BOLD contrast and a high-resolution T1-weighted anatomical image were acquired with a 3T. Each volume contained 40 axial slices (TR/TE/Flip angle = 2000 ms/27 ms/78°, resolution = 64  ∗  64, voxels size 3 ∗ 3 mm with 3 mm thickness, Parallel Acquisition Technique (PAT) factor 2). A shimming procedure minimized inhomogeneity of the static magnetic field. Image acquisition started after the recording of three dummy volumes to avoid T1 saturation. For each subject and for each task, we collected 370 functional volumes. In addition, we collected a high-resolution T1-weighted anatomical image (TR/TI/TE/Flip angle = 2300 ms/ms/4.18 ms/9°, resolution = 256 ∗  256  ∗  64, voxels size 1 × 1 mm with 1 mm thickness, no IPAT acceleration, 176 sagittal slices).</p>
<sec id="s0040">
<label>2.3.1</label>
<title>Preprocessing of functional images</title>
<p>Image processing and analyses were carried out using SPM8 (Wellcome Dept. of Cognitive Neurology, London, UK). Functional images were realigned to the first volume by rigid body transformation, corrected for time differences, spatially normalized to the standard Montreal Neurological Institute (MNI) EPI template, resampled to an isotropic voxel size of 2 mm, and spatially smoothed with an isotropic 8 mm full-width at half-maximum (FWHM) Gaussian kernel.</p>
</sec>
<sec id="s0045">
<label>2.3.2</label>
<title>Subject-wise fMRI analysis</title>
<p>At the individual level, we performed standard analyses using the general linear model (GLM) in SPM8. Each task was modeled separately and included 4 conditions of interest as described above (exp. 1 crossed gaze orientation  ∗  emotion and exp. 2 crossed attention  ∗  emotion). For each condition, a covariate was calculated by convolving stick functions at the onset of stimulus presentation with the canonical hemodynamic response function (HRF). The length of each event encompassed the stimulation and the fixed response period (exp. 2 only). Estimation of models resulted in the calculation of a beta map for each experimental condition (i.e. a total of 8 beta maps/conditions per participant) that was used for classification.</p>
</sec>
<sec id="s0050">
<label>2.3.3</label>
<title>Regression of motion parameters</title>
<p>We gave careful consideration to the issue of residual head motion-related artifacts which may contaminate results even after volume realignment and lead to spurious patterns biasing the classification. We regressed out the effect of head motion by including the realignment parameters estimated at the preprocessing stage in all GLM models. We compared the results of our classification using models which included two flavors of motion parameters. In a first set of models, we followed a standard procedure and included the 6 raw realignment parameters (referred here as Rawrp6 models). In a second set of models (referred here as Friston24 models), we applied a more conservative method proposed by (<xref ref-type="bibr" rid="bb0105">Friston et al., 1996</xref>), which takes into account the participant's movement in the current but also in the previous scan and that has been used elsewhere (<xref ref-type="bibr" rid="bb0260">Power et al., 2014</xref>, <xref ref-type="bibr" rid="bb0280">Satterthwaite et al., 2013</xref>). The later method takes 24 motion regressors into consideration: the 6 raw realignment parameters, the same 6 regressors shifted-back by one time point to take into account the movement during the preceding scan (to capture a “memory” effect as in Volterra expansions), and each of the 12 regressors squared. We could have used other regression method, yet our goal was more to ensure that classification results remained high despite a more stringent control of motion artifact, rather than investigating the influence of different motion correction methods on classification per se.</p>
</sec>
</sec>
</sec>
<sec id="s0055">
<label>3</label>
<title>Analysis 1 — classification of autistic individuals and controls</title>
<sec id="s0060">
<label>3.1</label>
<title>Method</title>
<p>Our first goal was to train classifiers to discriminate patients from controls and to evaluate the classifier's performance. The proposed classification method took into account the specificity of our dataset, which consists of non-uniformly distributed beta maps generated from different tasks and experiments. Secondly, feature selection was applied to improve classification performance and to identify the most discriminative voxels across the two experiments.</p>
<p>We used the beta maps of each condition estimated at the individual step level as inputs for the classification to verify the hypothesis that fMRI brain activity discriminates ASD individuals from the controls. The space formed by the beta map voxels was directly used as the feature space for classification, without including any prior information such as a priori ROIs. Hence, our classification method was fully data-driven and aimed to find the most discriminative voxels. Indeed, using the information of the entire brain in an explorative approach may allow for: A) more accurate classification than restricting classification to a limited set of a priori regions; and B) identifying brain activity patterns which may not have been identified using a ROI approach, for instance because the discriminative activity would lie at the boundary of two ROIs.</p>
<p>Voxels that were not available due to brain volume differences between participants were rejected, still leading to a high dimensional space represented by N = 186217 features (i.e. beta map voxels). Many machine learning algorithms are available and applicable to fMRI data (<xref ref-type="bibr" rid="bb0220">Pereira et al., 2009</xref>). We opted for a Support Vector Machine (SVM), as implemented in Matlab 2014a (The MathWorks Inc.), given its ability to perform well in high dimensional spaces (<xref ref-type="bibr" rid="bb0025">Bishop, 2006</xref>). Since the number of samples employed for training the SVM is rather low compared to the dimension of the feature space, the two classes are always separable by a linear hyperplane and employing non-linear decision boundaries is of limited interest. We used a linear SVM with a model of the form:<disp-formula id="fo0005"><mml:math altimg="si1.gif" id="M1" overflow="scroll"><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:math></disp-formula>where <bold><italic>x</italic></bold><sub><bold><italic>i</italic></bold></sub> represent a feature vector (i.e. a 3D beta map flattened into a vector). When minimizing its cost function to obtain the weights <bold><italic>w</italic></bold>, the SVM relies on the parameter <italic>C</italic> that adjusts the tradeoff between misclassification and regularization. Following the advices in <xref ref-type="bibr" rid="bb9000">Laconte et al. (2005)</xref>, regularization was achieved by setting the <italic>C</italic> parameter to 1. The class <italic>y</italic><sub><italic>i</italic></sub> associated to a given feature vector <bold><italic>x</italic></bold><sub><bold><italic>i</italic></bold></sub> is determined by the nature of the corresponding participant with:<disp-formula id="fo0010"><mml:math altimg="si2.gif" id="M2" overflow="scroll"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.25em"></mml:mspace></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mn>1</mml:mn><mml:mspace width="0.25em"></mml:mspace></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi mathvariant="italic">if</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">the</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">participant</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">is</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi>a</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">control</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:mi mathvariant="italic">if</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">the</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">participant</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">is</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">diagnosed</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi mathvariant="italic">with</mml:mi><mml:mspace width="0.25em"></mml:mspace><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p>
<p>It follows that <italic>f</italic>(<bold><italic>x</italic></bold><sub><bold><italic>i</italic></bold></sub>) will be negative (respectively positive) for samples classified as controls (respectively ASD).</p>
<p>As for most classification methods, SVMs make the assumption that the input data is identically and independently distributed. In our case, the assumption of identically distributed data is violated since beta maps come from several experimental conditions which can activate different brain regions. For this reason a classifier was trained on the data of each condition independently (2 experiments ∗ 4 conditions per experiment = 8 conditions) and the decisions of these classifiers were subsequently combined to obtain a final decision for each participant. This fusion of classifiers decisions was achieved by averaging the outputs <italic>f</italic>(<bold><italic>x</italic></bold><sub><bold><italic>i</italic></bold></sub>) of each condition <italic>i</italic> belonging to a given participant <italic>j</italic>, which gives a final decision score <inline-formula><mml:math altimg="si3.gif" id="M3" overflow="scroll"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mi>j</mml:mi></mml:mfenced></mml:math></inline-formula>. For one participant where an experiment was missing, the decision was taken using the available conditions. The sign of <inline-formula><mml:math altimg="si3.gif" id="M4" overflow="scroll"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mi>j</mml:mi></mml:mfenced></mml:math></inline-formula> then determined the class assigned to the participant <italic>j</italic>.</p>
<sec id="s0065">
<label>3.1.1</label>
<title>SVM training and leave-one-participant out cross-validation method</title>
<p>This method was employed to test the performance of the classification method on unseen data. For each participant and each condition, a classifier was trained using the data of all other participants (on the same condition); the obtained model was then applied on the beta map of the tested participant. As described above average fusion was performed to combine the models of both experiments and obtain a final decision per participant. Finally performance was computed. Two types of fusion were employed by combining different conditions together to assess:<list list-type="simple"><list-item id="o0030"><label>1.</label><p>whether the combination of all models (i.e. both experiments) improved the classification accuracy; the performance obtained from combining the conditions of all experiments (i.e. 8 conditions in total) was compared with the performance obtained from combining the conditions separately for each experiment (i.e. 4 conditions per experiment);</p></list-item><list-item id="o0035"><label>2.</label><p>whether models trained on emotional versus neutral information improved classification accuracy; the performance obtained from combining the conditions corresponding to angry expressions was compared with the performance obtained with combining the conditions corresponding to neutral expressions (independently of the experiment, 4 conditions in both cases).</p></list-item></list></p>
<p>The complete classification framework, in the case data from the two experiments was used, is depicted in <xref ref-type="fig" rid="f0010">Fig. 2</xref>. Using this method, 8 classifiers were trained (4 conditions per experiment ∗ 2 experiments) at each cross validation loop. Given the high accuracy variance of the leave-one-out cross-validation method, a leave-pair-out cross-validation was also employed. As results from both methods were very similar, only the leave-one-out results are reported here.</p>
</sec>
<sec id="s0070">
<label>3.1.2</label>
<title>Classification using SVM Recursive Feature Elimination</title>
<p>Although SVMs achieve good performance in high dimensional spaces, they can still benefit from feature selection methods. Therefore we employed a modified version of the SVM Recursive Feature Elimination (SVM RFE) algorithm (<xref ref-type="bibr" rid="bb0130">Guyon et al., 2002</xref>) to restrain the classification to a subset of discriminant voxels. In SVM RFE the classification is first achieved on the whole set of features using a linear SVM classifier. The feature with the lowest weight <italic>w</italic><sub><italic>i</italic></sub> is eliminated and the procedure is iterated on the remaining features up to the point where no feature remains. The features are then ranked according to their order of elimination, the first eliminated feature being the worse. Because the number of features is very high, we chose to speed up the RFE algorithm by removing 10% of the remaining features at each iteration. Hence, many features are rejected at the first iteration while during subsequent iterations, the algorithm rejects fewer and fewer features and becomes more and more specific in feature ranking. The value of 10% was chosen to obtain a reasonable processing time while trying to keep the number of rejected features low. Once features are ranked, it is possible to select the best (b<sup>⁎</sup>) features on the training set to perform the classification. The number b<sup>⁎</sup> was chosen by nested cross-validation (<xref ref-type="fig" rid="f0010">Fig. 2</xref>) among several possible <italic>b<sub>i</sub></italic> values. For computational speed, the <italic>b</italic><sub>i</sub> numbers were following the geometric progression defined below:<disp-formula id="fo0015"><mml:math altimg="si5.gif" id="M5" overflow="scroll"><mml:mfenced open="{"><mml:mtable columnalign="center"><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr columnalign="center"><mml:mtd columnalign="center"><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mspace width="0.25em"></mml:mspace><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" open="[" separators=","><mml:mn>0</mml:mn><mml:mn>11</mml:mn></mml:mfenced><mml:mtext>.</mml:mtext></mml:math></disp-formula></p>
<p>The <italic>b</italic><sub>i</sub>'s were rounded to select an exact number of features. Using this method the number of selected features ranged from <italic>b</italic><sub>11</sub> = 45 to <italic>b</italic><sub>0</sub> = 93109.</p>
</sec>
<sec id="s0075">
<label>3.1.3</label>
<title>Classification performance</title>
<p>The classification performance was measured using accuracy (percentage of correctly classified participants), sensitivity (i.e. recall) and specificity. The significance of the classification accuracy was tested using a binomial test with the null hypothesis that the class labels are estimated randomly and equiprobably. Given that Binomial tests can be too lenient when applied on small datasets (<xref ref-type="bibr" rid="bb0210">Noirhomme et al., 2014</xref>), we also performed permutation tests. The two tests were found to be very similar and only the Binomial tests are presented.</p>
</sec>
<sec id="s0080">
<label>3.1.4</label>
<title>Visualization of the most discriminative voxels</title>
<p>After testing the classification performance, we sought to identify the most discriminative voxels across experiments to render them on a whole-brain anatomical volume. A possibility is to employ permutation tests to find which of the classifier weights <italic>w</italic> are significantly high or low. However this method is suboptimal because spatial correlation drives the weights of correlated (but discriminative) voxels toward zero (<xref ref-type="bibr" rid="bb0220">Pereira et al., 2009</xref>). This is particularly problematic for our data-driven approach considering all voxels of the brain volume. Another solution could be to combine correlated voxels together using, for instance, a searchlight algorithm (<xref ref-type="bibr" rid="bb0175">Kriegeskorte et al., 2006</xref>). Here, we propose to perform the voxel analysis using the output of the previously described SVM RFE algorithm. The basic idea is that voxels which are often well ranked by SVM RFE are significantly discriminative, while those which are not well ranked or only occasionally well ranked are not very discriminative. As proposed in <xref ref-type="bibr" rid="bb0035">Breitling et al (2004)</xref>, the rank product test can be used to determine which features (i.e. voxels) are better ranked than chance. When the number of features and the rank products are high, it is possible to use an accurate and cost effective approximation (gamma distribution based) of the rank product test (<xref ref-type="bibr" rid="bb0100">Eisinga et al., 2013</xref>, <xref ref-type="bibr" rid="bb0170">Koziol, 2010</xref>). Since we are dealing with N = 186217 voxels, this Gamma test was employed. As defined in Koziol (<xref ref-type="bibr" rid="bb0170">Koziol, 2010</xref>), the test consists of computing the <italic>z</italic> statistic from <italic>k</italic> rankings. The <italic>z</italic> statistic follows a Gamma distribution under the null hypothesis that the features are ranked equiprobably with:<disp-formula id="fo0020"><label>(2)</label><mml:math altimg="si6.gif" id="M6" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.25em"></mml:mspace><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover></mml:mstyle><mml:mo>log</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Z</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mfenced close=")" open="(" separators=","><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>r<sub>i</sub></italic> is the rank of the tested feature for ranking <italic>i</italic>. It is necessary to generate <italic>k</italic> independent rankings of voxels to apply the test. This was achieved by building a model for each condition. As described previously, the SVM RFE algorithm was applied 8 times (one time per condition), each iteration generating a unique ranking of voxels. The Gamma test was then applied to sets of rankings to find the most discriminative voxels. More precisely, this test was applied on the conditions of each experiment independently (<italic>k</italic> = 4) and on both experiments (<italic>k</italic> = 8) leading to the generation of 3 maps representing discriminative brain activities for Experiment 1, Experiment 2 and the fusion of the two experiments. Since the number <italic>k</italic> of rankings can be different (4 or 8), we adjusted the Gamma test so that the <italic>z</italic> statistics are comparable by replacing the sum in Eq. <xref ref-type="disp-formula" rid="fo0020">(2)</xref> by an average:<disp-formula id="fo0025"><mml:math altimg="si7.gif" id="M7" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mspace width="0.25em"></mml:mspace><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mstyle><mml:mo>log</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>Z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>~</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mfenced close=")" open="(" separators=","><mml:mi>k</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac></mml:mfenced><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
<p>This method allows finding voxels which are discriminative among all conditions and experiments by selecting voxels where <inline-formula><mml:math altimg="si8.gif" id="M8" overflow="scroll"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>≥</mml:mo><mml:mi>z</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:mfenced><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:mfrac></mml:math></inline-formula>. In our study <italic>α</italic> was set to 0.05. Note that the proposed modification does not change the statistical significance of the test since <italic>p</italic>(<italic>Z</italic>′ ≥ <italic>z</italic> ') = <italic>p</italic>(<italic>Z</italic> ≥ <italic>z</italic>).</p>
<p>However, the proposed test is not informative of the direction of the effect. To circumvent this issue, we retrieved voxels' weight signs by training a unique SVM model on the full dataset and we displayed in <xref ref-type="fig" rid="f0015">Fig. 3</xref> the <italic>z</italic> statistics (which are always positive) multiplied by the sign of the associated weights to visualize the main direction of the effect. Anatomical labeling was performed with reference to the anatomy toolbox (<xref ref-type="bibr" rid="bb0095">Eickhoff et al., 2005</xref>). Coordinates of homologue regions in both hemispheres were pooled together in <xref ref-type="table" rid="t0020">Table 4</xref> when distance was less than 10 mm. Rendering was made using MRIcron (<xref ref-type="bibr" rid="bb0270">Rorden et al., 2007</xref>) and the standard Colin brain available in SPM8.</p>
</sec>
</sec>
<sec id="s0085">
<label>3.2</label>
<title>Results</title>
<sec id="s0090">
<label>3.2.1</label>
<title>Classification accuracy</title>
<p>The accuracy, sensitivity and specificity of the different classification methods and movement correction are reported in <xref ref-type="table" rid="t0010">Table 2</xref>. For better readability, sensitivity and specificity are reported only for the SVM RFE feature selection algorithm, which performed best in most cases.</p>
<p>When both experiments were considered together for classification, and no feature selection was applied, the Friston24 motion correction method reached a better performance than the Rawrp6 method (Friston24: 82.8%, Rawrp6: 72.4%). This suggests that regressing out residual motion helps improving the classification. When applying feature selection (RFE), the accuracy remained relatively unchanged for the Friston24 method but improved considerably for the Rawrp6 method (Friston24: 79.3%, Rawrp6: 89.7%). While specificity (true negative rate) was equivalent for both methods (80%), sensitivity (true positive rate) was much higher in the Rawrp6 method than in the Friston24 method (100% versus 78.6%), possibly because of the influence of residual motion artifacts that were less well accounted for by the Rawrp6 method than by the Friston24 method.</p>
<p>When experiments were considered separately, the best accuracy was achieved with Experiment 2 (performance: 92.3%, sensitivity: 92.3%, specificity: 92.3%) which manipulated attention toward or away from emotional bodily actions. In conclusion, these results confirmed that potential residual motion in the signal has a relatively low but non-null influence on classification and indicates that the Friston24 correction method helps to reduce spurious classification linked to residual head-motion. Moreover, our results indicate that there seems to be an advantage of using SVM RFE to increase classification accuracy (mostly for the Rawrp6 models) since performance improved in 4 analyses (out of 6), was left unchanged in 1 analysis, and marginally decreased in 1 analysis. This also demonstrates that brain activity was discriminative in all experiments, despite the explorative method we employed.</p>
<p>To ensure that RFE and fusion are not selecting features related to movement, we computed correlations between the continuous output of classifiers and 13 motion parameters. Importantly, we observed no correlations for the fusion of both experiments after Friston24 motion correction and/or the application of RFE feature selection (all p &gt; 0.05 uncorrected). Furthermore, the RFE algorithm helped reducing the influence of motion on classifiers' output. For additional details, see Supplementary Fig. 1 and results.</p>
<p>Finally, since it has been proposed that ASD individuals have a specific deficit in processing emotional cues, we tested whether or not classification performed on “anger” conditions achieved a better accuracy than when it was performed on “neutral” conditions. To do so, we combined the classifier scores of the two experiments for anger and neutral conditions separately. Results indicate that accuracy for anger or neutral conditions was very similar with an average accuracy difference of 1.7% (<xref ref-type="table" rid="t0015">Table 3</xref>). It is important to note, however, that the fact that classification with emotional stimuli was only marginally better than the one with neutral stimuli does not necessarily mean that emotions are not beneficial to classification. The use of a diverse set of social stimuli probably increases subjects' attention, which benefits to classification overall.</p>
</sec>
<sec id="s0095">
<label>3.2.2</label>
<title>Visualization of the most discriminative voxels</title>
<p>The most discriminative voxels (see <xref ref-type="fig" rid="f0015">Fig. 3</xref>) across experiments were found in regions related to social cognition, namely regions involved in the processing of faces and bodies [FFA: fusiform face area, OFA: occipital face area, EBA: extrastriate body area — (<xref ref-type="bibr" rid="bb0150">Kanwisher et al., 1997</xref>, <xref ref-type="bibr" rid="bb0265">Puce et al., 1996</xref>), and STS: sulcus temporal superior — (<xref ref-type="bibr" rid="bb0005">Allison et al., 2000</xref>, <xref ref-type="bibr" rid="bb0115">Giese and Poggio, 2003</xref>, <xref ref-type="bibr" rid="bb0250">Pitcher, 2014</xref>)], active during mentalizing [TPJ: temporo-parietal junction and precuneus — (<xref ref-type="bibr" rid="bb0040">Castelli et al., 2000</xref>, <xref ref-type="bibr" rid="bb0275">Samson et al., 2004</xref>)] or during action and emotion perception [PM: premotor cortex — (<xref ref-type="bibr" rid="bb0125">Grèzes et al., 2007</xref>, <xref ref-type="bibr" rid="bb0225">Pichon et al., 2008</xref>, <xref ref-type="bibr" rid="bb0235">Pichon et al., 2012</xref>)]. These regions consistently showed reduced contribution in ASD participants compared to controls. The fusion of both experiments dramatically increased the significance of discriminative features, which is probably due to the increased sample size with a stable effect size. The fusion of both experiments increased the number of significant discriminative voxels by roughly 50%, with 139 significant voxels for Experiment 1 (gaze), 219 for Experiment 2 (bodies) and 546 for the fusion of both experiments.</p>
</sec>
</sec>
</sec>
<sec id="s0100">
<label>4</label>
<title>Analysis 2 — dimensional approach</title>
<sec id="s0105">
<label>4.1</label>
<title>Methods</title>
<p>In analysis 1, we focused on the accuracy of the classifier to discriminate participants based on their diagnosis and we examined discriminative brain activity patterns between patients and controls. This first step was useful to compare the benefits of our cross-experiment MVPA methodology with classification performed on single tasks. However, recent recommendations in psychiatry have emphasized the need to go beyond diagnostic boundaries and to adopt a more dimensional approach for a finer understanding of the neurobiological substrate of psychiatric conditions. The rationale is that until the formulation of clinical diagnosis is improved, research in neuroscience should focus on discrete dimensions of behavior which are likely to be more directly linkable to neurobiology (<xref ref-type="bibr" rid="bb0180">London, 2014</xref>). One promising dimension in the domain of ASD research is social motivation (<xref ref-type="bibr" rid="bb0055">Chevallier et al., 2012b</xref>). Social motivation can be described as a set of biological mechanisms driving individuals to preferentially orient their attention to the social world and to treat social interactions as rewarding. Social motivation can be assessed using a number of tools including self-report questionnaires (<xref ref-type="bibr" rid="bb0085">Eckblad et al., 1982</xref>). In analysis 2, we therefore departed from the standard diagnosis-based approach in order to assess whether the classification outputs <inline-formula><mml:math altimg="si3.gif" id="M9" overflow="scroll"><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mfenced close=")" open="("><mml:mi>j</mml:mi></mml:mfenced></mml:math></inline-formula> were related to social anhedonia (SAS) and other personality or diagnostic measures. Since the classifiers were trained to distinguish participants with ASD from control participants, we expected that correlations would be driven by mere group differences. Hence we used partial correlations to remove the group effect. We computed additional correlations within each group whenever the partial correlation was marginally correlated (p &lt; 0.1).</p>
<sec id="s0110">
<label>4.1.1</label>
<title>Questionnaires</title>
<p>Participants completed the revised-Social Anhedonia Scale (<xref ref-type="bibr" rid="bb0165">Kosmadakis et al., 1995</xref>), which is a 40-item true/false scale commonly used to assess the ability to anticipate and experience interpersonal pleasure (e.g., being with people, talking, exchanging expressions of feelings, and doing things with others). High scores reflect diminished pleasurable responses, hence greater social anhedonia. We recently revealed selective social anhedonia (deficit in social desire and drive) in adolescents with ASD, with ASD severity (ADOS scores) correlating positively with the level of social anhedonia (<xref ref-type="bibr" rid="bb0050">Chevallier et al., 2012a</xref>). In addition to the ADOS, autistic traits were assessed using the AQ questionnaire (<xref ref-type="bibr" rid="bb0020">Baron-Cohen et al., 2001</xref>).</p>
<p>Participants also filled the STAI (form Y) questionnaire assessing state and trait anxiety (<xref ref-type="bibr" rid="bb0290">Spielberger et al., 1983</xref>). Indeed, anxiety is one of the most common psychiatric co-morbidities in ASD, with as many as 40–50% of individuals with ASD meeting conditions for clinical anxiety (<xref ref-type="bibr" rid="bb0160">Kerns et al., 2015</xref>). Since anxiety influences brain responses to emotional stimuli (<xref ref-type="bibr" rid="bb0030">Bishop, 2007</xref>, <xref ref-type="bibr" rid="bb0240">Pichon et al., 2015</xref>) and interferes with social motivation (<xref ref-type="bibr" rid="bb0200">Nettle and Bateson, 2012</xref>), we tested whether classification scores were related to inter-individual differences in anxiety (see <xref ref-type="table" rid="t0025">Table 5</xref>).</p>
</sec>
</sec>
<sec id="s0115">
<label>4.2</label>
<title>Results</title>
<p>Given that the SVM RFE algorithm performed the best, we used its classification outputs <italic>f(j)</italic> to estimate correlations with questionnaires and ASD scores in <xref ref-type="table" rid="t0030">Table 6</xref>. A first glance at the correlations shows that the Rawrp6 and Friston24 methods gave very similar results.</p>
<p>Interestingly, social anhedonia scores predicted classification scores in Experiment 2 (bodies) for the ASD group (r = 0.76, p &lt; .01, see <xref ref-type="fig" rid="f0020">Fig. 4</xref>) and for all subjects after removing the effect of group (r = .56, p &lt; .01). Importantly, neither trait (p = .09) nor state anxiety (p = .60) was related to classification scores when considering both groups. Multiple regression confirmed that social anhedonia predicted classification scores in ASD participants (t(6) = 2.57, p = .04) while anxiety state (p = .31) or trait (p = .21), IQ (p = .32) and age (p = .13) were not significant. Our result supports the suggestion that social motivation is an important factor to consider in autism research (<xref ref-type="bibr" rid="bb0055">Chevallier et al., 2012b</xref>).</p>
</sec>
</sec>
<sec id="s0120">
<label>5</label>
<title>Discussion</title>
<p>The data-driven method proposed in this paper combines BOLD measures from two heterogeneous experiments in order to classify ASD subjects and controls without any prior information such as the definition of ROIs. The originality of our approach is that the discriminative maps rely on feature selection rather than on the standard statistical methods commonly used in multivariate fMRI analyses. An advantage of this approach is that the computed statistic is only based on the rank of the feature. Consequently this method can be employed to combine heterogeneous data sources such as different fMRI experiments, different BOLD-related signals (beta maps, functional connectivity, etc.) or even different brain imaging modalities (i.e. structural MRI, voxel-based morphometry, functional MRI, and PET). The only requirement is that the brain images should be co-registered and have the same spatial resolution, a constraint that can easily be achieved at the preprocessing stage using realignment and interpolation methods.</p>
<p>Importantly, we show that the usage of a more stringent motion correction method than the classic inclusion of the 6 motion parameters in regression models helps to reduce potential residual influences of head motion on classification results. The use of SVM RFE improved classification accuracy for the Rawrp6 models, and this increase of performance was not due to motion artifacts since RFE reduced the correlation of classifiers' output with motion parameters observed in Experiment 1 (but not in Experiment 2 or after the fusion of both experiments). Taken together, these results hold the promise that the present method may become a valuable tool to help remove any potential residual influence of head movements in classification problems that involve comparing ASD Subjects (and more generally patients) with controls.</p>
<p>In the past few years, most fMRI classification studies relied on resting-state functional connectivity patterns and/or ROI analyses and reached categorization accuracy ranging from 79% to 96% (<xref ref-type="bibr" rid="bb0015">Anderson et al., 2011</xref>, <xref ref-type="bibr" rid="bb0075">Deshpande et al., 2013</xref>, <xref ref-type="bibr" rid="bb0145">Iidaka, 2014</xref>, <xref ref-type="bibr" rid="bb0195">Murdaugh et al., 2012</xref>, <xref ref-type="bibr" rid="bb0300">Wang et al., 2012</xref>, <xref ref-type="bibr" rid="bb0315">Zhou et al., 2014</xref>). Our method achieved accuracies in a similar range (between 69% and 92.3%) than the studies above relying on resting state connectivity measures. Note that even though RFE was not associated with obvious gains in all conditions, the best accuracy was obtained with this method (92.3%). These results are all the more encouraging that we faced a number of methodological challenges: 1) we relied on tasks that were not designed with multivariate pattern analysis in mind; 2) the samples we classified did not have the same distribution (i.e. they derived from different tasks and experiments); 3) we relied on relatively small samples lying in a highly dimensional space, which might have raised a curse of dimensionality issue. In response to this last challenge, our results indicate that feature selection with SVM RFE reduces the impact of the curse of dimensionality by selecting discriminant subsets of voxels. In response to the second challenge, the proposed fusion approach allowed to build a model for each task and condition thus solving the problem of non-identically distributed samples. Taken together, the present study confirms that classifiers can be successfully applied to mine information from multiple BOLD datasets without relying on a priori ROIs and even if they are not originally designed for multivariate pattern analysis.</p>
<p>Our second goal was to evaluate whether the fusion of heterogeneous data sources improved classification performance and revealed new additional topological information. We found that accuracy remained approximately the same when fusing the classifier outputs of our two experiments. However, the fusion method revealed 50% more significant voxels compared to the method taking each experiment separately. Specifically, the fusion led to an increase in the size of the largest significant clusters and to a disappearance of the smallest clusters. This suggests that the fusion method favors the selection of discriminative features that are common across experiments and validates the proposed method. Identified areas were astonishingly consistent with brain regions of the “social-brain” known to show aberrant functioning in ASD (<xref ref-type="bibr" rid="bb0045">Castelli et al., 2002</xref>, <xref ref-type="bibr" rid="bb0245">Pierce et al., 2004</xref>, <xref ref-type="bibr" rid="bb0285">Schultz, 2005</xref>, <xref ref-type="bibr" rid="bb0320">Zilbovicius et al., 2006</xref>). More specifically, we found a hypo-contribution of the fusiform gyrus and the occipital face area (OFA), which are both involved in face perception (<xref ref-type="bibr" rid="bb0150">Kanwisher et al., 1997</xref>, <xref ref-type="bibr" rid="bb0265">Puce et al., 1996</xref>); of the posterior STS, which plays a role in processing gaze direction (<xref ref-type="bibr" rid="bb0005">Allison et al., 2000</xref>), emotional displays (<xref ref-type="bibr" rid="bb0250">Pitcher, 2014</xref>) and biological motion (<xref ref-type="bibr" rid="bb0115">Giese and Poggio, 2003</xref>); and of the TPJ which is part of the mental state attribution network (<xref ref-type="bibr" rid="bb0040">Castelli et al., 2000</xref>, <xref ref-type="bibr" rid="bb0275">Samson et al., 2004</xref>).</p>
<p>Finally, to explore the clinical validity of our classifiers, we correlated classifier scores and phenotypic information. We focused on social motivation deficits, which are arguably an important dimension of the ASD phenotype and found that the classifier scores of the fusion method correlated with social motivation scores. In contrast to this dimensional approach past studies have often focused on overall diagnosis. <xref ref-type="bibr" rid="bb0015">Anderson et al., (2011)</xref> and <xref ref-type="bibr" rid="bb0065">Coutanche et al (2011)</xref>, for instance, found that their classifier scores correlated with ADOS total scores. Similarly, <xref ref-type="bibr" rid="bb0075">Deshpande et al. (2013)</xref> found that top rank features of connectivity measures were positively correlated with autistic traits' scores (AQ). In line with the RDoC framework (NIMH Research Domain Criteria), we would like to highlight that looking at relevant dimensions of behavior instead of overall diagnosis is a promising approach to understand the biological roots of ASD and, ultimately, to identify biomarkers.</p>
<p>In conclusion, the present study indicates that RFE is an interesting method to leverage information from several datasets and explore potential brain atypicalities in ASD or other psychiatric conditions (e.g. depression). In this paper, social brain areas were identified as most discriminative. This finding, however, is only a first step in the identification of potential biomarkers: first, our sample size was relatively limited, which prevents us from grasping the heterogeneity that is so characteristic of ASDs; second, and perhaps more importantly, we only compared participants with ASD to typically developing controls, which means that we cannot know whether the discriminative brain pattern we identified is specific to ASD. These cautionary notes have been underlined recently and suggest that “we must be patient when searching for an autism biomarker” (<xref ref-type="bibr" rid="bb0120">Goldani et al., 2014</xref>, <xref ref-type="bibr" rid="bb0295">Tager-Flusberg, 2014</xref>).</p>
</sec>
<sec id="s0130">
<title>Contributors</title>
<p>Study conception and design: Sylvie Berthoz, Coralie Chevallier, Laurence Conty, Julie Grèzes, and Swann Pichon.</p>
<p>Acquisition of data: Sylvie Berthoz, Coralie Chevallier, and Julie Grèzes.</p>
<p>Analysis and interpretation of data: Guillaume Chanel, Coralie Chevallier, Julie Grèzes, and Swann Pichon.</p>
<p>Drafting of manuscript: Guillaume Chanel, Coralie Chevallier, Julie Grèzes, and Swann Pichon.</p>
<p>Critical revision: All authors.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bb0005">
<element-citation id="rf0005" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Allison</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Puce</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>McCarthy</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Social perception from visual cues: role of the STS region</article-title>
<source/>Trends Cogn. Sci.
          <volume>4</volume>
<year>2000</year>
<fpage>267</fpage>
<lpage>278</lpage>
<pub-id pub-id-type="pmid">10859571</pub-id>
</element-citation>
</ref>
<ref id="bb0010">
<element-citation id="rf0010" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>American Psychiatric Association</surname>
</name>
</person-group>
<chapter-title>Diagnostic and Statistical Manual of Mental Disorders DSM-IV-TR Fourth Edition, Diagnostic and Statistical Manual of Mental Disorders</chapter-title>
<edition>4th edition</edition>
<year>2000</year>
<comment>(TR)</comment>
</element-citation>
</ref>
<ref id="bb0015">
<element-citation id="rf0015" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Anderson</surname>
<given-names>J.S.</given-names>
</name>
<name>
<surname>Nielsen</surname>
<given-names>J.A.</given-names>
</name>
<name>
<surname>Froehlich</surname>
<given-names>A.L.</given-names>
</name>
<name>
<surname>DuBray</surname>
<given-names>M.B.</given-names>
</name>
<name>
<surname>Druzgal</surname>
<given-names>T.J.</given-names>
</name>
<name>
<surname>Cariello</surname>
<given-names>A.N.</given-names>
</name>
<name>
<surname>Cooperrider</surname>
<given-names>J.R.</given-names>
</name>
<name>
<surname>Zielinski</surname>
<given-names>B.A.</given-names>
</name>
<name>
<surname>Ravichandran</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fletcher</surname>
<given-names>P.T.</given-names>
</name>
<name>
<surname>Alexander</surname>
<given-names>A.L.</given-names>
</name>
<name>
<surname>Bigler</surname>
<given-names>E.D.</given-names>
</name>
<name>
<surname>Lange</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Lainhart</surname>
<given-names>J.E.</given-names>
</name>
</person-group>
<article-title>Functional connectivity magnetic resonance imaging classification of autism</article-title>
<source/>Brain
          <volume>134</volume>
<year>2011</year>
<fpage>3742</fpage>
<lpage>3754</lpage>
<pub-id pub-id-type="pmid">22006979</pub-id>
</element-citation>
</ref>
<ref id="bb0020">
<element-citation id="rf0020" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Baron-Cohen</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Wheelwright</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Skinner</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Martin</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Clubley</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>The autism-spectrum quotient (AQ): evidence from Asperger syndrome/high-functioning autism, males and females, scientists and mathematicians</article-title>
<source/>J. Autism Dev. Disord.
          <volume>31</volume>
<year>2001</year>
<fpage>5</fpage>
<lpage>17</lpage>
<pub-id pub-id-type="pmid">11439754</pub-id>
</element-citation>
</ref>
<ref id="bb0025">
<element-citation id="rf0025" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Bishop</surname>
<given-names>C.M.</given-names>
</name>
</person-group>
<chapter-title>Pattern Recognition and Machine Learning</chapter-title>
<year>2006</year>
<publisher-name>Springer</publisher-name>
</element-citation>
</ref>
<ref id="bb0030">
<element-citation id="rf0030" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bishop</surname>
<given-names>S.J.</given-names>
</name>
</person-group>
<article-title>Neurocognitive mechanisms of anxiety: an integrative account</article-title>
<source/>Trends Cogn. Sci.
          <volume>11</volume>
<year>2007</year>
<fpage>307</fpage>
<lpage>316</lpage>
<pub-id pub-id-type="pmid">17553730</pub-id>
</element-citation>
</ref>
<ref id="bb0035">
<element-citation id="rf0035" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Breitling</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Armengaud</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Amtmann</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Herzyk</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Rank products: a simple, yet powerful, new method to detect differentially regulated genes in replicated microarray experiments</article-title>
<source/>FEBS Lett.
          <volume>573</volume>
<year>2004</year>
<fpage>83</fpage>
<lpage>92</lpage>
<pub-id pub-id-type="pmid">15327980</pub-id>
</element-citation>
</ref>
<ref id="bb0040">
<element-citation id="rf0040" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Castelli</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Happe</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Frith</surname>
<given-names>U.</given-names>
</name>
<name>
<surname>Frith</surname>
<given-names>C.D.</given-names>
</name>
</person-group>
<article-title>Movement and mind: a functional imaging study of perception and interpretation of complex intentional movement patterns</article-title>
<source/>Neuroimage
          <volume>12</volume>
<year>2000</year>
<fpage>314</fpage>
<lpage>325</lpage>
<pub-id pub-id-type="pmid">10944414</pub-id>
</element-citation>
</ref>
<ref id="bb0045">
<element-citation id="rf0045" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Castelli</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Frith</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Happe</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Frith</surname>
<given-names>U.</given-names>
</name>
</person-group>
<article-title>Autism, Asperger syndrome and brain mechanisms for the attribution of mental states to animated shapes</article-title>
<source/>Brain
          <volume>125</volume>
<year>2002</year>
<fpage>1839</fpage>
<lpage>1849</lpage>
<pub-id pub-id-type="pmid">12135974</pub-id>
</element-citation>
</ref>
<ref id="bb0050">
<element-citation id="rf0050" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chevallier</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Grèzes</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Molesworth</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Berthoz</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Happé</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Brief report: selective social anhedonia in high functioning autism</article-title>
<source/>J. Autism Dev. Disord.
          <volume>42</volume>
<year>2012</year>
<fpage>1504</fpage>
<lpage>1509</lpage>
<pub-id pub-id-type="pmid">21986875</pub-id>
</element-citation>
</ref>
<ref id="bb0055">
<element-citation id="rf0055" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chevallier</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Kohls</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Troiani</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Brodkin</surname>
<given-names>E.S.</given-names>
</name>
<name>
<surname>Schultz</surname>
<given-names>R.T.</given-names>
</name>
</person-group>
<article-title>The social motivation theory of autism</article-title>
<source/>Trends Cogn. Sci.
          <volume>16</volume>
<year>2012</year>
<fpage>231</fpage>
<lpage>239</lpage>
<pub-id pub-id-type="pmid">22425667</pub-id>
</element-citation>
</ref>
<ref id="bb0060">
<element-citation id="rf0060" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Conty</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Dezecache</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Hugueville</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Grezes</surname>
<given-names>J.</given-names>
</name>
</person-group>
<chapter-title>Early Binding of Gaze, Gesture, and Emotion: Neural Time Course and Correlates</chapter-title>
<year>2012</year>
<publisher-name>J. Neurosci</publisher-name>
</element-citation>
</ref>
<ref id="bb0065">
<element-citation id="rf0065" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Coutanche</surname>
<given-names>M.N.</given-names>
</name>
<name>
<surname>Thompson-Schill</surname>
<given-names>S.L.</given-names>
</name>
<name>
<surname>Schultz</surname>
<given-names>R.T.</given-names>
</name>
</person-group>
<article-title>Multi-voxel pattern analysis of fMRI data predicts clinical symptom severity</article-title>
<source/>Neuroimage
          <volume>57</volume>
<year>2011</year>
<fpage>113</fpage>
<lpage>123</lpage>
<pub-id pub-id-type="pmid">21513803</pub-id>
</element-citation>
</ref>
<ref id="bb0070">
<element-citation id="rf0070" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Deen</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Pelphrey</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Perspective: brain scans need a rethink</article-title>
<source/>Nature
          <volume>491</volume>
<year>2012</year>
<fpage>S20</fpage>
<pub-id pub-id-type="pmid">23136657</pub-id>
</element-citation>
</ref>
<ref id="bb0075">
<element-citation id="rf0075" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Deshpande</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Libero</surname>
<given-names>L.E.</given-names>
</name>
<name>
<surname>Sreenivasan</surname>
<given-names>K.R.</given-names>
</name>
<name>
<surname>Deshpande</surname>
<given-names>H.D.</given-names>
</name>
<name>
<surname>Kana</surname>
<given-names>R.K.</given-names>
</name>
</person-group>
<article-title>Identification of neural connectivity signatures of autism using machine learning</article-title>
<source/>Front. Hum. Neurosci.
          <volume>7</volume>
<year>2013</year>
<fpage>670</fpage>
<pub-id pub-id-type="pmid">24151458</pub-id>
</element-citation>
</ref>
<ref id="bb0080">
<element-citation id="rf0080" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dichter</surname>
<given-names>G.S.</given-names>
</name>
</person-group>
<article-title>Functional magnetic resonance imaging of autism spectrum disorders</article-title>
<source/>Dialogues Clin. Neurosci.
          <volume>14</volume>
<year>2012</year>
<fpage>319</fpage>
<lpage>351</lpage>
<pub-id pub-id-type="pmid">23226956</pub-id>
</element-citation>
</ref>
<ref id="bb0085">
<element-citation id="rf0085" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Eckblad</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Chapman</surname>
<given-names>L.J.</given-names>
</name>
<name>
<surname>Chapman</surname>
<given-names>J.P.</given-names>
</name>
<name>
<surname>Mishlove</surname>
<given-names>M.</given-names>
</name>
</person-group>
<chapter-title>The Revised Social Anhedonia Scale: University of Wisconsin</chapter-title>
<year>1982</year>
<publisher-name>Dep. Psychol. Unpubl. test</publisher-name>
<publisher-loc>Madison</publisher-loc>
</element-citation>
</ref>
<ref id="bb0090">
<element-citation id="rf0090" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ecker</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Murphy</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Neuroimaging in autism—from basic science to translational research</article-title>
<source/>Nat. Rev. Neurol.
          <volume>10</volume>
<year>2014</year>
<fpage>82</fpage>
<lpage>91</lpage>
<pub-id pub-id-type="pmid">24419683</pub-id>
</element-citation>
</ref>
<ref id="bb0095">
<element-citation id="rf0095" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eickhoff</surname>
<given-names>S.B.</given-names>
</name>
<name>
<surname>Stephan</surname>
<given-names>K.E.</given-names>
</name>
<name>
<surname>Mohlberg</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Grefkes</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fink</surname>
<given-names>G.R.</given-names>
</name>
<name>
<surname>Amunts</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Zilles</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>
<source/>Neuroimage
          <volume>25</volume>
<year>2005</year>
<fpage>1325</fpage>
<lpage>1335</lpage>
<pub-id pub-id-type="pmid">15850749</pub-id>
</element-citation>
</ref>
<ref id="bb0100">
<element-citation id="rf0100" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eisinga</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Breitling</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Heskes</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>The exact probability distribution of the rank product statistics for replicated experiments</article-title>
<source/>FEBS Lett.
          <volume>587</volume>
<year>2013</year>
<fpage>677</fpage>
<lpage>682</lpage>
<pub-id pub-id-type="pmid">23395607</pub-id>
</element-citation>
</ref>
<ref id="bb0105">
<element-citation id="rf0105" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Friston</surname>
<given-names>K.J.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Howard</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Frackowiak</surname>
<given-names>R.S.</given-names>
</name>
<name>
<surname>Turner</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Movement-related effects in fMRI time-series</article-title>
<source/>Magn. Reson. Med.
          <volume>35</volume>
<year>1996</year>
<fpage>346</fpage>
<lpage>355</lpage>
<pub-id pub-id-type="pmid">8699946</pub-id>
</element-citation>
</ref>
<ref id="bb0110">
<element-citation id="rf0110" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fu</surname>
<given-names>C.H.Y.</given-names>
</name>
<name>
<surname>Mourao-Miranda</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Costafreda</surname>
<given-names>S.G.</given-names>
</name>
<name>
<surname>Khanna</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Marquand</surname>
<given-names>A.F.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>S.C.R.</given-names>
</name>
<name>
<surname>Brammer</surname>
<given-names>M.J.</given-names>
</name>
</person-group>
<article-title>Pattern classification of sad facial processing: toward the development of neurobiological markers in depression</article-title>
<source/>Biol. Psychiatry
          <volume>63</volume>
<year>2008</year>
<fpage>656</fpage>
<lpage>662</lpage>
<pub-id pub-id-type="pmid">17949689</pub-id>
</element-citation>
</ref>
<ref id="bb0115">
<element-citation id="rf0115" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Giese</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Poggio</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Neural mechanisms for the recognition of biological movements</article-title>
<source/>Nat. Rev. Neurosci.
          <volume>4</volume>
<year>2003</year>
<fpage>179</fpage>
<lpage>192</lpage>
<pub-id pub-id-type="pmid">12612631</pub-id>
</element-citation>
</ref>
<ref id="bb0120">
<element-citation id="rf0120" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goldani</surname>
<given-names>A.A.S.</given-names>
</name>
<name>
<surname>Downs</surname>
<given-names>S.R.</given-names>
</name>
<name>
<surname>Widjaja</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Lawton</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Hendren</surname>
<given-names>R.L.</given-names>
</name>
</person-group>
<article-title>Biomarkers in autism</article-title>
<source/>Front. Psychiatry
          <volume>5</volume>
<year>2014</year>
<fpage>100</fpage>
<pub-id pub-id-type="pmid">25161627</pub-id>
</element-citation>
</ref>
<ref id="bb0125">
<element-citation id="rf0125" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Grèzes</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pichon</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>de Gelder</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Perceiving fear in dynamic body expressions</article-title>
<source/>Neuroimage
          <volume>35</volume>
<year>2007</year>
<fpage>959</fpage>
<lpage>967</lpage>
<pub-id pub-id-type="pmid">17270466</pub-id>
</element-citation>
</ref>
<ref id="bb0130">
<element-citation id="rf0130" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Guyon</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Weston</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Barnhill</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Vapnik</surname>
<given-names>V.</given-names>
</name>
</person-group>
<article-title>Gene selection for cancer classification using support vector machines</article-title>
<source/>Mach. Learn.
          <volume>46</volume>
<year>2002</year>
<fpage>389</fpage>
<lpage>422</lpage>
</element-citation>
</ref>
<ref id="bb0135">
<element-citation id="rf0135" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Haxby</surname>
<given-names>J.V.</given-names>
</name>
<name>
<surname>Gobbini</surname>
<given-names>M.I.</given-names>
</name>
<name>
<surname>Furey</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Ishai</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Schouten</surname>
<given-names>J.L.</given-names>
</name>
<name>
<surname>Pietrini</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>
<source/>Science
          <volume>293</volume>
<year>2001</year>
<fpage>2425</fpage>
<lpage>2430</lpage>
<comment>(80-.)</comment>
<pub-id pub-id-type="pmid">11577229</pub-id>
</element-citation>
</ref>
<ref id="bb0140">
<element-citation id="rf0140" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Haxby</surname>
<given-names>J.V.</given-names>
</name>
<name>
<surname>Connolly</surname>
<given-names>A.C.</given-names>
</name>
<name>
<surname>Guntupalli</surname>
<given-names>J.S.</given-names>
</name>
</person-group>
<article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title>
<source/>Annu. Rev. Neurosci.
          <volume>37</volume>
<year>2014</year>
<fpage>435</fpage>
<lpage>456</lpage>
<pub-id pub-id-type="pmid">25002277</pub-id>
</element-citation>
</ref>
<ref id="bb0145">
<element-citation id="rf0145" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Iidaka</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Resting state functional magnetic resonance imaging and neural network classified autism and control</article-title>
<source/>Cortex
          <volume>63C</volume>
<year>2014</year>
<fpage>55</fpage>
<lpage>67</lpage>
<pub-id pub-id-type="pmid">25243989</pub-id>
</element-citation>
</ref>
<ref id="bb0150">
<element-citation id="rf0150" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kanwisher</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>McDermott</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Chun</surname>
<given-names>M.M.</given-names>
</name>
</person-group>
<article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>
<source/>J. Neurosci.
          <volume>17</volume>
<year>1997</year>
<fpage>4302</fpage>
<lpage>4311</lpage>
<pub-id pub-id-type="pmid">9151747</pub-id>
</element-citation>
</ref>
<ref id="bb0155">
<element-citation id="rf0155" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kennedy</surname>
<given-names>D.P.</given-names>
</name>
<name>
<surname>Redcay</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Courchesne</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Failing to deactivate: resting functional abnormalities in autism</article-title>
<source/>Proc. Natl. Acad. Sci. U. S. A.
          <volume>103</volume>
<year>2006</year>
<fpage>8275</fpage>
<lpage>8280</lpage>
<pub-id pub-id-type="pmid">16702548</pub-id>
</element-citation>
</ref>
<ref id="bb0160">
<element-citation id="rf0160" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kerns</surname>
<given-names>C.M.</given-names>
</name>
<name>
<surname>Kendall</surname>
<given-names>P.C.</given-names>
</name>
<name>
<surname>Zickgraf</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Franklin</surname>
<given-names>M.E.</given-names>
</name>
<name>
<surname>Miller</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Herrington</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Not to be overshadowed or overlooked: functional impairments associated with comorbid anxiety disorders in youth with ASD</article-title>
<source/>Behav. Ther.
          <volume>46</volume>
<year>2015</year>
<fpage>29</fpage>
<lpage>39</lpage>
<pub-id pub-id-type="pmid">25526833</pub-id>
</element-citation>
</ref>
<ref id="bb0165">
<element-citation id="rf0165" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kosmadakis</surname>
<given-names>C.S.</given-names>
</name>
<name>
<surname>Bungener</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Pierson</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Jouvent</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Widlöcher</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Translation and validation of the Revised Social Anhedonia Scale. Study of the internal and concurrent validity in 126 normal subjects</article-title>
<source/>Encéphale
          <volume>21</volume>
<year>1995</year>
<fpage>437</fpage>
<lpage>443</lpage>
<pub-id pub-id-type="pmid">8674468</pub-id>
</element-citation>
</ref>
<ref id="bb0170">
<element-citation id="rf0170" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Koziol</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>Comments on the rank product method for analyzing replicated experiments</article-title>
<source/>FEBS Lett.
          <volume>584</volume>
<year>2010</year>
<fpage>941</fpage>
<lpage>944</lpage>
<pub-id pub-id-type="pmid">20093118</pub-id>
</element-citation>
</ref>
<ref id="bb0175">
<element-citation id="rf0175" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kriegeskorte</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Goebel</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Bandettini</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Information-based functional brain mapping</article-title>
<source/>Proc. Natl. Acad. Sci. U. S. A.
          <volume>103</volume>
<year>2006</year>
<fpage>3863</fpage>
<lpage>3868</lpage>
<pub-id pub-id-type="pmid">16537458</pub-id>
</element-citation>
</ref>
<ref id="bb9000">
<element-citation id="rf9000" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laconte</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Strother</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Cherkassky</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Anderson</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Hu</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Support vector machines for temporal classification of block design fMRI data</article-title>
<source/>NeuroImage
          <volume>26</volume>
<issue>2</issue>
<year>2005</year>
<fpage>317</fpage>
<lpage>329</lpage>
<pub-id pub-id-type="pmid">15907293</pub-id>
</element-citation>
</ref>
<ref id="bb0180">
<element-citation id="rf0180" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>London</surname>
<given-names>E.B.</given-names>
</name>
</person-group>
<article-title>Categorical diagnosis: a fatal flaw for autism research?</article-title>
<source/>Trends Neurosci.
          <volume>37</volume>
<year>2014</year>
<fpage>683</fpage>
<lpage>686</lpage>
<pub-id pub-id-type="pmid">25465942</pub-id>
</element-citation>
</ref>
<ref id="bb0185">
<element-citation id="rf0185" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lord</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Risi</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lambrecht</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Cook</surname>
<given-names>E.H.</given-names>
</name>
<name>
<surname>Leventhal</surname>
<given-names>B.L.</given-names>
</name>
<name>
<surname>DiLavore</surname>
<given-names>P.C.</given-names>
</name>
<name>
<surname>Pickles</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Rutter</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>The autism diagnostic observation schedule-generic: a standard measure of social and communication deficits associated with the spectrum of autism</article-title>
<source/>J. Autism Dev. Disord.
          <volume>30</volume>
<year>2000</year>
<fpage>205</fpage>
<lpage>223</lpage>
<pub-id pub-id-type="pmid">11055457</pub-id>
</element-citation>
</ref>
<ref id="bb0190">
<element-citation id="rf0190" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mourão-Miranda</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Bokde</surname>
<given-names>A.L.W.</given-names>
</name>
<name>
<surname>Born</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Hampel</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Stetter</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Classifying brain states and determining the discriminating activation patterns: Support Vector Machine on functional MRI data</article-title>
<source/>Neuroimage
          <volume>28</volume>
<year>2005</year>
<fpage>980</fpage>
<lpage>995</lpage>
<pub-id pub-id-type="pmid">16275139</pub-id>
</element-citation>
</ref>
<ref id="bb0195">
<element-citation id="rf0195" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Murdaugh</surname>
<given-names>D.L.</given-names>
</name>
<name>
<surname>Shinkareva</surname>
<given-names>S.V.</given-names>
</name>
<name>
<surname>Deshpande</surname>
<given-names>H.R.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pennick</surname>
<given-names>M.R.</given-names>
</name>
<name>
<surname>Kana</surname>
<given-names>R.K.</given-names>
</name>
</person-group>
<article-title>Differential deactivation during mentalizing and classification of autism based on default mode network connectivity</article-title>
<source/>PLoS One
          <volume>7</volume>
<year>2012</year>
<fpage>e50064</fpage>
<pub-id pub-id-type="pmid">23185536</pub-id>
</element-citation>
</ref>
<ref id="bb0200">
<element-citation id="rf0200" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nettle</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Bateson</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>The evolutionary origins of mood and its disorders</article-title>
<source/>Curr. Biol.
          <volume>22</volume>
<year>2012</year>
<fpage>R712</fpage>
<lpage>R721</lpage>
<pub-id pub-id-type="pmid">22975002</pub-id>
</element-citation>
</ref>
<ref id="bb0205">
<element-citation id="rf0205" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nichols</surname>
<given-names>T.E.</given-names>
</name>
<name>
<surname>Holmes</surname>
<given-names>A.P.</given-names>
</name>
</person-group>
<article-title>Nonparametric Permutation Tests For Functional Neuroimaging: A Primer with Examples</article-title>
<volume>25</volume>
<year>2001</year>
<fpage>1</fpage>
<lpage>25</lpage>
</element-citation>
</ref>
<ref id="bb0210">
<element-citation id="rf0210" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Noirhomme</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Lesenfants</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Gomez</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Soddu</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Schrouff</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Garraux</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Luxen</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Phillips</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Laureys</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Biased binomial assessment of cross-validated estimation of classification accuracies illustrated in diagnosis predictions</article-title>
<source/>NeuroImage Clin.
          <volume>4</volume>
<year>2014</year>
<fpage>687</fpage>
<lpage>694</lpage>
<pub-id pub-id-type="pmid">24936420</pub-id>
</element-citation>
</ref>
<ref id="bb0215">
<element-citation id="rf0215" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pelphrey</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Adolphs</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Morris</surname>
<given-names>J.P.</given-names>
</name>
</person-group>
<article-title>Neuroanatomical substrates of social cognition dysfunction in autism</article-title>
<source/>Ment. Retard. Dev. Disabil. Res. Rev.
          <volume>10</volume>
<year>2004</year>
<fpage>259</fpage>
<lpage>271</lpage>
<pub-id pub-id-type="pmid">15666336</pub-id>
</element-citation>
</ref>
<ref id="bb0220">
<element-citation id="rf0220" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pereira</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Mitchell</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Botvinick</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title>
<source/>Neuroimage
          <volume>45</volume>
<year>2009</year>
<fpage>S199</fpage>
<lpage>S209</lpage>
<pub-id pub-id-type="pmid">19070668</pub-id>
</element-citation>
</ref>
<ref id="bb0225">
<element-citation id="rf0225" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pichon</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>de Gelder</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Grèzes</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Emotional modulation of visual and motor areas by dynamic body expressions of anger</article-title>
<source/>Soc. Neurosci.
          <volume>3</volume>
<year>2008</year>
<fpage>199</fpage>
<lpage>212</lpage>
<pub-id pub-id-type="pmid">18979376</pub-id>
</element-citation>
</ref>
<ref id="bb0230">
<element-citation id="rf0230" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pichon</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>de Gelder</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Grèzes</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Two different faces of threat. Comparing the neural systems for recognizing fear and anger in dynamic body expressions</article-title>
<source/>Neuroimage
          <volume>47</volume>
<year>2009</year>
<fpage>1873</fpage>
<lpage>1883</lpage>
<pub-id pub-id-type="pmid">19371787</pub-id>
</element-citation>
</ref>
<ref id="bb0235">
<element-citation id="rf0235" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pichon</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>de Gelder</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Grèzes</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Threat prompts defensive brain responses independently of attentional control</article-title>
<source/>Cereb. Cortex
          <volume>22</volume>
<year>2012</year>
<fpage>274</fpage>
<lpage>285</lpage>
<pub-id pub-id-type="pmid">21666127</pub-id>
</element-citation>
</ref>
<ref id="bb0240">
<element-citation id="rf0240" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pichon</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Miendlarzewska</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Eryilmaz</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Vuilleumier</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Cumulative activation during positive and negative events and state anxiety predicts subsequent inertia of amygdala reactivity</article-title>
<source/>Soc. Cogn. Affect. Neurosci.
          <volume>10</volume>
<year>2015</year>
<fpage>180</fpage>
<lpage>190</lpage>
<pub-id pub-id-type="pmid">24603023</pub-id>
</element-citation>
</ref>
<ref id="bb0245">
<element-citation id="rf0245" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pierce</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Haist</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Sedaghat</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Courchesne</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>The brain response to personally familiar faces in autism: findings of fusiform activity and beyond</article-title>
<source/>Brain
          <volume>127</volume>
<year>2004</year>
<fpage>2703</fpage>
<lpage>2716</lpage>
<pub-id pub-id-type="pmid">15319275</pub-id>
</element-citation>
</ref>
<ref id="bb0250">
<element-citation id="rf0250" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pitcher</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Facial expression recognition takes longer in the posterior superior temporal sulcus than in the occipital face area</article-title>
<source/>J. Neurosci.
          <volume>34</volume>
<year>2014</year>
<fpage>9173</fpage>
<lpage>9177</lpage>
<pub-id pub-id-type="pmid">24990937</pub-id>
</element-citation>
</ref>
<ref id="bb0255">
<element-citation id="rf0255" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Power</surname>
<given-names>J.D.</given-names>
</name>
<name>
<surname>Barnes</surname>
<given-names>K.A.</given-names>
</name>
<name>
<surname>Snyder</surname>
<given-names>A.Z.</given-names>
</name>
<name>
<surname>Schlaggar</surname>
<given-names>B.L.</given-names>
</name>
<name>
<surname>Petersen</surname>
<given-names>S.E.</given-names>
</name>
</person-group>
<article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title>
<source/>Neuroimage
          <volume>59</volume>
<year>2012</year>
<fpage>2142</fpage>
<lpage>2154</lpage>
<pub-id pub-id-type="pmid">22019881</pub-id>
</element-citation>
</ref>
<ref id="bb0260">
<element-citation id="rf0260" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Power</surname>
<given-names>J.D.</given-names>
</name>
<name>
<surname>Mitra</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Laumann</surname>
<given-names>T.O.</given-names>
</name>
<name>
<surname>Snyder</surname>
<given-names>A.Z.</given-names>
</name>
<name>
<surname>Schlaggar</surname>
<given-names>B.L.</given-names>
</name>
<name>
<surname>Petersen</surname>
<given-names>S.E.</given-names>
</name>
</person-group>
<article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title>
<source/>Neuroimage
          <volume>84</volume>
<year>2014</year>
<fpage>320</fpage>
<lpage>341</lpage>
<pub-id pub-id-type="pmid">23994314</pub-id>
</element-citation>
</ref>
<ref id="bb0265">
<element-citation id="rf0265" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Puce</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Allison</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Asgari</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Gore</surname>
<given-names>J.C.</given-names>
</name>
<name>
<surname>McCarthy</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Differential sensitivity of human visual cortex to faces, letterstrings, and textures: a functional magnetic resonance imaging study</article-title>
<source/>J. Neurosci.
          <volume>16</volume>
<year>1996</year>
<fpage>5205</fpage>
<lpage>5215</lpage>
<pub-id pub-id-type="pmid">8756449</pub-id>
</element-citation>
</ref>
<ref id="bb0270">
<element-citation id="rf0270" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rorden</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Karnath</surname>
<given-names>H.O.</given-names>
</name>
<name>
<surname>Bonilha</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Improving lesion-symptom mapping</article-title>
<source/>J. Cogn. Neurosci.
          <volume>19</volume>
<year>2007</year>
<fpage>1081</fpage>
<lpage>1088</lpage>
<pub-id pub-id-type="pmid">17583985</pub-id>
</element-citation>
</ref>
<ref id="bb0275">
<element-citation id="rf0275" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Samson</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Apperly</surname>
<given-names>I.A.</given-names>
</name>
<name>
<surname>Chiavarino</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Humphreys</surname>
<given-names>G.W.</given-names>
</name>
</person-group>
<article-title>Left temporoparietal junction is necessary for representing someone else's belief</article-title>
<source/>Nat. Neurosci.
          <volume>7</volume>
<year>2004</year>
<fpage>499</fpage>
<lpage>500</lpage>
<pub-id pub-id-type="pmid">15077111</pub-id>
</element-citation>
</ref>
<ref id="bb0280">
<element-citation id="rf0280" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Satterthwaite</surname>
<given-names>T.D.</given-names>
</name>
<name>
<surname>Elliott</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Gerraty</surname>
<given-names>R.T.</given-names>
</name>
<name>
<surname>Ruparel</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Loughead</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Calkins</surname>
<given-names>M.E.</given-names>
</name>
<name>
<surname>Eickhoff</surname>
<given-names>S.B.</given-names>
</name>
<name>
<surname>Hakonarson</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Gur</surname>
<given-names>R.C.</given-names>
</name>
<name>
<surname>Gur</surname>
<given-names>R.E.</given-names>
</name>
<name>
<surname>Wolf</surname>
<given-names>D.H.</given-names>
</name>
</person-group>
<article-title>An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data</article-title>
<source/>Neuroimage
          <volume>64</volume>
<year>2013</year>
<fpage>240</fpage>
<lpage>256</lpage>
<pub-id pub-id-type="pmid">22926292</pub-id>
</element-citation>
</ref>
<ref id="bb0285">
<element-citation id="rf0285" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schultz</surname>
<given-names>R.T.</given-names>
</name>
</person-group>
<article-title>Developmental deficits in social perception in autism: the role of the amygdala and fusiform face area</article-title>
<source/>Int. J. Dev. Neurosci.
          <volume>23</volume>
<year>2005</year>
<fpage>125</fpage>
<lpage>141</lpage>
<pub-id pub-id-type="pmid">15749240</pub-id>
</element-citation>
</ref>
<ref id="bb0290">
<element-citation id="rf0290" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Spielberger</surname>
<given-names>C.D.</given-names>
</name>
<name>
<surname>Gorsuch</surname>
<given-names>R.L.</given-names>
</name>
<name>
<surname>Lushene</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Vagg</surname>
<given-names>P.R.</given-names>
</name>
<name>
<surname>Jacobs</surname>
<given-names>G.A.</given-names>
</name>
</person-group>
<chapter-title>Manual for the State-Trait Anxiety Inventory</chapter-title>
<year>1983</year>
<publisher-name>Consulting Psychologists Press, Inc.</publisher-name>
<publisher-loc>Palo Alto, CA</publisher-loc>
</element-citation>
</ref>
<ref id="bb0295">
<element-citation id="rf0295" publication-type="other">
<person-group person-group-type="author">
<name>
<surname>Tager-Flusberg</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Why we must be patient when searching for an autism biomarker [WWW Document]</article-title>
<comment>URL</comment>
<ext-link ext-link-type="uri" id="ir0015" xlink:href="http://sfari.org/news-and-opinion/columnists/helen-tager-flusberg/2014/why-we-must-be-patient-when-searching-for-an-autism-biomarker">http://sfari.org/news-and-opinion/columnists/helen-tager-flusberg/2014/why-we-must-be-patient-when-searching-for-an-autism-biomarker</ext-link>
<year>2014</year>
<comment>(accessed 5.6.15)</comment>
</element-citation>
</ref>
<ref id="bb0300">
<element-citation id="rf0300" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fushing</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Extracting multiscale pattern information of fMRI based functional brain connectivity with application on classification of autism spectrum disorders</article-title>
<source/>PLoS One
          <volume>7</volume>
<year>2012</year>
<fpage>e45502</fpage>
<pub-id pub-id-type="pmid">23056205</pub-id>
</element-citation>
</ref>
<ref id="bb0305">
<element-citation id="rf0305" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>World Health Organization</surname>
</name>
</person-group>
<article-title>The ICD-10 classification of mental and behavioural disorders</article-title>
<source/>Int. Classif.
          <volume>10</volume>
<year>1992</year>
<fpage>1</fpage>
<lpage>267</lpage>
</element-citation>
</ref>
<ref id="bb0310">
<element-citation id="rf0310" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Tian</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Yuan</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Zhuo</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Qin</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>von Deneen</surname>
<given-names>K.M.</given-names>
</name>
<name>
<surname>Klahr</surname>
<given-names>N.J.</given-names>
</name>
<name>
<surname>Gold</surname>
<given-names>M.S.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Distinct resting-state brain activities in heroin-dependent individuals</article-title>
<source/>Brain Res.
          <volume>1402</volume>
<year>2011</year>
<fpage>46</fpage>
<lpage>53</lpage>
<pub-id pub-id-type="pmid">21669407</pub-id>
</element-citation>
</ref>
<ref id="bb0315">
<element-citation id="rf0315" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhou</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Yu</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Duong</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Multiparametric MRI characterization and prediction in autism spectrum disorder using graph theory and machine learning</article-title>
<source/>PLoS One
          <volume>9</volume>
<year>2014</year>
<fpage>e90405</fpage>
<pub-id pub-id-type="pmid">24922325</pub-id>
</element-citation>
</ref>
<ref id="bb0320">
<element-citation id="rf0320" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zilbovicius</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Meresse</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Chabane</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Brunelle</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Samson</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Boddaert</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Autism, the superior temporal sulcus and social perception</article-title>
<source/>Trends Neurosci.
          <volume>29</volume>
<year>2006</year>
<fpage>359</fpage>
<lpage>366</lpage>
<pub-id pub-id-type="pmid">16806505</pub-id>
</element-citation>
</ref>
</ref-list>
<sec id="s0125" sec-type="supplementary-material">
<label>Appendix A</label>
<title>Supplementary data</title>
<p>
<supplementary-material content-type="local-data" id="ec0005">
<caption>
<title>Supplementary material</title>
<p>Correlations of classifier' outputs with movement parameters show that SVM RFE and Friston24 movement correction reduced movement artefacts.</p>
</caption>
<media xlink:href="mmc1.docx"></media>
</supplementary-material>
</p>
</sec>
<ack id="ac0005">
<title>Acknowledgments</title>
<p>The authors would like to acknowledge Prof. Dimitri Van De Ville and Dr. Djalel Eddine Meskaldji for their helpful comments and insightful suggestions on some of the methodological aspects of this work. This work was supported by grants from the <funding-source id="gts0005">French National Research Agency</funding-source> ANR-11-EMCO-00902, ANR-11-0001-02 PSL*, ANR-10-LABX-0087, by the Fondation ROGER DE SPOELBERCH, Fondation pour la Recherche Psychiatrie &amp; Santé Mentale and by INSERM. Guillaume Chanel is supported by the National Center of Competence in Research (NCCR) Affective sciences financed by the <funding-source id="gts0015">Swiss National Science Foundation</funding-source> (no. 51NF40_104897) and hosted by the University of Geneva. Swann Pichon is supported by an Ambizione starting grant of the Swiss National Science Foundation (no. PZ00P1_148035).</p>
</ack>
<fn-group>
<fn fn-type="supplementary-material" id="s0135">
<label>Appendix A</label>
<p>Supplementary data to this article can be found online at <ext-link ext-link-type="doi" id="ir0010" xlink:href="10.1016/j.nicl.2015.11.010">http://dx.doi.org/10.1016/j.nicl.2015.11.010</ext-link>.</p>
</fn>
</fn-group>
</back>
<floats-group>
<fig id="f0005">
<label>Fig. 1</label>
<caption>
<p>The two paradigms and examples of stimuli. A) In Experiment 1 (static faces), participants observed angry or neutral facial expressions with direct or averted gaze. Participants were instructed to observe each picture attentively and to press a button whenever they perceived an upside-down oddball picture (<xref ref-type="bibr" rid="bb0060">Conty et al., 2012</xref>). B) In Experiment 2 (dynamic bodies), participants observed short video-clips showing angry or neutral body expressions with a color-dot appearing briefly for 40 ms onto the actor's upper body. Depending on the instruction, subjects categorized the emotion or the color of the dot (<xref ref-type="bibr" rid="bb0235">Pichon et al., 2012</xref>).</p>
</caption>
<graphic xlink:href="gr1"></graphic>
</fig>
<fig id="f0010">
<label>Fig. 2</label>
<caption>
<p>Classification and feature selection frameworks. Left) For each participant and each condition a classifier was trained using the data of the other participants (on the same condition). Next, the outputs of the classifiers were averaged across conditions and a final decision was taken for each participant based on the sign of the average classification score. Right) Cross-validated feature selection was applied to select the most discriminative features and to find discriminative patterns of brain activity.</p>
</caption>
<graphic xlink:href="gr2"></graphic>
</fig>
<fig id="f0015">
<label>Fig. 3</label>
<caption>
<p>Visualization of the most discriminative voxels. These voxels were found in regions related to social cognition and consistently showed reduced contribution in ASD participants compared to controls. Overall, the fusion of both experiments increased the size of the largest significant clusters while smaller clusters disappeared. Results were corrected for multiple comparisons (FWE p &lt; 0.05).</p>
</caption>
<graphic xlink:href="gr3"></graphic>
</fig>
<fig id="f0020">
<label>Fig. 4</label>
<caption>
<p>Correlations for Experiment 2 (bodies) for which classification scores best predicted social anhedonia in the ASD group. Anxiety scores were unrelated to classification scores (we used classification scores from the SVM RFE feature selection and the Friston24 movement correction methods).</p>
</caption>
<graphic xlink:href="gr4"></graphic>
</fig>
<table-wrap id="t0005" position="float">
<label>Table 1</label>
<caption>
<p>Participant variables employed for group-matching and ADOS data.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">
<hr/>
</th>
<th align="left" colspan="3">ASD (n = 15)<hr/></th>
<th align="left" colspan="3">Controls (n = 14)<hr/></th>
<th align="left" colspan="2">Group difference<hr/></th>
</tr>
<tr>
<th align="left"></th>
<th align="left">Mean</th>
<th align="left">SEM</th>
<th align="left">Range</th>
<th align="left">Mean</th>
<th align="left">SEM</th>
<th align="left">Range</th>
<th align="left">t-test</th>
<th align="left">p-Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Age</td>
<td align="left">28.6</td>
<td align="char">1.87</td>
<td align="char">19–43</td>
<td align="left">31.6</td>
<td align="char">2.61</td>
<td align="char">19–53</td>
<td align="char">.94</td>
<td align="char">0.35</td>
</tr>
<tr>
<td align="left">IQ</td>
<td align="left">108.06</td>
<td align="char">4.5</td>
<td align="char">77–150</td>
<td align="left">116.78</td>
<td align="char">4.6</td>
<td align="char">84–141</td>
<td align="char">1.35</td>
<td align="char">0.18</td>
</tr>
<tr>
<td align="left">Handedness<xref ref-type="table-fn" rid="tf0005">⁎</xref></td>
<td align="left">3L/12R</td>
<td></td>
<td></td>
<td align="left">4L/10R</td>
<td></td>
<td></td>
<td align="char">.29</td>
<td align="char">0.59</td>
</tr>
<tr>
<td align="left">Gender<xref ref-type="table-fn" rid="tf0005">⁎</xref></td>
<td align="left">13M/2F</td>
<td></td>
<td></td>
<td align="left">12M/2F</td>
<td></td>
<td></td>
<td align="char">.006</td>
<td align="char">0.94</td>
</tr>
<tr>
<td align="left">ADOS total</td>
<td align="left">10.3</td>
<td align="char">2.46</td>
<td align="char">5–15</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">ADOS communication</td>
<td align="left">3.23</td>
<td align="char">1.73</td>
<td align="char">0–7</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">ADOS social interaction</td>
<td align="left">7.07</td>
<td align="char">0.95</td>
<td align="char">5–8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="tf0005">
<label>⁎</label>
<p>Pearson Khi-2; SEM: standard error of the mean.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="t0010" position="float">
<label>Table 2</label>
<caption>
<p>Classification performance for the fusion of conditions belonging to each or both experiments.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">Fusion/experiment<hr/></th>
<th align="left">Motion correction<hr/></th>
<th align="left" colspan="2">Accuracy (%)<hr/></th>
<th align="left">Sensitivity (%)<hr/></th>
<th align="left">Specificity (%)<hr/></th>
</tr>
<tr>
<th align="left"></th>
<th align="left"></th>
<th align="left">SVM<sup>noFS</sup></th>
<th align="left">SVM RFE<sup>FS</sup></th>
<th align="left">SVM RFE<sup>FS</sup></th>
<th align="left">SVM RFE<sup>FS</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Both experiments</td>
<td align="left">Rawrp6</td>
<td align="char">72.4<sup>⁎</sup></td>
<td align="char">89.7<sup>⁎⁎⁎</sup></td>
<td align="char">100</td>
<td align="char">80</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">82.8<sup>⁎⁎⁎</sup></td>
<td align="char">79.3<sup>⁎⁎</sup></td>
<td align="char">78.6</td>
<td align="char">80</td>
</tr>
<tr>
<td align="left">Exp. 1 (faces)</td>
<td align="left">Rawrp6</td>
<td align="char">62.1</td>
<td align="char">69.0<sup>⁎</sup></td>
<td align="char">71.4</td>
<td align="char">66.7</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">65.5†</td>
<td align="char">69.0<sup>⁎</sup></td>
<td align="char">57.2</td>
<td align="char">80</td>
</tr>
<tr>
<td align="left">Exp. 2 (bodies)</td>
<td align="left">Rawrp6</td>
<td align="char">76.9<sup>⁎⁎</sup></td>
<td align="char">92.3<sup>⁎⁎⁎</sup></td>
<td align="char">92.3</td>
<td align="char">92.3</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">80.8<sup>⁎⁎</sup></td>
<td align="char">80.8<sup>⁎⁎</sup></td>
<td align="char">92.3</td>
<td align="char">69.2</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>Significance values assessing that the classification achieved best than chance are indicated only for the accuracy columns (FS: features selection, †: p  &lt;  0.1, *: p  &lt;  0.05, **:  p &lt;  0.01, ***: p  &lt;  0.001). Exp. 1 stands for the experiment where static faces were used. Exp. 2 stands for the experiment where dynamic bodies were used.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="t0015" position="float">
<label>Table 3</label>
<caption>
<p>Classification accuracy (%) after the fusion of either the Anger conditions or the Neutral conditions from both experiments.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">Motion correction</th>
<th align="left">Classification method</th>
<th align="left">Accuracy (%) for anger conditions</th>
<th align="left">Accuracy (%) for neutral conditions</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Rawrp6</td>
<td align="left">SVM<sup>noFS</sup></td>
<td align="left">69</td>
<td align="left">75.9</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="left">SVM<sup>noFS</sup></td>
<td align="left">82.8</td>
<td align="left">72.4</td>
</tr>
<tr>
<td align="left">Rawrp6</td>
<td align="left">SVM RFE<sup>FS</sup></td>
<td align="left">89.7</td>
<td align="left">89.7</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="left">SVM RFE<sup>FS</sup></td>
<td align="left">79.3</td>
<td align="left">75.9</td>
</tr>
<tr>
<td align="left" colspan="2">Mean accuracy (± STD)</td>
<td align="left">80.2 (± 8.6)</td>
<td align="left">78.5 (± 7.7)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>The mean accuracy and standard deviation were computed across all movement correction and classification methods.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="t0020" position="float">
<label>Table 4</label>
<caption>
<p>Discriminative voxels across experiments using SVM RFE.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">
<hr/>
</th>
<th align="left">
<hr/>
</th>
<th align="left" colspan="3">Both experiments<hr/></th>
<th align="left" colspan="3">Experiment 1 (faces)<hr/></th>
<th align="left" colspan="3">Experiment 2 (bodies)<hr/></th>
</tr>
<tr>
<th align="left">R/L<hr/></th>
<th align="left">Anatomical region<hr/></th>
<th align="left" colspan="3">MNI coordinates<hr/></th>
<th align="left" colspan="3">MNI coordinates<hr/></th>
<th align="left" colspan="3">MNI coordinates<hr/></th>
</tr>
<tr>
<th align="left"></th>
<th align="left"></th>
<th align="left">x</th>
<th align="left">y</th>
<th align="left">z</th>
<th align="left">x</th>
<th align="left">y</th>
<th align="left">z</th>
<th align="left">x</th>
<th align="left">y</th>
<th align="left">z</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="11">
<italic>TD &gt; ASD</italic>
</td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Premotor cortex</td>
<td align="left">52</td>
<td align="left">10</td>
<td align="left">46</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Temporo-parietal junction (TPJ)</td>
<td align="left">54</td>
<td align="left">− 36</td>
<td align="left">24</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Supramarginal gyrus</td>
<td align="left">68</td>
<td align="left">− 42</td>
<td align="left">26</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R &amp; L</td>
<td align="left">Fusiform face area (FFA)</td>
<td align="left">± 44</td>
<td align="left">− 54</td>
<td align="left">− 16</td>
<td align="left">± 39</td>
<td align="left">− 52</td>
<td align="left">− 23</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Superior temporal sulcus (STS)</td>
<td align="left">− 54</td>
<td align="left">− 56</td>
<td align="left">10</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Lingual gyrus</td>
<td align="left">− 18</td>
<td align="left">− 58</td>
<td align="left">0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Superior parietal lobule (SPL)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="left">− 16</td>
<td align="left">− 56</td>
<td align="left">70</td>
</tr>
<tr>
<td align="left">R &amp; L</td>
<td align="left">Calcarine sulcus</td>
<td align="left">± 12</td>
<td align="left">− 68</td>
<td align="left">16</td>
<td></td>
<td></td>
<td></td>
<td align="left">± 14</td>
<td align="left">− 70</td>
<td align="left">16</td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Occipital face area (OFA)</td>
<td align="left">44</td>
<td align="left">− 70</td>
<td align="left">− 4</td>
<td></td>
<td></td>
<td></td>
<td align="left">44</td>
<td align="left">− 70</td>
<td align="left">− 2</td>
</tr>
<tr>
<td align="left">R &amp; L</td>
<td align="left">Extrastriate body area (EBA)</td>
<td align="left">− 52</td>
<td align="left">− 72</td>
<td align="left">6</td>
<td></td>
<td></td>
<td></td>
<td align="left">− 54</td>
<td align="left">− 68</td>
<td align="left">14</td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Precuneus</td>
<td align="left">14</td>
<td align="left">− 72</td>
<td align="left">62</td>
<td></td>
<td></td>
<td></td>
<td align="left">14</td>
<td align="left">− 70</td>
<td align="left">60</td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Superior occipital gyrus</td>
<td align="left">− 20</td>
<td align="left">− 76</td>
<td align="left">38</td>
<td></td>
<td></td>
<td></td>
<td align="left">− 18</td>
<td align="left">− 78</td>
<td align="left">− 40</td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Lunal gyrus</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td align="left">20</td>
<td align="left">− 80</td>
<td align="left">− 6</td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Occipito-temporal face area (OFA)</td>
<td align="left">− 42</td>
<td align="left">− 80</td>
<td align="left">− 6</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Middle/superior occipital gyrus</td>
<td align="left">− 20</td>
<td align="left">− 82</td>
<td align="left">16</td>
<td></td>
<td></td>
<td></td>
<td align="left">− 18</td>
<td align="left">84</td>
<td align="left">18</td>
</tr>
<tr>
<td align="left">R &amp; L</td>
<td align="left">Occipital pole</td>
<td align="left">± 18</td>
<td align="left">− 92</td>
<td align="left">− 8</td>
<td align="left">± 22</td>
<td align="left">− 95</td>
<td align="left">− 6</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R &amp; L</td>
<td align="left">Occipital pole</td>
<td align="left">± 32</td>
<td align="left">− 96</td>
<td align="left">− 10</td>
<td align="left">30</td>
<td align="left">− 95</td>
<td align="left">− 7</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left" colspan="11">  </td>
</tr>
<tr>
<td align="left" colspan="11">
<italic>ASD &gt; TD</italic>
</td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Angular gyrus/inferior parietal lobule (IPL)</td>
<td align="left">− 36</td>
<td align="left">− 70</td>
<td align="left">40</td>
<td></td>
<td></td>
<td></td>
<td align="left">− 36</td>
<td align="left">− 70</td>
<td align="left">38</td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Angular gyrus/inferior parietal lobule (IPL)</td>
<td align="left">48</td>
<td align="left">− 60</td>
<td align="left">50</td>
<td></td>
<td></td>
<td></td>
<td align="left">48</td>
<td align="left">− 62</td>
<td align="left">52</td>
</tr>
<tr>
<td align="left">L</td>
<td align="left">Posterior cingulate cortex (PCC)</td>
<td align="left">− 14</td>
<td align="left">− 40</td>
<td align="left">38</td>
<td align="left">− 16</td>
<td align="left">− 44</td>
<td align="left">36</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Inferior temporal gyrus</td>
<td align="left">64</td>
<td align="left">− 30</td>
<td align="left">− 18</td>
<td align="left">52</td>
<td align="left">− 20</td>
<td align="left">− 26</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td align="left">R</td>
<td align="left">Middle temporal gyrus</td>
<td></td>
<td></td>
<td></td>
<td align="left">64</td>
<td align="left">− 38</td>
<td align="left">− 10</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="t0025" position="float">
<label>Table 5</label>
<caption>
<p>Participant scores for social anhedonia (SAS) and anxiety.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">
<hr/>
</th>
<th align="left" colspan="3">ASD (n = 15)<hr/></th>
<th align="left" colspan="3">TD (n = 14)<hr/></th>
<th align="left" colspan="2">Group difference<hr/></th>
</tr>
<tr>
<th align="left"></th>
<th align="left">Mean</th>
<th align="left">SEM</th>
<th align="left">Range</th>
<th align="left">Mean</th>
<th align="left">SEM</th>
<th align="left">Range</th>
<th align="left">T (ASD vs TD)</th>
<th align="left">p-Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Social Anhedonia SAS</td>
<td align="char">18.76</td>
<td align="char">2.19</td>
<td align="char">8–31</td>
<td align="char">7.42</td>
<td align="char">0.84</td>
<td align="char">2–13</td>
<td align="char">− 5.39</td>
<td align="char">&lt; 0.001</td>
</tr>
<tr>
<td align="left">Anxiety (trait)</td>
<td align="char">47.5</td>
<td align="char">2.96</td>
<td align="char">29–77</td>
<td align="char">38</td>
<td align="char">2.98</td>
<td align="char">23–63</td>
<td align="char">− 2.2</td>
<td align="char">&lt; 0.05</td>
</tr>
<tr>
<td align="left">Anxiety (state)</td>
<td align="char">36.3</td>
<td align="char">2.69</td>
<td align="char">20–51</td>
<td align="char">32</td>
<td align="char">2.48</td>
<td align="char">20–52</td>
<td align="char">− 1.14</td>
<td align="char">0.26</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="t0030" position="float">
<label>Table 6</label>
<caption>
<p>Pearson r values for partial correlations (both) and correlation in each group (ASD and TD) between the averaged SVM outputs (with SVM RFE feature selection) and scores from scales.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left">Rawrp<hr/></th>
<th align="left">
<hr/>
</th>
<th align="left" colspan="3">Social anhedonia (SAS)<hr/></th>
<th align="left" colspan="3">Autism quotient (AQ)<hr/></th>
<th align="left" colspan="3">Anxiety (trait)<hr/></th>
</tr>
<tr>
<th align="left"></th>
<th align="left"></th>
<th align="left">Both</th>
<th align="left">ASD</th>
<th align="left">TD</th>
<th align="left">Both</th>
<th align="left">ASD</th>
<th align="left">TD</th>
<th align="left">Both</th>
<th align="left">ASD</th>
<th align="left">TD</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Both exp</td>
<td align="left">Rawrp6</td>
<td align="char">0.3</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">.36†</td>
<td align="left">0.46†</td>
<td align="left">0.14</td>
<td align="char">0.15</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">.32†</td>
<td align="left">0.29</td>
<td align="left">.49†</td>
<td align="char">.33†</td>
<td align="left">0.45</td>
<td align="left">0.14</td>
<td align="char">0.11</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">Exp. 1 (gaze)</td>
<td align="left">Rawrp6</td>
<td align="char">0.15</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">.38†</td>
<td align="left">0.35</td>
<td align="left">.46†</td>
<td align="char">0.2</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">0.01</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">0.23</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">0.14</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">Exp. 2 (bodies)</td>
<td align="left">Rawrp6</td>
<td align="char">.50<sup>⁎</sup></td>
<td align="left">.65<sup>⁎</sup></td>
<td align="left">0.05</td>
<td align="char">0.28</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">0.15</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">Friston24</td>
<td align="char">.56<sup>⁎⁎</sup></td>
<td align="left">.76<sup>⁎⁎</sup></td>
<td align="left">0.14</td>
<td align="char">0.3</td>
<td align="left">–</td>
<td align="left">–</td>
<td align="char">0.11</td>
<td align="left">–</td>
<td align="left">–</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn>
<p>The “Both” column indicates that a partial correlation was employed to remove the effect of group. The columns ASD and TD refer to the correlations performed in either group (†: p  &lt;  = 0.1, *: p  &lt;  0.05, **: p  &lt;  0.01, Two-tailed positive Pearson correlation). Correlations within each group were further computed when the partial correlation approached significance (p &lt; 0.1).</p>
</fn>
</table-wrap-foot>
</table-wrap>
</floats-group>
</article>
</pmc-articleset>