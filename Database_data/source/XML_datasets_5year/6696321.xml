<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="review-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
<journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
<journal-id journal-id-type="publisher-id">sensors</journal-id>
<journal-title-group>
<journal-title>Sensors (Basel, Switzerland)</journal-title>
</journal-title-group>
<issn pub-type="epub">1424-8220</issn>
<publisher>
<publisher-name>MDPI</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">31349617</article-id>
<article-id pub-id-type="pmc">6696321</article-id>
<article-id pub-id-type="doi">10.3390/s19153274</article-id>
<article-id pub-id-type="publisher-id">sensors-19-03274</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Review</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Computer Methods for Automatic Locomotion and Gesture Tracking in Mice and Small Animals for Neuroscience Applications: A Survey</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3544-1549</contrib-id>
<name>
<surname>Abbas</surname>
<given-names>Waseem</given-names>
</name>
<xref ref-type="corresp" rid="c1-sensors-19-03274">*</xref>
<xref ref-type="author-notes" rid="fn1-sensors-19-03274">†</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7898-1847</contrib-id>
<name>
<surname>Masip Rodo</surname>
<given-names>David</given-names>
</name>
<xref ref-type="author-notes" rid="fn1-sensors-19-03274">†</xref>
</contrib>
</contrib-group>
<aff id="af1-sensors-19-03274">Multimedia and Telecommunications Department, Universitat Oberta de Catalunya, 08018 Barcelona, Spain</aff>
<author-notes>
<corresp id="c1-sensors-19-03274"><label>*</label>Correspondence: <email>abbas@uoc.edu</email></corresp>
<fn id="fn1-sensors-19-03274">
<label>†</label>
<p>These authors contributed equally to this work.</p>
</fn>
</author-notes>
<pub-date pub-type="epub">
<day>25</day>
<month>7</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2019</year>
</pub-date>
<volume>19</volume>
<issue>15</issue>
<elocation-id>3274</elocation-id>
<history>
<date date-type="received">
<day>31</day>
<month>5</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>7</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-statement>© 2019 by the authors.</copyright-statement>
<copyright-year>2019</copyright-year>
<license license-type="open-access">
<license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
</license>
</permissions>
<abstract>
<p>Neuroscience has traditionally relied on manually observing laboratory animals in controlled environments. Researchers usually record animals behaving freely or in a restrained manner and then annotate the data manually. The manual annotation is not desirable for three reasons; (i) it is time-consuming, (ii) it is prone to human errors, and (iii) no two human annotators will 100% agree on annotation, therefore, it is not reproducible. Consequently, automated annotation for such data has gained traction because it is efficient and replicable. Usually, the automatic annotation of neuroscience data relies on computer vision and machine learning techniques. In this article, we have covered most of the approaches taken by researchers for locomotion and gesture tracking of specific laboratory animals, i.e. rodents. We have divided these papers into categories based upon the hardware they use and the software approach they take. We have also summarized their strengths and weaknesses.</p>
</abstract>
<kwd-group>
<kwd>locomotion tracking</kwd>
<kwd>gesture tracking</kwd>
<kwd>behavioral phenotyping</kwd>
<kwd>automated annotation</kwd>
<kwd>neuroscience</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="sec1-sensors-19-03274" sec-type="intro">
<title>1. Introduction</title>
<p>Neuroscience has found an unusual ally in the form of computer science which has strengthened and widened its scope. The wide availability and easy-to-use nature of video equipment have enabled neuroscientists to record large volumes of behavioral data of animals and analyze them from the neuroscience perspective. Traditionally, neuroscientists would record videos of animals they wanted to study and then annotate the video data manually. Normally, this approach is reasonable if the video being annotated is not large, but the bigger the volume of recorded data gets, the more inconvenient, tiresome, erroneous and slow the manual annotation becomes. Moreover, the annotations made by human annotators are not perfectly reproducible. Two annotations of the same sample done by two different persons will likely differ from each other. Even the annotation done for the same sample at different times by the same person might not be the same. All of these factors have contributed to the demand for a general-purpose automated annotation approach for video data. For behavioral phenotyping and neuroscience applications, researchers are usually interested in gesture and locomotion tracking. Fortunately, computer science has an answer to this problem in the form of machine learning and computer vision-based tracking methods. The research in this area is still not mature, but it is receiving a lot of attention lately. The primary motivation for automated annotation is the reproducibility and ability to annotate large volumes of data in a practical amount of time.</p>
<p>Some researchers approach this problem by treating a video as a sequence of still images and then applying computer vision algorithms to every frame without considering their temporal relationships [<xref ref-type="bibr" rid="B1-sensors-19-03274">1</xref>,<xref ref-type="bibr" rid="B2-sensors-19-03274">2</xref>]. Some of the researchers include temporal information to some extent while others use the assistance of additional hardware [<xref ref-type="bibr" rid="B3-sensors-19-03274">3</xref>,<xref ref-type="bibr" rid="B4-sensors-19-03274">4</xref>,<xref ref-type="bibr" rid="B5-sensors-19-03274">5</xref>]. The general framework is similar. Animals (mice/rats/insects) are kept in a controlled environment, either restrained or free where the lighting and illumination can be manipulated. To acquire the video data, single or multiple video cameras are installed. These might be simple video cameras or depth/IR cameras. There might be some additional accessories installed such as physical markers or body-mounted sensors [<xref ref-type="bibr" rid="B6-sensors-19-03274">6</xref>,<xref ref-type="bibr" rid="B7-sensors-19-03274">7</xref>,<xref ref-type="bibr" rid="B8-sensors-19-03274">8</xref>,<xref ref-type="bibr" rid="B9-sensors-19-03274">9</xref>]. In this article, we review the state of the approaches for rodents’ gesture and locomotion tracking. Nevertheless, we do not restrict the review only to previous works which focus only on rodents, but we include similar approaches that could be easily ported to this particular case (typically other small mammals and insect monitoring applications).</p>
</sec>
<sec id="sec2-sensors-19-03274">
<title>2. Problem Statement</title>
<p>Behavioral phenotyping depends upon the annotated activity of rodents/small animals. We can identify the activity when we see how the rodents/small animals move, behave and act over an extended periods of time. One of the many proposed approaches is to track the limb movements of the rodents and convert them into quantifiable patterns. Limb tracking can be either achieved by recording the limbs from the frontal, lateral, top or bottom view. Cases shown in <xref ref-type="fig" rid="sensors-19-03274-f001">Figure 1</xref> and <xref ref-type="fig" rid="sensors-19-03274-f002">Figure 2</xref> are typical examples of activity tracking in rodents and small animals. They present the following challenges:</p>
<list list-type="order">
<list-item>
<p>Spatial resolution in most consumer-grade video cameras is not sufficient for effective tracking when the temporal resolution increases. Usually, cameras increase the frames per second ratio by decreasing the image resolution.</p>
</list-item>
<list-item>
<p>Limbs might move faster at one point in time while they might be stationary at another point in time, rendering the development of uniform motion model impossible.</p>
</list-item>
<list-item>
<p>The limbs might overlap with each other or other body parts, therefore, presenting occlusions.</p>
</list-item>
<list-item>
<p>Some settings require specific lighting conditions, which may make automated gesture recognition more difficult.</p>
</list-item>
</list>
</sec>
<sec id="sec3-sensors-19-03274">
<title>3. Motion Tracking Principles in Videos</title>
<p>Videos are sequences of images/frames. When displayed with sufficient frequency, they will appear as continuous content to the human eye. Therefore, all the image processing techniques can be applied to the individual video frames [<xref ref-type="bibr" rid="B10-sensors-19-03274">10</xref>,<xref ref-type="bibr" rid="B11-sensors-19-03274">11</xref>]. Moreover, the contents of two consecutive frames are often closely related, thus, making object and motion tracking possible in videos. Motion detection/object tracking in videos is done by detecting objects in individual frames. It involves monitoring an object’s shape and motion trajectory in every frame. This is achieved by solving the temporal correspondence problem, to match a region in successive frames of a video sequence [<xref ref-type="bibr" rid="B12-sensors-19-03274">12</xref>,<xref ref-type="bibr" rid="B13-sensors-19-03274">13</xref>,<xref ref-type="bibr" rid="B14-sensors-19-03274">14</xref>].</p>
<p>Motion detection provides additional information for detection and tracking. Most of the state-of-the-art methods involve single or multiple techniques for motion detection. For the sake of clarity in this survey, we divide these approaches in background subtraction/temporal differencing and statistical/learning-based approaches.</p>
<sec id="sec3dot1-sensors-19-03274">
<title>3.1. Background Subtraction-Based Approaches</title>
<p>Commonly used for motion segmentation in static scenes, background subtraction attempts to detect and track motion by subtracting the current image pixel-by-pixel from a reference/background image. The pixels which yield a difference above a threshold are considered as foreground. The creation of the background image is known as background modeling. Once the foreground pixels are classified, some morphological post-processing is done to enhance the detected motion regions. Different techniques for background modeling, subtraction and post-processing result in different approaches for the background subtraction method [<xref ref-type="bibr" rid="B15-sensors-19-03274">15</xref>,<xref ref-type="bibr" rid="B16-sensors-19-03274">16</xref>,<xref ref-type="bibr" rid="B17-sensors-19-03274">17</xref>,<xref ref-type="bibr" rid="B18-sensors-19-03274">18</xref>,<xref ref-type="bibr" rid="B19-sensors-19-03274">19</xref>].</p>
<p>In temporal differencing, motion is detected by taking pixel-by-pixel difference of consecutive frames (two or three). It is different from background subtraction in the sense that the background or reference image is not stationary. It is mainly used in scenarios involving a moving camera [<xref ref-type="bibr" rid="B20-sensors-19-03274">20</xref>,<xref ref-type="bibr" rid="B21-sensors-19-03274">21</xref>,<xref ref-type="bibr" rid="B22-sensors-19-03274">22</xref>,<xref ref-type="bibr" rid="B23-sensors-19-03274">23</xref>,<xref ref-type="bibr" rid="B24-sensors-19-03274">24</xref>].</p>
</sec>
<sec id="sec3dot2-sensors-19-03274">
<title>3.2. Statistical and Learning-Based Approaches</title>
<p>Some methods distinguish between foreground and background keeping and updating statistics of the foreground and background pixels. Foreground and background pixels are differentiated by comparing pixel statistics with that of the background model. So, in essence, motion tracking is achieved by tracking the statistical models of foreground (object) and the background in each video frame. This approach is stable in the presence of noise, illumination changes and shadows [<xref ref-type="bibr" rid="B25-sensors-19-03274">25</xref>,<xref ref-type="bibr" rid="B26-sensors-19-03274">26</xref>,<xref ref-type="bibr" rid="B27-sensors-19-03274">27</xref>,<xref ref-type="bibr" rid="B28-sensors-19-03274">28</xref>,<xref ref-type="bibr" rid="B29-sensors-19-03274">29</xref>,<xref ref-type="bibr" rid="B30-sensors-19-03274">30</xref>,<xref ref-type="bibr" rid="B31-sensors-19-03274">31</xref>,<xref ref-type="bibr" rid="B32-sensors-19-03274">32</xref>,<xref ref-type="bibr" rid="B33-sensors-19-03274">33</xref>]. Some approaches employ optical flow to track the apparent motion and then use it to predict the position/pose in the next frames. Optical flow is the distribution of apparent velocities/ movement of brightness patterns in an image [<xref ref-type="bibr" rid="B34-sensors-19-03274">34</xref>,<xref ref-type="bibr" rid="B35-sensors-19-03274">35</xref>,<xref ref-type="bibr" rid="B36-sensors-19-03274">36</xref>,<xref ref-type="bibr" rid="B37-sensors-19-03274">37</xref>,<xref ref-type="bibr" rid="B38-sensors-19-03274">38</xref>,<xref ref-type="bibr" rid="B39-sensors-19-03274">39</xref>,<xref ref-type="bibr" rid="B40-sensors-19-03274">40</xref>]. In some cases, the optical flow-based prediction is further reinforced by introducing a learning element, algorithms are trained either to predict the position of the object in successive frames or pose of a person/animal or detect a specific object in each frame. Some of these learning-based approaches do not rely on the explicit estimation of optical flow at all, instead, they try to solve this problem by learning how the object looks and then tracking similar objects in every frame or by learning how the object moves [<xref ref-type="bibr" rid="B41-sensors-19-03274">41</xref>,<xref ref-type="bibr" rid="B42-sensors-19-03274">42</xref>,<xref ref-type="bibr" rid="B43-sensors-19-03274">43</xref>].</p>
</sec>
</sec>
<sec id="sec4-sensors-19-03274">
<title>4. Major Trends</title>
<p>Motion tracking for neuroscience applications is not formally different from general motion tracking; therefore, all the motion tracking techniques can be applied to it in one way or the other. Although the general idea is the same, the environment for such type of motion tracking can be different from general-purpose tracking. A typical setup for neuroscience applications includes a closed environment (either a room or a box), video cameras, the animal and control systems. The animal can either be restrained or freely behaving. There might multiple cameras recording the motion from different angles. For this survey, we will go through all those cases which involve motion tracking (especially limbs tracking, head tracking and gesture tracking) of laboratory animals for behavioral phenotyping or medical assessment purposes. It is to be noted that research on gesture tracking/pose estimation for humans has seen significant improvements in recent years, those techniques cannot be applied as they are to gesture tracking in rodents and small animals for the following reasons:<list list-type="order"><list-item><p>Human gait parameters and motion patterns are inherently different than those of four-legged animals/rodents/lab animals.Human gait can be solely represented by an inverted pendulum model while in rats/four-legged animals, the inverted pendulum model represents only a percentage of the gait (up to 70% according to some researchers). Moreover, the degree of freedom for human gait is different than that of four-legged animals. [<xref ref-type="bibr" rid="B44-sensors-19-03274">44</xref>,<xref ref-type="bibr" rid="B45-sensors-19-03274">45</xref>,<xref ref-type="bibr" rid="B46-sensors-19-03274">46</xref>,<xref ref-type="bibr" rid="B47-sensors-19-03274">47</xref>].</p></list-item><list-item><p>Gesture tracking techniques developed for humans are mostly optimized for the environments in which humans dwell, therefore, they can’t be directly imported for lab environments.</p></list-item><list-item><p>Human subjects do not need to be trained to perform a supervised task. For example, let’s say a neuroscientist wants to investigate the effect of a certain neurophysiological regime on physical activity, he/she can simply ask the test subject to either walk or exercise. The same cannot be said for small animals/rodents. They need to be trained on a treadmill, therefore, the tracking methods developed for humans might not have the same efficiency for rodents.</p></list-item><list-item><p>For reliable behavioral phenotyping, the gesture tracking/pose estimation should be highly accurate, therefore, they often need more fine-tuning.</p></list-item></list></p>
<p>Although techniques developed for human gesture tracking/pose estimation cannot be applied as they are to gesture tracking in rodents and small animals, many of the studies mentioned in this survey either take inspiration from those techniques or built their solutions based upon them.</p>
<p>Based on their intended use and nature, we have divided the approaches according to the hierarchy outlined in <xref ref-type="fig" rid="sensors-19-03274-f003">Figure 3</xref>.</p>
</sec>
<sec id="sec5-sensors-19-03274">
<title>5. Hardware Based Methods</title>
<p>LocoWhisk is a commercial solution that proposes to quantify and track locomotion by tracking the whiskers’ movements using a specialized hardware setup [<xref ref-type="bibr" rid="B48-sensors-19-03274">48</xref>]. The setup is comprised of high-speed cameras, a pedobarograph, and infrared lighting. Open-source image processing techniques are then used to track the infra-red illuminated whiskers. The inventors behind this solution haven’t provided any objective evaluation of how effective is their solution in tracking whisking movements. The solution is not compatible with any existing equipment, therefore, it has to be bought and installed from scratch. Also, since they haven’t provided the details into the image processing pipeline, we cannot compare and validate its effectiveness.</p>
<p>Kain et al. [<xref ref-type="bibr" rid="B49-sensors-19-03274">49</xref>] proposed an explicit hardware-based leg tracking method for automated behavior classification in Drosophila flies. The fly is made to walk on a spherical treadmill. Dyes which are sensitive to specific wavelengths of light are applied to its legs and then the leg movement is recorded by two mounted cameras. This way, 15 gait features are recorded and tracked in real-time. This approach has the appeal for real-time deployment but it cannot be generalized to any limb tracking application because it needs a specific hardware setup. Moreover, being heavily dependent on photo-sensitive dyes decreases its robustness. Also, the flies have to walk on a spherical treadmill for this method to be effective which is not always easy since it is very hard to train the flies.</p>
<p>Snigdha et al. [<xref ref-type="bibr" rid="B50-sensors-19-03274">50</xref>] proposed 3D tracking of mice whiskers using optical motion capture hardware. The 3D tracking system (Hawk Digital real-time System, Motion Analysis Corp., Santa Rosa, CA, USA) is composed of two joint cameras and the Cortex analysis software (Motion Analysis, CA, USA). The whiskers are marked with retro-reflective markers and their X, Y, and Z coordinates are digitized and stored along with video recordings of the marker movements. The markers are made from a retro-reflective tape backed with adhesive (Motion Analysis Corp., Santa Rosa, CA, USA) and fastened onto the whiskers using the tape’s adhesive. Markers were affixed to the whisker at a distance of about 1 cm from the base. Reliable 3D tracking requires a marker to be visible at all times by both cameras. This condition can be satisfied in head-fixed mice where the orientation of the mouse to the cameras remains fixed. The system was connected to a dual processor Windows-based computer for data collection. The proposed tracking framework is easy to install and computationally cheap. The hardware components are not also high end and expensive, therefore, this system can be set up relatively cheaply. But like other hardware-assisted frameworks, it also needs specialized hardware and thus, it isn’t very scalable and portable. Moreover, for reliable tracking, the retro-reflective markers should be visible to the cameras at all times, therefore, it cannot handle occlusions and thus, it is not robust. Also, since the method is invasive, it might affect the mice behavior, therefore, rendering the behavioral analysis results skewed.</p>
<p>Scott Tashman et al. [<xref ref-type="bibr" rid="B51-sensors-19-03274">51</xref>] proposed a bi-plane radiography assisted by static CT scan-based method for 3D tracking of skeletons of small animals. The high-speed biplane radiography system consists of two 150 kV X-ray generators optically coupled to synchronized high-speed video cameras. For static radiostereometric analysis, they implanted a minimum of three radio-opaque bone markers per bone to enable accurate registration between the two views. The acquired radiographs are first corrected for geometric distortion. They calculated ray-scale weighted centroids for each marker with sub-pixel resolution. They tested this system on dogs and reported an error of 0.02 mm when inter-marker distance calculated by their system was compared to the true inter-marker distance of 30 mm. For dynamic gait tracking, this system is reported to be very accurate but the accuracy comes at a cost, the system is expensive and need dedicated hardware. Also, since the system includes specialized hardware, it is not easy to operate. Moreover, since the marker implantation is invasive, it can alter the behavior of animals being studied.</p>
<p>Harvey et al. [<xref ref-type="bibr" rid="B52-sensors-19-03274">52</xref>] proposed an optoelectronic based whisker tracking method for head-fixed rats. In the proposed method, the rat’s head is fixed to a metal bar protruding from the top of the restraining device. Its paw rests on a micro switch that records lever presses. A turntable driven by a stepper motor rotates a single sphere/cube into the rat’s “whisking space”. The whiskers are marked to increase the chances of detection. The movements of a single whisker are detected by a laser emitter and an array of CCD detectors. Once the data is recorded, a single whisker is identified manually which serves as a reference point. As the article is more focused on whisking responses of rodents to external stimuli, they have not reported the whiskers’ detection and tracking accuracy. R. Bermejo et al. [<xref ref-type="bibr" rid="B53-sensors-19-03274">53</xref>] reported a similar approach for tracking individual whiskers. They restrained the rats and then used a combination of CCDs and laser emitters. The rats were placed in such a way that their whiskers blocked the path of the laser, casting a shadow over CCDs, thus, registering the presence of a whisker which can be tracked by tracking the voltage shifts on CCD array. They also have not reported tracking accuracy. Both of these methods need the whiskers to be visible at all time, therefore, these approaches cannot perform well in the case of occlusions. Moreover, the head of the rats need to be fixed, so they cannot be studied while behaving freely. Also, apart from the need for specialized hardware, the system needs the user to initialize tracking, so it is not completely automated.</p>
<p>Kyme et al. [<xref ref-type="bibr" rid="B54-sensors-19-03274">54</xref>] proposed a marker-assisted hardware-based method for head motion tracking of freely behaving and tube-bounded rats. They glued a marker with a specific black and white pattern to the rat’s head. Motion tracking was performed using the Micron-Tracker <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (ClaronTech. Inc., Toronto, ON, Canada), a binocular-tracking system that computes a best-fit pose of printed markers in the field of measurement [<xref ref-type="bibr" rid="B55-sensors-19-03274">55</xref>]. The authors have reported accurate tracking for more than 95% of the time in the case of tube-bounded rats and similar performance for freely behaving rats if the tracking algorithm is assisted 10% of the time. The system is simple and effective for tube-bound rats and can be operated easily. But the approach has one major drawback; it can only be used in a very specific setting. It requires a specialized setup and it needs to glue external markers to the test subject’s head, which might affect its behavior. Moreover, the same authors have used the Micron-Tracker based approach for synchronizing head movements of a rat with positron emission tomography scans of their brains and have reported that the marker-assisted tracking method was able to synchronize the head movements with scan intervals with an error of less than 10 ms [<xref ref-type="bibr" rid="B56-sensors-19-03274">56</xref>].</p>
<p>Pasquet et al. [<xref ref-type="bibr" rid="B57-sensors-19-03274">57</xref>] proposed a wireless inertial sensors-based approach for tracking and quantifying head movements in rats. The inertial measurement unit (IMU) contains a digital 9-axis inertial sensor (MPU-9150, Invensense, San Jose, CA, USA) that samples linear acceleration, angular velocity and magnetic field strength in three dimensions, a low-power programmable microcontroller (PIC16, Microchip, Chandler, AZ, USA) running a custom firmware and a Bluetooth radio, whose signal is transmitted through a tuned chip antenna. This system was configured with Labview for data acquisition and the analysis was done in R. The sensors record any head movements by registering the relative change in acceleration. Since the sensors record data in nine axes, the method is used to detect events in rat’s behavior based on head movements. The authors have reported a detection accuracy of 96.3% and a mean correlation coefficient of 0.78 ± 0.14 when the recorded data is compared for different rats (n = 19 rats).Since the proposed system records a head’s acceleration, angular velocity and magnetic field strength in all three dimensions, this opens up the possibility of using this dataset for high-end learning algorithms for behavioral classification. Moreover, since the dataset is based on well-studied physical phenomena (acceleration and velocity), it can also be used to develop deterministic models of the head’s movements. The reported performance figures are very good in terms of event detection and consistency but the system can only be used to track head movements. Also, the system requires specialized hardware which limits its portability. Since the method needs an inertial sensor to be attached to the head of the rats, it is invasive and therefore, can alter the rat’s behavior.</p>
<p>Hamers et al. proposed a specific setup based on inner-reflecting plexiglass walkway [<xref ref-type="bibr" rid="B58-sensors-19-03274">58</xref>]. The animals traverse a walkway (plexiglass walls, spaced 8 cm apart) with a glass floor (109, 3, 15, 3, 0.6 cm) located in a darkened room. The walkway is illuminated by a fluorescent tube from the long edge of the glass floor. For most of the way, the light travels internally in the glass walkway, but when some pressure is applied, for example by motion of a mouse, the light escapes and is visible from outside. The escaped light, which is scattered from the paws of the mouse, is recorded by a video camera aimed at a <inline-formula><mml:math id="mm2"><mml:mrow><mml:msup><mml:mn>45</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> mirror beneath the glass walkway. The video frames are then thresholded to detect bright paw prints. The paws are labeled (left, right, front, hind). The system can extrapolate a tag (label of the footprint) to the bright areas in the next frame which minimizes the need for user intervention but in some cases, user intervention becomes necessary. The authors haven’t reported paw detection/tracking performance. The system is reliable for paw-tracking but the required setup makes its wide-scale application less likely, therefore, it can only be used for one specific purpose.</p>
</sec>
<sec id="sec6-sensors-19-03274">
<title>6. Video Tracking Aided by Hardware</title>
<sec id="sec6dot1-sensors-19-03274">
<title>6.1. Semi-Automated</title>
<p>Dorman et al. [<xref ref-type="bibr" rid="B59-sensors-19-03274">59</xref>] conducted a comparative study of two commercially available hardware-assisted gait analysis systems; DigiGait and TreadScan. The <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> imaging system uses a high-speed, 147 frames-per-second video camera mounted inside a stainless steel treadmill chassis below a transparent treadmill belt to capture ventral images of the subject. The treadmill is lit from the inside of the chassis by two fluorescent lights and overhead by one fluorescent light. The <inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> imaging system uses a high-speed, 100 frames-per-second video camera adjacent to a translucent treadmill belt to capture video reflected from a mirror mounted under the belt at <inline-formula><mml:math id="mm5"><mml:mrow><mml:msup><mml:mn>45</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Images are automatically digitized by <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> systems. <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> videos are manually cropped and imported, and then automatically analyzed. The software identifies the portions of the paw that are in contact with the treadmill belt in the stance phase of stride as well as tracks the foot through the swing phase of the stride. Measures are calculated for 41 postural and kinematic metrics of gait. The authors found that <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> system consistently measured significantly longer stride measures than <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Both systems’ measures of variability were equal. Reproducibility was inconsistent in both systems. Only the <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> detected normalization of gait measures and the time spent on analysis was dependent on operator experience. <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> have been particularly well received in neurophysiological research [<xref ref-type="bibr" rid="B60-sensors-19-03274">60</xref>,<xref ref-type="bibr" rid="B61-sensors-19-03274">61</xref>,<xref ref-type="bibr" rid="B62-sensors-19-03274">62</xref>,<xref ref-type="bibr" rid="B63-sensors-19-03274">63</xref>,<xref ref-type="bibr" rid="B64-sensors-19-03274">64</xref>,<xref ref-type="bibr" rid="B65-sensors-19-03274">65</xref>,<xref ref-type="bibr" rid="B66-sensors-19-03274">66</xref>,<xref ref-type="bibr" rid="B67-sensors-19-03274">67</xref>,<xref ref-type="bibr" rid="B68-sensors-19-03274">68</xref>,<xref ref-type="bibr" rid="B69-sensors-19-03274">69</xref>,<xref ref-type="bibr" rid="B70-sensors-19-03274">70</xref>].</p>
<p>Cleversys Inc. (<uri xlink:href="http://cleversysinc.com/CleverSysInc/">http://cleversysinc.com/CleverSysInc/</uri>) introduced a commercial solution for gait analysis in rodents, called GaitScan [<xref ref-type="bibr" rid="B71-sensors-19-03274">71</xref>]. GaitScan system records videos of the rodent running either on a transparent belt treadmill or on a clear free-walk runway. The video of the ventral (underside) view of the animal is obtained using a high-speed digital camera. The video essentially captures the footprints of the animal as they walk/run. GaitScan software can work with videos taken from any treadmill or runway device that allows the capture of its footprints on any video capturing hardware system with a high-speed camera. The accompanying software lets the user track multiple gait parameters which can be later used for behavioral phenotyping. This solution has also been used in multiple studies [<xref ref-type="bibr" rid="B72-sensors-19-03274">72</xref>,<xref ref-type="bibr" rid="B73-sensors-19-03274">73</xref>,<xref ref-type="bibr" rid="B74-sensors-19-03274">74</xref>,<xref ref-type="bibr" rid="B75-sensors-19-03274">75</xref>,<xref ref-type="bibr" rid="B76-sensors-19-03274">76</xref>].</p>
<p>TrackSys ltd. (<uri xlink:href="http://www.tracksys.co.uk/">http://www.tracksys.co.uk/</uri>) introduced two commercial systems for rodents’ motor analysis. One system is called ’ErasmusLadder’. The mouse traverses a horizontal ladder between two goal boxes. Each rung of the ladder contains a touch-sensitive sensor. These sensors allow the system to measure numerous parameters relative to motor performance and learning such as step time and length, missteps, back steps and jumps [<xref ref-type="bibr" rid="B77-sensors-19-03274">77</xref>]. It has been used in multiple studies [<xref ref-type="bibr" rid="B78-sensors-19-03274">78</xref>,<xref ref-type="bibr" rid="B79-sensors-19-03274">79</xref>,<xref ref-type="bibr" rid="B80-sensors-19-03274">80</xref>,<xref ref-type="bibr" rid="B81-sensors-19-03274">81</xref>,<xref ref-type="bibr" rid="B82-sensors-19-03274">82</xref>]. Its tracking performance hasn’t been reported by its manufacturer. The other system is called ’CatWalk’ [<xref ref-type="bibr" rid="B83-sensors-19-03274">83</xref>]. It is comprised of a plexiglass walkway that can reflect light internally. When the animals’ paws touch the glass, the light escapes as their paw print and is captured by a high-speed camera mounted beneath the walkway. It can be used to quantify several gait parameters such as pressure, stride length, swing and stance duration. Multiple researchers have used ’CatWalk’ in gait analysis [<xref ref-type="bibr" rid="B84-sensors-19-03274">84</xref>,<xref ref-type="bibr" rid="B85-sensors-19-03274">85</xref>,<xref ref-type="bibr" rid="B86-sensors-19-03274">86</xref>,<xref ref-type="bibr" rid="B87-sensors-19-03274">87</xref>,<xref ref-type="bibr" rid="B88-sensors-19-03274">88</xref>].</p>
<p>Knutsen et al. [<xref ref-type="bibr" rid="B89-sensors-19-03274">89</xref>] proposed the use of overhead IR LEDs along with video cameras for head and whisker tracking of unrestrained behaving mice. The overhead IR LEDs are used to flash IR light onto the mouse head which is reflected from its eyes. The reflected flash is recorded by an IR camera. In the first few frames of every movie, a user identifies a region-of-interest (ROI) for the eyes which encircles a luminous spot (reflection from the eye). This luminous spot is tracked in subsequent frames by looking for pixels with high luminosity in the shifted ROI. Once eyes are located in every frame, they are used to track head and whiskers in intensity videos. First, a mask averaged over the frames containing no mice is subtracted from the frame. Then user-initiated points are used to form whisker shaft by spline interpolation. For the next frame, sets of candidate points are initiated and shaft from current frames is convolved with candidate shafts from the next frame to locate the set of points most likely being a whisker. Although the pipeline has no temporal context involved, yet it is quite effective in whisker tracking with a high Pearson correlation between ground truth and tracked whisker shafts. The downside of this approach is the need for high-speed videos and additional IR hardware. Moreover, since the whisker tracking is dependent upon the accurate detection of eyes in every frame for finding the region of interest which contains the head, any flashes onto the IR camera or any occlusions of the eyes can result in a considerable deviation in whisker tracking.</p>
<p>Gravel et al. [<xref ref-type="bibr" rid="B90-sensors-19-03274">90</xref>] proposed a tracking method assisted by an X-Ray area scan camera for gait parameters of rats walking on a treadmill. The system consists of a Coroskop C arm X-ray system from Siemens, equipped with an image intensifier OPTILUX 27HD. The X-Ray system is used to detect fluoroscopic markers placed on hind limbs of the rat. A high-speed area scan camera from Dalsa (DS-41-300K0262), equipped with a C-mount zoom lens (FUJINON-TV, H6X12.R, 1:1.2/12.5–75) mounted on the image intensifier is used for video acquisition and a computer is used to overlay the detected markers on the video. The treadmill with the overlaying box is placed on a free moving table and positioned near the X-ray image intensifier. The X-ray side view videos of locomotion are captured while the animal walks freely at different speeds imposed by the treadmill. The acquired video and marker data are processed in four steps; correction for image distortion, image denoising and contrast enhancement, frame-to-frame morphological marker identification and statistical gait analysis. The data analysis process can be run in automated mode for image correction and enhancement however the morphological marker identification is user-assisted. The kinematic gait patterns are computed using a Bootstrap method [<xref ref-type="bibr" rid="B91-sensors-19-03274">91</xref>]. After multiple Monte Carlo runs, the authors have reported consistent gait prediction and tracking with confidence of 95%. They have compared the performance of the proposed system with manual marker annotation by a user by first manually processing 1 h 30 min of data and then processing only 12 min data by the system assisted by the same user. They have reported only 8% deviation in gait cycle duration, therefore, claiming a 7-fold decrease in processing time with acceptable loss in accuracy. The system is robust for gait pattern analysis, it can track multiple gait parameters, therefore, making complex behavioral classification possible. However, the system is still not scalable and portable because it relies on dedicated hardware. The system is not fully automated as well as it relies upon continuous user assistance. Moreover, the system needs physical markers painted on the limbs, therefore, it cannot work reliably in a situation where painting markers is not an option.</p>
<p>John et al. [<xref ref-type="bibr" rid="B92-sensors-19-03274">92</xref>] proposed a semi-automated approach for simultaneously extracting three-dimensional kinematics of multiple points on each of an insect’s six legs. White dots are first painted on insect’s leg joints. Two synchronized video cameras placed under the glass floor of the platform are used to record video data at 500 frames per second. The synchronized video data is then used to generate 3D point clouds for the regions of interest by triangulation. The captured video frames are first subtracted from a background frame modeled by a Gaussian mean of 100 frames with no insects. After image enhancement, a user defines the initial tracking positions of leg joints in a 3D point cloud which are then tracked both in forward and backward direction automatically. The user can correct any mismatched prediction in any frame. The authors have reported a tracking accuracy of 90% when the user was allowed to make corrections in 3–5% of the frames. The proposed approach is simple in terms of implementation, accurate in terms of spatial and temporal resolution and easy to operate. Also, the proposed method produced a rich dataset of insects’ legs kinematics, therefore, making complex behavioral analysis possible. However, it needs constant user assistance and does not have any self-correction capability.</p>
</sec>
<sec id="sec6dot2-sensors-19-03274">
<title>6.2. Completely Automated</title>
<sec id="sec6dot2dot1-sensors-19-03274">
<title>6.2.1. Background Subtraction-Based Approaches</title>
<p>Akihiro Nakamura et al. [<xref ref-type="bibr" rid="B93-sensors-19-03274">93</xref>] proposed a depth sensor-based approach for paw tracking of mice on a transparent floor. The system is composed of an open-field apparatus, a Kinect sensor, and a personal computer. It captures the subject’s shape from below using a low-cost infrared depth sensor (Microsoft Kinect) and an opaque infrared-pass filter. The open field is a square of <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mn>400</mml:mn><mml:mspace width="3.33333pt"></mml:mspace><mml:mi>mm</mml:mi><mml:mo>×</mml:mo><mml:mn>400</mml:mn><mml:mspace width="3.33333pt"></mml:mspace><mml:mi>mm</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and the height of the surrounding wall is 320 mm. The Kinect device is fixed 430 mm below the floor so that the entire open-field area can be captured by the device. For the experiment in the opaque conditions, the floor of the open field was covered with tiled infrared-pass filters (FUJIFILM IR-80 (Fuji Film, Tokyo, Japan)), which are commonly used in commercial cameras. The depth maps, consisting of <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mn>320</mml:mn><mml:mo>×</mml:mo><mml:mn>240</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> depth pixels, are captured at 30 frames per second. The tracking algorithm has four steps; pre-processing, feature-point extraction, footprint detection, and labeling. During pre-processing, the subject’s depth information is extracted from the raw depth map by applying background subtraction to the raw depth map. The noise produced by pre-processing steps is removed by morphological operations. AGEX algorithm [<xref ref-type="bibr" rid="B94-sensors-19-03274">94</xref>] is used for feature extraction after pre-processing. Center of mass of AGEX point clouds is used for paw detection and labeling. All those pixels whose Euclidean distance is lower than a threshold from the center of mass are considered to be member pixels of the paws. This framework offers the benefits of low computational cost and easy-to-install system. The proposed system can also be used in real-time. However, it is not robust. It can be used only for paw tracking in a specific setting. Moreover, it cannot be used for other gesture tracking measures, such as head or whiskers tracking.</p>
<p>César S. Mendes et al. [<xref ref-type="bibr" rid="B95-sensors-19-03274">95</xref>] proposed an integrated hardware and software system called ’MouseWalker’ that provides a comprehensive and quantitative description of kinematic features in freely walking rodents. The MouseWalker apparatus is comprised of four components: the fTIR floor and walkway wall, the supporting posts, the <inline-formula><mml:math id="mm16"><mml:mrow><mml:msup><mml:mn>45</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> mirror, and the background light. A white LED light strip for black and white cameras or a colored LED light strip for color cameras is glued to a 3/8-inch U-channel aluminum base LED mount. This LED/aluminum bar is clamped to the long edges of a 9.4-mm (3/8-inch) thick piece of acrylic glass measuring 8 by 80 cm. A strip of black cardboard is glued and sewn over the LED/acrylic glass contact areas. To build the acrylic glass walkway, all four sides were glued together with epoxy glue and cable ties and placed over the fTIR floor. Videos are acquired using a Gazelle 2.2-MP camera (Point Grey, Richmond, VA, Canada) mounted on a tripod and connected to a Makro-Planar T 2/50 lens (Carl Zeiss, Jena, Germany) at maximum aperture (f/2.0) to increase light sensitivity and minimize depth of field. The ’MouseWalker’ program is developed and compiled in MATLAB (The Mathworks, MA, USA) [<xref ref-type="bibr" rid="B96-sensors-19-03274">96</xref>]. The body and footprints of the mouse are distinguished from the background and each other based on their color or pixel intensity. The RGB color of the mouse body and footprints are user-defined. The tail is identified as a consecutive part of the body below a thickness threshold. Three equidistant points along the tail are used to characterize tail curvature. The head is defined by the relative position of the nose. The center and direction of the head are also recorded along with the center of the body without the tail and its orientation. A body “back” point is defined as the point which is halfway between the body center and the start of the tail. For the footprints of the animal, the number of pixels within a footprint, as well as the sum of the brightness of these pixels, are stored by the software. The ’MouseWalker’ can be used to track speed, steps frequency, swing period and length of steps, stance time, body linearity index footprint clustering and leg combination indexes: no swing, single-leg swing, diagonal-leg swing, lateral-leg swing, front or hind swing, three-leg swing, or all-legs swing (unitless). The system is quite robust; it can track multiple gait parameters and can create a rich dataset which can be used to train advanced learning-based algorithms for behavioral classification. However, the system is not very scalable and portable. The enclosing setup of the mice and the hardware configuration is too specific for portability.</p>
<p>Wang et al. [<xref ref-type="bibr" rid="B97-sensors-19-03274">97</xref>] proposed a pipeline for tracking motion and identifying the micro-behavior of small animals based on Microsoft Kinect sensors and IR cameras. This is achieved by employing Microsoft Kinect cameras along with normal video cameras to record movement of freely behaving rodents from three different perspectives. The IR depth images from Microsoft Kinect are used to extract the shape of the rodents by background subtraction. After shape extraction, five pixel-based features are extracted from the resultant blobs which are used for tracking and behavior classification by Support Vector Machines. Although the pipeline is not exclusively used for motion tracking, the idea of using depth cameras is potentially a good candidate for motion tracking as well.</p>
<p>Voigts et al. proposed an unsupervised whisker tracking pipeline aided by the use of IR sensors for selective video capturing [<xref ref-type="bibr" rid="B98-sensors-19-03274">98</xref>]. They capture high speed (1000 frames per second) video data by selectively recording those frames which contain the mice. It is achieved by sensing the mice by an IR sensor which then triggers the video camera to start recording. Once the mice leave the arena, the IR sensors trigger the video camera to stop capturing. This selectively-acquired video data is used for whisker tracking. First, a background mask is calculated by averaging over 100 frames containing no mice. This mask is subtracted from every single frame. Then vector fields from each frame that resulted in a convergence of flows on whisker-like structures are generated. These fields are then integrated to generate spatially continuous traces of whiskers which are grouped into whisker splines. This approach is completely unsupervised when it comes to whisker tracking with a rough temporal context as well. Moreover, instead of high speed video acquisition, the on-demand recording when the mice are in the arena cuts the memory requirements and this approach can be exported to other motion/gesture tracking pipelines. However, it is very greedy in terms of computational resources so it cannot be employed in real-time.</p>
<p>Nashaat et al. proposed an automated optical method for tracking animal behavior in both head-fixed and freely moving animals, in real-time and offline [<xref ref-type="bibr" rid="B99-sensors-19-03274">99</xref>]. They use a Pixy camera (Charmed labs, Carnegie Mellon University, equipped with a 10–30 mm f1.6 IR lens, controlled by open-source PixyMon software) based system for real-time tracking. The Pixy camera needs the whisker to be painted with a UV sensitive dye. For more detailed tracking, they use the tracking from the Pixy camera to guide the tracking from high definition cameras offline. They haven’t provided tracking results in comparison to ground truth but they have validated their system in different lighting conditions and environments. On one hand, their system does not need high performance and computationally expensive tracking algorithms while on the other hand, their system is not robust because it requires specialized hardware. Also, their approach is invasive because it requires physical markers (UV sensitive dye on whiskers).</p>
</sec>
<sec id="sec6dot2dot2-sensors-19-03274">
<title>6.2.2. Statistical/Learning-Based Approaches</title>
<p>Monteiro et al. [<xref ref-type="bibr" rid="B100-sensors-19-03274">100</xref>] took a similar approach to Wang et al. [<xref ref-type="bibr" rid="B97-sensors-19-03274">97</xref>] by using Microsoft Kinect depth cameras for video capturing [<xref ref-type="bibr" rid="B100-sensors-19-03274">100</xref>]. Instead of using background subtraction, they introduced a rough temporal context by tracking the morphological features of multiple frames. In their approach, the morphological features are extracted frame by frame. Then features from multiple adjacent frames are concatenated to introduce a rough temporal context. A decision tree is then trained from this dataset for automatic behavior classification. The authors have reported a classification accuracy of 66.9% when the classifier is trained to classify four behaviors on depth map videos of 25 min duration. When only three behaviors are considered, the accuracy jumps to 76.3%. Although the introduced temporal context is rough and the features are primitive, the classification performance achieved firmly establishes the usefulness of machine learning in gesture tracking for behavioral classification. Like [<xref ref-type="bibr" rid="B100-sensors-19-03274">100</xref>], this approach is also not solely used for motion tracking, but they have introduced a rough temporal context for tracking along with depth cameras which can be beneficial in motion-tracking-only approaches.</p>
<p>Petrou et al. [<xref ref-type="bibr" rid="B101-sensors-19-03274">101</xref>] proposed a marker-assisted pipeline for tracking legs of female crickets. The crickets are filmed with three cameras, two mounted above and one mounted below the crickets which are made to walk on a transparent glass floor. Leg joints are marked with fluorescent dyes for better visualization. The tracking procedure is initiated by a user by selecting a marker position in initial frames. The initial tracking is carried out to the next frames by constrained optimization and Euclidean distance between joints of the current frame and the next frame. This pipeline does a decent job in terms of tracking performance as the average deviation between human-annotated ground truth (500 digitized frames) and automatic tracking is 0.5 mm where the spatial depth of the camera is 6 pixel/mm. This approach has the potential to be applied in real-time and the required setup is not too difficult to make. However, since markers have to be painted on the legs of the cricket, it is invasive and thus, it can alter the behavior of subject under study. Moreover, this approach has only been tested on crickets, so we cannot assume that it will work with rodents/small animals.</p>
<p>Xu et al. [<xref ref-type="bibr" rid="B102-sensors-19-03274">102</xref>] proposed another marker-assisted tracking pipeline for small animals. In the proposed pipeline, the limbs and joints are first shaved, marked with dyes and then recorded with consumer-grade cameras (200 frames per second). Tracking is then done in three steps which include marker position estimation, position prediction, and mismatch occlusion. The marker position is estimated by correlation in two methods. In one method, normalized cross-correlation between the grayscale region of interest and user-generated sample markers is found. The pixels with the highest correlation are considered as the marker pixels. In the second method, the normalized covariance matrix of marker model and color ROI is used to estimate pixels with the highest normalized covariance values which are considered as marker pixels. Once the marker positions are estimated in the current frame, they are projected to the next frame by polynomial fitting and Kalman filters. For occlusion handling, they assume that a marker position or image background cannot change abruptly, so if there is a sudden change, it must be an occlusion. The approach is simple and scalable enough to be exported to any environment. Moreover, this pipeline has included measures to solve the problem of occlusions as well. However, due to its dependency on markers, it cannot be exported for general purpose motion/gesture tracking. Also, the markers are placed in a very invasive way, therefore, possibly altering the behavior of test subjects.</p>
<p>Hwang et al. [<xref ref-type="bibr" rid="B103-sensors-19-03274">103</xref>] followed a similar approach to the one proposed by John et al. [<xref ref-type="bibr" rid="B92-sensors-19-03274">92</xref>] but without the use of markers. They used a combination of six-color charge-coupled device (CCD) cameras (Sca640-70fc, BASLER Co., Schiller Park, IL, USA) for video recording of the insects. To capture the diverse motions of the target animal, they used two downward cameras and four lateral cameras as well as a transparent acrylic box. The initial skeleton of the insect was calculated manually, so the method is not completely automated. After the initial skeleton, they estimated the roots and extremities of the legs followed by middle joints estimation. Any errors in the estimation were corrected by Forward And Backward Reaching Inverse Kinematics (FABRIK) [<xref ref-type="bibr" rid="B104-sensors-19-03274">104</xref>]. The authors have not reported any quantitative results which might help us to compare it with other similar approaches however they have included graphics of their estimation results in the paper. This paper does not directly deal with motion estimation in rodents, however, given the unique approach to using cameras and pose estimation, it is a worthwhile addition to the research in the field. Since the pipeline tries to solve gesture tracking from a pose estimation perspective, it opens the possibility of using state-of-the-art pose estimation techniques for gesture tracking in rodents/small animals. However, because of its reliance on initial skeleton, the system cannot be exported for general purpose use.</p>
</sec>
</sec>
</sec>
<sec id="sec7-sensors-19-03274">
<title>7. Video Tracking Methods Mostly Dependent on Software-Based Tracking</title>
<p>In this section, we will focus on those research works that try to solve the locomotion and gesture tracking problem by processing raw and unaided video streams. In this scenario, there is neither specialized hardware installed apart from one or multiple standard video cameras nor physical markers on the mice/animals bodies that can help to track its motion. These works approach the problem from a pure computer vision point of view.</p>
<sec id="sec7dot1-sensors-19-03274">
<title>7.1. Semi-Automated</title>
<sec>
<title>Background Subtraction-Based Approaches</title>
<p>Gyory et al. [<xref ref-type="bibr" rid="B105-sensors-19-03274">105</xref>] proposed a semi-automated pipeline for tracking rat’s whiskers. In the proposed pipeline, videos are acquired with high-speed cameras (500 frames per second) and are first pre-processed to adjust the brightness. The brightness adjusted images are eroded to get rid of small camera artifacts. Then a static background subtraction is applied which leaves only the rat body in the field of view. As whiskers are represented by arcs with varying curvature, a polar-rectangular transform is applied and then a horizontal circular shift is introduced so that whiskers are aligned as straight lines on a horizontal plane. Once the curved whiskers are represented by straight lines, the Hough transform is used to locate them. This approach can be used a starting point by a researcher who wants to experiment with different automated background subtraction methods for gesture tracking/pose estimation but the approach is too weak itself and not robust enough to be considered for any future improvements. The reported computational cost is high (processing speed of 2 fps). Also, it works on high-speed videos (&gt;500 fps). It is highly sensitive to artifacts and it cannot take care of occlusion, dynamic noise, and broken whisker representation.</p>
</sec>
</sec>
<sec id="sec7dot2-sensors-19-03274">
<title>7.2. Completely Automated</title>
<sec id="sec7dot2dot1-sensors-19-03274">
<title>7.2.1. Background Subtraction-Based Approaches</title>
<p>Da Silva et al. [<xref ref-type="bibr" rid="B106-sensors-19-03274">106</xref>] conducted a study on the reproducibility of automated tracking of behaving rodents in controlled environments. Rats in a circular box of 1 m diameter with 30 cm walls. The monitoring camera was mounted in such a way that it captured the rodents from top view while they were behaving. They used a simple thresholding algorithm to determine pixels belonging to the rodent. Although the method is rudimentary as compared to state-of-the-art, the authors have reported a Pearson correlation of <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.873</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> when they repeated the same experiment at different ages of the animals, thus, validating its reproducibility. However, this setup can only be used to track the whole body of rodents, it cannot identify micro-movements such as limbs motion.</p>
<p>Leroy et al. [<xref ref-type="bibr" rid="B107-sensors-19-03274">107</xref>] proposed the combination of transparent Plexiglas floor and background modeling-based motion tracking. The rodents are made to walk on a transparent plexiglass floor illuminated by fluorescent light and is recorded from below. A background image is taken when there is no mouse on the floor. This background image is then subtracted from every video frame to produce a continuously updating mouse silhouette. The tail of the mouse is excluded by an erosion followed by dilation of the mouse silhouette. Then the center of mass of the mouse is calculated which was tracked through time to determine if the mouse is running or walking. Since the paws are colored, color segmentation is used to isolate paws from the body. The authors have reported a maximum tracking error of <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mspace width="3.33333pt"></mml:mspace><mml:mi>mm</mml:mi><mml:mo>±</mml:mo><mml:mn>1.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a minimum tracking error of 2 mm <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mspace width="3.33333pt"></mml:mspace><mml:mn>1.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> when 203 manually annotated footprints are compared to their automatic counterparts. This approach has the advantages of easy-installation and simplicity, yet it can track the mouse from only one side. Moreover, being dependent on colored paws, this approach cannot be exported for general purpose gesture tracking/pose estimation.</p>
<p>Nathan et al. [<xref ref-type="bibr" rid="B108-sensors-19-03274">108</xref>] proposed a whisker tracking method for mice based on background subtraction, whisker modeling, and statistical approaches. The heads of the mice are fixed, so they are not behaving freely. They use a high-speed camera with a shutter speed of 500 frames per second. To track whiskers, an average background image is modeled from all the video frames and then subtracted from every single frame. Afterward, pixel-level segmentation is done to initiate candidate sites by looking for line like artifacts. Once the candidate boxes are initiated, they are modeled by two ellipsoids with perpendicular axes. The ellipsoid with higher eccentricity is the best possible candidate site for whiskers. These whiskers are then traced in every single frame of the video sequence by using expectation maximization. The approach has some strong points. It requires no manual initiation, it is highly accurate and because of superb spatial resolution and pixel-level tracking, even micro-movements of whiskers can be tracked. But all the strengths come at a cost; the approach is computationally very expensive which means it cannot be deployed in real-time. There is another downside to pixel-level and frame-level processing, the temporal context is lost in the process.</p>
<p>Heidi et al. [<xref ref-type="bibr" rid="B109-sensors-19-03274">109</xref>], proposed Automated Gait Analysis Through Hues and Areas (AGATHA). AGATHA first isolates the sagittal view of the animal by subtracting a background image where the animal is not present, transforming the frame into an HSV (Hues, Saturation, Value) image. The hue values are used to convert the HSV image into a binomial silhouette. Next, AGATHA locates the row of pixels representing the interface between the rat and the floor. AGATHA may not accurately locate the rat-floor interface if the animal moves with a gait pattern containing a completely aerial phase. Second, AGATHA excludes the majority of nose and tail contacts with the floor by comparing the contact point to the animal’s center of the area in the sagittal view. Foot contact with the ground is visualized over time by stacking the rat/floor interface across multiple frames. The paw contact stacked over multiple frames is then used for gait analysis. Multiple gait parameters such as limbs velocity, stride frequency can be calculated. When results from AGATHA were compared to manual annotation on a 1000 fps video, they deviated by a small amount. For example, limbs velocity calculated b AGATHA was 1.5% off from the velocity calculated manually. Similarly, AGATHA registered a difference of 0.2 cm in stride length from the manual annotation. This approach is simple and scalable and can be easily exported to track gait parameters from other angles of view. However, the gait parameters calculations rely on the subject contact with the floor, therefore, it might not be able to calculate the gait parameters in an aerial pose, therefore, this approach is limited in scope.</p>
</sec>
<sec id="sec7dot2dot2-sensors-19-03274">
<title>7.2.2. Statistical/Learning-Based Approaches</title>
<p>Dankert et al. [<xref ref-type="bibr" rid="B110-sensors-19-03274">110</xref>] proposed a machine vision-based automated behavioral classification approach for Drosophila. The approach does not cover locomotion in rodents, it covers micro-movements in flies. Videos of a pair of male and female flies are recorded for 30 min in a controlled environment. Wingbeat and legs motion data is manually annotated for lunging, chasing, courtship and aggression. The data analysis consists of four stages. In the first stage, the Foreground image <inline-formula><mml:math id="mm20"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed by dividing the original image I by (<inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) (<inline-formula><mml:math id="mm22"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> values in false-colors). In the second stage, The fly body is localized by fitting a Gaussian mixture model [<xref ref-type="bibr" rid="B111-sensors-19-03274">111</xref>] (GMM) with three Gaussians; background, other parts, and body to the histogram of <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> values (gray curve) using the Expectation-Maximization (EM) algorithm [<xref ref-type="bibr" rid="B111-sensors-19-03274">111</xref>]. First (top) and final (bottom) iterations of the GMM-EM optimization. All pixels with brightness values greater than a threshold are assigned to the body and are fitted with an ellipse. In the third stage, the full fly is detected by segmenting the complete fly from the background, with body parts and wings [<xref ref-type="bibr" rid="B112-sensors-19-03274">112</xref>]. In the fourth stage, head and abdomen are resolved by dividing the fly along the minor axis of the body ellipsoid and comparing the brightness-value distribution of both halves. In the fifth stage, 25 measurements are computed, characterizing body size, wing pose, and position and velocity of the fly pair. A k-nearest neighbor classifier is trained for action detection. The authors have reported a false positive rate for lunging at 0.01 when 20 min worth of data was used for training the classifier. Although this article does not directly deal with rodents, the detection and tracking algorithms used for legs and wings can be used for legs motion detection in rodents too. The approach is built upon proven statistical models. It can handle instrument noise. Since it has a learning element, the more data it sees, the better it gets. However, since the learning is not end to end, and the data processing pipeline is complex and need in depth understanding of statistical theory, therefore, it is hard to work with for a neuroscientist. Since it has a background modeling component, therefore, it also suffers from the inherent weaknesses of background subtraction models i.e., sensitivity to sudden changes in environment.</p>
<p>Kim et al. [<xref ref-type="bibr" rid="B113-sensors-19-03274">113</xref>] proposed a method similar to the one proposed by Clack et al. [<xref ref-type="bibr" rid="B108-sensors-19-03274">108</xref>] to track whisker movements in freely behaving mice. They use Otsu’s algorithm to separate foreground and background and then find the head of the mouse by locating a triangular-shaped object in the foreground. Once the head and snout are detected, the Hough transform is used to find line-like shapes (whiskers) on each side of the snout. Midpoints of the detected lines are used to form ellipsoidal regions which help track whiskers in every single frame. This pipeline was proposed to track whisking in mice after a surgical procedure. There is no ground truth available, so the approach cannot be evaluated for tracking quantitatively. The pipeline is simple and easy to follow. It can be used to track heads and whiskers in freely behaving mice. However, it is not feasible for real-time deployment due to high computational costs</p>
<p>Palmer et al. [<xref ref-type="bibr" rid="B114-sensors-19-03274">114</xref>] proposed a paw-tracking algorithm for mice when they grab food and can be used for gesture tracking as well. They developed the algorithm by treating it as a pose estimation problem. They model each digit as a combination of three phalanges (bones). Each bone is modeled by an ellipsoid. For 4 digits, there is a total of 12 ellipsoids. The palm is modeled by an additional ellipse. The forearm is also modeled as an ellipsoid while the nose is modeled as an elliptic paraboloid. The paw is modeled using 16 parameters for the digits (four degrees of freedom per digit), four constant vectors representing the metacarpal bones and 6 parameters for position and rotation of the palm of the paw. Furthermore, the forearm is assumed to be fixated at the wrist and can rotate along all three axes in space. This amounts to a total of 22 parameters. In each frame, these ellipsoids are projected in such a way that they best represent the edges. The best projection of ellipsoids is found by optimization and is considered a paw. They haven’t reported any quantitative results. This approach is very useful if the gesture tracking problem is treated as pose estimation with a temporal context. Since the approach treats gesture tracking as a pose-estimation problem, it opens the possibility of using state-of-the-art pose-estimation methods in gesture tracking. However, the computational cost is high for real-time deployment without graphical accelerators.</p>
<p>In [<xref ref-type="bibr" rid="B115-sensors-19-03274">115</xref>], Palmer et al. extended their work from [<xref ref-type="bibr" rid="B114-sensors-19-03274">114</xref>]. The basic idea is the same. It models the paw made of different parts. Four digits (fingers), each digit having 3 phalanges (bones). Each phalange is modeled by an ellipsoid, so there is a total of 12 ellipsoids for the phalanges plus an additional one for the palm. In this paper, the movement of the 13 ellipsoids is modeled by vectors with 19 degrees of freedom, unlike 22 from [<xref ref-type="bibr" rid="B114-sensors-19-03274">114</xref>]. The solution hypothesis is searched not simultaneously, but in stages to reduce the number of calculations. This is done by creating a different number of hypotheses for every joint of every digit and then finding the optimum hypotheses.</p>
<p>A Giovannucci et al. [<xref ref-type="bibr" rid="B116-sensors-19-03274">116</xref>] proposed an optical flow and cascade learners-based approach for tracking of head and limb movements in head-fixed mice walking/running on a spherical/cylindrical treadmill. Unlike other approaches, only one camera installed from a lateral field of view was used for limb tracking and one camera installed in front of the mouse was used for whisker tracking. They calculated dense optical flow fields in a frame-to-frame method for whisker tracking. The estimated optical flow fields were used to train dictionary learning algorithms for motion detection in whiskers. They annotated 4217 frames for limb detection and 1053 frames for tails detection and then used them to train Haar-Cascades classifiers for both the cases. They have reported a high correlation of <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mn>0.78</mml:mn><mml:mo>±</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for whiskers and <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mn>0.85</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for hind limb. The proposed hardware solution in the paper is low cost and easy to implement. The tracking approach is also computationally not demanding and can be run in real-time. They, however, did not deal with the micro-patterns in motion dynamics which can be best captured with the inclusion of temporal context to the tracking approach. Moreover, accurate estimation of flow fields either takes too much time or requires graphical processing units.</p>
<p>Mathis et al. [<xref ref-type="bibr" rid="B117-sensors-19-03274">117</xref>] introduced a user-defined body-parts tracking method based on deep learning called DeepLabCut. The body-part (which can be either limbs or tail or head) is built on top of the human pose estimation method based on deep learning called DeeperCut [<xref ref-type="bibr" rid="B118-sensors-19-03274">118</xref>]. The DeepLabCut employs the feature detectors of DeeperCut to build user-defined body part detectors in laboratory animals. The training procedure is standard, a user manually annotates limbs/tail/body parts in some of the video frames which are used to fine-tune the DepperCut feature detectors. Then another prediction layer predicts the pose of the animal by labeling the body parts in question. The authors have reported accuracy of <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mn>4.17</mml:mn><mml:mo>±</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels on test data. The reported architecture is remarkable since it is a general-purpose architecture and can be modified to track another body part relatively easily. Also, since the architecture is built upon existing state-of-the-art deep networks for pose estimation, it is easy to train and inherits all the strengths of parent deep networks. However, the reported pipeline can have a problem in case of occlusions.</p>
<p>In DeepBehavior, the authors have proposed an open-source behavioral analysis toolbox built on top of existing validated approaches [<xref ref-type="bibr" rid="B119-sensors-19-03274">119</xref>]. The toolbox contains routines for gesture tracking, 3D kinematics analysis for humans and rodents and behavioral analysis for rodents. The toolbox is built on top of three existing and validated convolutional neural networks architecture named Tensorbox [<xref ref-type="bibr" rid="B120-sensors-19-03274">120</xref>], YOLOv3 [<xref ref-type="bibr" rid="B121-sensors-19-03274">121</xref>], and Openpose [<xref ref-type="bibr" rid="B122-sensors-19-03274">122</xref>]. For 3D kinematics tracking, the toolbox needs a stereo system with a properly calibrated camera. They recommend to use Tensorbox if only one test subject needs to be tracked, YOLOv3 if multiple test subjects need to be tracked and Openpose if human subjects need to be tracked. They have initialized the networks by using models trained on ImageNet and fine tuned them with custom datasets. The authors have not provided paw tracking results for rodents. This toolbox is a good example of using gesture and pose tracking approaches developed and tested for humans to be used for rodents and small animals. Since the system is built upon existing state-of-the-art pose estimation architectures, it inherits their strengths and weaknesses. For instance, Openpose can have a hard time identifying a pose it has not seen. It also does not know how to tell two subjects apart, therefore, it can try to impose one pose upon two test animals in a situation in which one animal is partially occluded by the other. Also, it can face difficulties in estimating pose of animals at an angle.</p>
</sec>
</sec>
</sec>
<sec id="sec8-sensors-19-03274">
<title>8. Applications</title>
<p>Gesture tracking is finding applications not only in behavioral research but in other fields of research as well. For instance, researchers working in neurophysiology can benefit greatly from a dataset in which they can link brain patterns to a certain physical activity. They can see how the body responds to certain brain injuries or gene mutations. If reliable gesture tracking methods are available, the researchers can analyze the response of test animals to external stimuli, treatment regimes, and brain injuries. Moreover, researchers working on physiotherapies can benefit from gesture tracking. To sum it up, researchers working in one of the following fields may find some of the methods described in this survey useful for their purpose:<list list-type="order"><list-item><p>Research on behavioral phenotyping needs huge volumes of annotated data to understand and classify rodents’ and animals’ behaviors. By looking at the current state-of-the-art of gesture tracking/pose estimation methods, a researcher working on behavioral phenotyping can choose the gesture tracking/pose estimation method most suitable to their needs [<xref ref-type="bibr" rid="B6-sensors-19-03274">6</xref>].</p></list-item><list-item><p>Research on depression analysis in normal and transgenic mice/animals can also benefit from this survey because for reliable quantification of depression, the researcher needs to understand the mice/animals behavior and once they have an appropriate quantification of behavior in terms of pose, locomotion, and gait patterns, they can understand how this behavior changes in response to genetic mutations. Scientists can now breed genetically-altered mice called "transgenic mice" that carry genes that are similar to those that cause human diseases. Likewise, select genes can be turned off or made inactive, creating "knockout mice," which can be used to evaluate the effects of cancer-causing chemicals (carcinogens) and assess drug safety, according to the FBR [<xref ref-type="bibr" rid="B123-sensors-19-03274">123</xref>,<xref ref-type="bibr" rid="B124-sensors-19-03274">124</xref>,<xref ref-type="bibr" rid="B125-sensors-19-03274">125</xref>,<xref ref-type="bibr" rid="B126-sensors-19-03274">126</xref>,<xref ref-type="bibr" rid="B127-sensors-19-03274">127</xref>,<xref ref-type="bibr" rid="B128-sensors-19-03274">128</xref>,<xref ref-type="bibr" rid="B129-sensors-19-03274">129</xref>].</p></list-item><list-item><p>Researchers working on anxiety in rats/small animals can also benefit from the methods described this survey because with a suitable gesture tracking/pose estimation method, they can quantify rats/animals behavior efficiently and understand how it alters with anxiety [<xref ref-type="bibr" rid="B130-sensors-19-03274">130</xref>,<xref ref-type="bibr" rid="B131-sensors-19-03274">131</xref>,<xref ref-type="bibr" rid="B132-sensors-19-03274">132</xref>,<xref ref-type="bibr" rid="B133-sensors-19-03274">133</xref>].</p></list-item><list-item><p>Research on the effects of drugs and cancer on locomotion can also benefit from the methods described in this survey as gesture tracking/pose estimation can be used to understand the changes in locomotion patterns of rats/animals in response to tumors and drugs [<xref ref-type="bibr" rid="B134-sensors-19-03274">134</xref>,<xref ref-type="bibr" rid="B135-sensors-19-03274">135</xref>,<xref ref-type="bibr" rid="B136-sensors-19-03274">136</xref>,<xref ref-type="bibr" rid="B137-sensors-19-03274">137</xref>,<xref ref-type="bibr" rid="B138-sensors-19-03274">138</xref>,<xref ref-type="bibr" rid="B139-sensors-19-03274">139</xref>,<xref ref-type="bibr" rid="B140-sensors-19-03274">140</xref>,<xref ref-type="bibr" rid="B141-sensors-19-03274">141</xref>,<xref ref-type="bibr" rid="B142-sensors-19-03274">142</xref>].</p></list-item><list-item><p>Another field which can benefit from automated gesture analysis is the research on understanding how neural activity controls physical activity or how the brain responds to external stimuli [<xref ref-type="bibr" rid="B143-sensors-19-03274">143</xref>,<xref ref-type="bibr" rid="B144-sensors-19-03274">144</xref>,<xref ref-type="bibr" rid="B145-sensors-19-03274">145</xref>,<xref ref-type="bibr" rid="B146-sensors-19-03274">146</xref>,<xref ref-type="bibr" rid="B147-sensors-19-03274">147</xref>].</p></list-item><list-item><p>Research on neurophysiological and physical therapies can also benefit from the methods described in this survey. If a researcher can quantify the changes in gait patterns and pose over an extended time, he/she can track the success of therapy on recovery [<xref ref-type="bibr" rid="B148-sensors-19-03274">148</xref>,<xref ref-type="bibr" rid="B149-sensors-19-03274">149</xref>,<xref ref-type="bibr" rid="B150-sensors-19-03274">150</xref>,<xref ref-type="bibr" rid="B151-sensors-19-03274">151</xref>,<xref ref-type="bibr" rid="B152-sensors-19-03274">152</xref>].</p></list-item><list-item><p>Researchers working on systems biology can also benefit from the methods described in this survey as with a proper gesture/gait analysis method. They can provide useful numerical evidence to understand the behavior of biological systems under different physical and pathological conditions [<xref ref-type="bibr" rid="B153-sensors-19-03274">153</xref>].</p></list-item></list></p>
</sec>
<sec id="sec9-sensors-19-03274" sec-type="conclusions">
<title>9. Conclusions</title>
<p>In this paper, we provide a comprehensive survey of the main approaches for gesture tracking on small rodents, although we did not restrict the review to papers dealing with rodents but included related works that could be ported to the field. We included in <xref ref-type="app" rid="app1-sensors-19-03274">Appendix A</xref> a complete summary of the approaches selected in this survey, with a special focus on the main characteristics of each paper (code availability, performance measures, setup, and invasiveness).</p>
<p>Gesture detection and tracking approaches are still in the developing phase. There is no single approach strong enough which can track micro-movements of limbs, whiskers or snouts of the rodents which are necessary for gesture identification and behavioral phenotyping. In general, those approaches which use specialized hardware are more successful than those approaches which solely depend on standard video cameras. For example, the use of X-Ray imaging to detect surgically implanted markers has been proven very successful in tracking limb and joint movements with high precision. Moreover, the use of specific markers attached to either limbs or whiskers of the rodents also increases the overall tracking accuracy of an approach. However, there is a downside to this approach; the rodents might not behave naturally. Therefore, more and more research is being conducted on scalable, portable and noninvasive tracking methods that only need standard video cameras.</p>
<sec>
<title>Future Research</title>
<p>Based on the literature survey we conducted, we have the following recommendations for future research:<list list-type="order"><list-item><p>Methods would benefit from the effective use of different camera configurations to get spatial data at high resolution in 3D space. Until now, only low-resolution time-of-flight sensors [<xref ref-type="bibr" rid="B93-sensors-19-03274">93</xref>,<xref ref-type="bibr" rid="B97-sensors-19-03274">97</xref>]. Acquiring spatial data from high-resolution cameras will help understand the gait of rodents in 3D.</p></list-item><list-item><p>One of the most relevant shortcomings of the field is the lack of public databases to validate new algorithms. Different approaches are tested on the (usually private) data from the lab developing the solution. Building a standardized gesture tracking dataset which can be used as a benchmark would similarly benefit the community as large object recognition databases (PASCAL, ImageNet or MS COCO) allowed significant progress in the Computer Vision literature.</p></list-item><list-item><p>Currently, large amounts of non-labeled data samples are in existence (thousands of video hours). The use of unsupervised learning algorithms that could benefit the parameter learning of supervised methods is one of the most challenging future research lines. Since unsupervised and weakly supervised gesture tracking/pose estimation is being researched for other species, extending it to rodents/small animals will make the large volumes of unlabeled data useful [<xref ref-type="bibr" rid="B154-sensors-19-03274">154</xref>,<xref ref-type="bibr" rid="B155-sensors-19-03274">155</xref>,<xref ref-type="bibr" rid="B156-sensors-19-03274">156</xref>,<xref ref-type="bibr" rid="B157-sensors-19-03274">157</xref>,<xref ref-type="bibr" rid="B158-sensors-19-03274">158</xref>,<xref ref-type="bibr" rid="B159-sensors-19-03274">159</xref>].</p></list-item><list-item><p>Data augmentation using synthetic samples. Now, methods based on GANs are obtaining extraordinary results in Computer Vision. Using GAN networks can help generate large amounts of annotated training data. The annotated data can then be used to validate gesture tracking/pose estimation techniques for rodents/small animals and those techniques can be further fine-tuned by a small set of human-annotated data.</p></list-item><list-item><p>Combine hardware-based methods with markers to create large scale databases for further automated learning just from the image. Up until, physical markers and specialized hardware have been used only in specific settings. They can be used to generate large volumes of annotated data by careful data acquisition as the markers can be reliably tracked by specialized hardware.</p></list-item><list-item><p>Besides, the use of semi-supervised and weakly-supervised learning algorithms could benefit the community. The challenge in this particular case is to minimize the user intervention (supervision) maximizing the improvements on the accuracy.</p></list-item><list-item><p>Very few of the surveyed approaches in the software-based method section consider temporal coherence while developing a solution for gesture tracking/pose estimation of rodents and small animals. Since locomotion is temporally coherent, machine learning methods such as Long Short Term Memory networks can be efficiently trained to track the rodents’ pose by evaluating the specific pose history.</p></list-item><list-item><p>Finally, deep learning methods have been shown to outperform many computer vision tasks. For instance, deep learning-based methods for gesture tracking/pose estimation in humans. Exploring these validated approaches can increase the reliability of gesture tracking/pose estimation in rodents/small animals [<xref ref-type="bibr" rid="B160-sensors-19-03274">160</xref>,<xref ref-type="bibr" rid="B161-sensors-19-03274">161</xref>].</p></list-item></list></p>
</sec>
</sec>
</body>
<back>
<notes>
<title>Author Contributions</title>
<p>Conceptualization, W.A. and D.M.R.; methodology, W.A. and D.M.R.; software, W.A.; formal analysis, W.A. and D.M.R.; investigation, W.A. and D.M.R.; resources, W.A. and D.M.R.; data curation, W.A.; writing—original draft preparation, W.A. and D.M.R.; writing—review and editing, W.A. and D.M.R.; visualization, W.A.; supervision, D.M.R.; project administration, D.M.R.; funding acquisition, D.M.R.</p>
</notes>
<notes>
<title>Funding</title>
<p>This research was supported by TIN2015-66951-C2-2-R, RTI2018-095232-B-C22 grant from the Spanish Ministry of Science, Innovation and Universities (FEDER funds), and NVIDIA Hardware grant program.</p>
</notes>
<notes notes-type="COI-statement">
<title>Conflicts of Interest</title>
<p>The authors declare no conflict of interest.</p>
</notes>
<app-group>
<app id="app1-sensors-19-03274">
<title>Appendix A. Summary of Selected Approaches</title>
<p>We have summarized some important aspects of selected approaches in <xref ref-type="table" rid="sensors-19-03274-t0A1">Table A1</xref>. We use the following notation to properly interpret the table:</p>
<p>Code availability</p>
<p>It means whether the code is available or not. If it is available, is it free or paid.</p>
<p>Performance:</p>
<p>If the performance is given in terms of standard deviation, it signifies the consistency of proposed approach either against itself or an annotated dataset (which is pointed out). For example, if the table says that the proposed system can make a 90% accurate estimation of limbs velocity with an SD of 3%, it means that the system performance fluctuates somewhere between 87% to 93%. If absolute accuracy is given, it means each and every detected instant is compared to manually annotated samples. If only % SD is given or just SD is given, it means that the system can consistently reproduce the same result with specified amount of standard deviation, regardless of its performance against the ground truth.</p>
<p>Need specialized setup &amp; Invasiveness</p>
<p>This indicates whether the method needs any specialized hardware other than the housing setup or video cameras. If the housing setup itself is arranged in a specific way but it does not contain any specialized materials, we say that the hardware setup required is not specialized. By invasiveness, we mean that a surgery has to be conducted to implant the markers. If no surgery is needed to implant markers, we call it semi-invasive. If no markers are needed, we call it non-invasive.</p>
<table-wrap id="sensors-19-03274-t0A1" orientation="portrait" position="anchor">
<object-id pub-id-type="pii">sensors-19-03274-t0A1_Table A1</object-id>
<label>Table A1</label>
<caption>
<p>Comparison of different approaches. Legend:: Invasive: Approaches which requires surgery to put markers for tracking, semi-invasive: Approaches which do not need surgery for marker insertion, non-invasive: no marker needed. Real time means that the system can process frames at the same rate they are being acquired. If it needs specialized equipment apart from standard video cameras and housing setup, it is pointed out in the last column.</p>
</caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">
</th>
<th align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">
<bold>Type</bold>
</th>
<th align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">
<bold>Code Availability</bold>
</th>
<th align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">
<bold>Performance</bold>
</th>
<th align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">
<bold>Real Time or Offline</bold>
</th>
<th align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"><bold>Need Specialized Setup</bold> &amp; <bold>Invasiveness</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B59-sensors-19-03274">59</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Commercial</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Paid</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Comparison with ground truth not provided. One paper reports the reproducibility: 2.65% max SD</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Yes</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Yes</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B71-sensors-19-03274">71</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Commercial</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Paid</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Comparison with ground truth not provided. One paper reports the reproducibility: 1.57% max SD</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Yes</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Yes</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B49-sensors-19-03274">49</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">data and code for demo available at <uri xlink:href="http://bit.do/eTTai">http://bit.do/eTTai</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">tracking performance not reported, behavioral classification of 12 traits reported to be max at 71%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Tracking real time, classification offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B51-sensors-19-03274">51</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">tracking: SD of only 0.034% when compared with ground truth, Max SD of 1.71 degrees in estimating joint angle</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time legs and joints tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B52-sensors-19-03274">52</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">tracking performance not reported explicitly</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time whisker tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B53-sensors-19-03274">53</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">available on request</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">whisker tracking performance not reported explicitly</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time single whisker tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B54-sensors-19-03274">54</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">head motion tracked correctly with a max false positive of 13%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time head and snout tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B55-sensors-19-03274">55</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">head motion tracked continuously with a reported SD of only 0.5 mm</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time head and snout tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B57-sensors-19-03274">57</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">head motion tracked with an accuracy of 96.3% and the tracking can be reproduced over multiple studies with a correlation coefficient of 0.78</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time head tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B89-sensors-19-03274">89</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code and demo data available at <uri xlink:href="https://goo.gl/vYaYPy">https://goo.gl/vYaYPy</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">they reported a correlation between whisking amplitude and velocity as a measure of reliability, R = 0.89</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Offline head and whisker tracking</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B90-sensors-19-03274">90</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Tracking and gait prediction with confidence of 95%, deviation between human annotator and computer at 8%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B93-sensors-19-03274">93</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Paw tracked with an accuracy of 88.5 on transparent floor and 83.2% on opaque floor</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B95-sensors-19-03274">95</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="https://goo.gl/58DQij">https://goo.gl/58DQij</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">tail and paws tracked with an accuracy &gt;90%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Real time</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B97-sensors-19-03274">97</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">5 class behavioral classification problem, accuracy in bright condition is 95.34 and in dark conditions is 89.4%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B100-sensors-19-03274">100</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">6 behavioral class accuracy: 66.9%, 4 behavioral class accuracy: 76.3%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B98-sensors-19-03274">98</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="https://goo.gl/eY2Yza">https://goo.gl/eY2Yza</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">whisker detection rate: 76.9%, peak spatial error in whisker detection: 10 pixels</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B101-sensors-19-03274">101</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Peak deviation between human annotator and automated annotation: 0.5 mm with a camera of 6 pixel/mm resolution</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B92-sensors-19-03274">92</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Tracking accuracy &gt;90% after the algorithm was assisted by human users in 3–5% of the frames</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B105-sensors-19-03274">105</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="https://goo.gl/Gny89o">https://goo.gl/Gny89o</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">A max deviation of 17.7% between human and automated whisker annotation</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">yes, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B107-sensors-19-03274">107</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Maximum paw detection error: 5.9%, minimum error : 0.4%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B110-sensors-19-03274">110</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Source code at <uri xlink:href="https://goo.gl/zesyez">https://goo.gl/zesyez</uri>, demo data at <uri xlink:href="https://goo.gl/dn2L3y">https://goo.gl/dn2L3y</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Behavioral classification: 1% false positive rate</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, semi-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B108-sensors-19-03274">108</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Source code available at <uri xlink:href="https://goo.gl/JCv3AV">https://goo.gl/JCv3AV</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Whisker tracing accuracy: max error of 0.45 pixels</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B116-sensors-19-03274">116</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">not available</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Correlation with annotated data; for whiskers r = 0.78, for limbs r = 0.85</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B109-sensors-19-03274">109</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="https://goo.gl/V54mpL">https://goo.gl/V54mpL</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Velocity calculated by AGATHA was off from manually calculated velocity by 1.5%</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B117-sensors-19-03274">117</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="http://bit.ly/2vgJUbr">http://bit.ly/2vgJUbr</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">Detected pose matched ground truth with an accuracy of <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:mn>4.17</mml:mn><mml:mo>±</mml:mo><mml:mn>0.32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">real time on GPUs</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
<tr>
<td align="center" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">[<xref ref-type="bibr" rid="B119-sensors-19-03274">119</xref>]</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle"> Research</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">code available at <uri xlink:href="https://bit.ly/2XuJmPv">https://bit.ly/2XuJmPv</uri></td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">No performance metric reported</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">offline</td>
<td align="left" colspan="1" rowspan="1" style="border-bottom:solid thin" valign="middle">no, non-invasive</td>
</tr>
</tbody>
</table>
</table-wrap>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="B1-sensors-19-03274">
<label>1.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Deori</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Thounaojam</surname>
<given-names>D.M.</given-names>
</name>
</person-group>
<article-title>A Survey on Moving Object Tracking in Video</article-title>
<source/>Int. J. Inf. Theory
          <year>2014</year>
<volume>3</volume>
<fpage>31</fpage>
<lpage>46</lpage>
<pub-id pub-id-type="doi">10.5121/ijit.2014.3304</pub-id>
</element-citation>
</ref>
<ref id="B2-sensors-19-03274">
<label>2.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yilmaz</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Javed</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Shah</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Object tracking: A survey</article-title>
<source/>ACM Comput. Surv. (CSUR)
          <year>2006</year>
<volume>38</volume>
<fpage>13</fpage>
<pub-id pub-id-type="doi">10.1145/1177352.1177355</pub-id>
</element-citation>
</ref>
<ref id="B3-sensors-19-03274">
<label>3.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wei</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Learning Spatio-Temporal Information for Multi-Object Tracking</article-title>
<source/>IEEE Access
          <year>2017</year>
<volume>5</volume>
<fpage>3869</fpage>
<lpage>3877</lpage>
<pub-id pub-id-type="doi">10.1109/ACCESS.2017.2686482</pub-id>
</element-citation>
</ref>
<ref id="B4-sensors-19-03274">
<label>4.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Kratz</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Nishino</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Tracking with local spatio-temporal motion patterns in extremely crowded scenes</article-title>
<source/>Proceedings of the2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
          <conf-loc>San Francisco, CA, USA</conf-loc>
<conf-date>13–18 June 2010</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2010</year>
</element-citation>
</ref>
<ref id="B5-sensors-19-03274">
<label>5.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Teng</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Xing</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Lang</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Feng</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Jin</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Robust object tracking based on temporal and spatial deep networks</article-title>
<source/>Proceedings of the IEEE International Conference on Computer Vision
          <conf-loc>Venice, Italy</conf-loc>
<conf-date>22–29 October 2017</conf-date>
</element-citation>
</ref>
<ref id="B6-sensors-19-03274">
<label>6.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sousa</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Almeida</surname>
<given-names>O.F.X.</given-names>
</name>
<name>
<surname>Wotjak</surname>
<given-names>C.T.</given-names>
</name>
</person-group>
<article-title>A hitchhiker’s guide to behavioral analysis in laboratory rodents</article-title>
<source/>Genes Brain Behav.
          <year>2006</year>
<volume>5</volume>
<fpage>5</fpage>
<lpage>24</lpage>
<pub-id pub-id-type="doi">10.1111/j.1601-183X.2006.00228.x</pub-id>
<pub-id pub-id-type="pmid">16681797</pub-id>
</element-citation>
</ref>
<ref id="B7-sensors-19-03274">
<label>7.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crawley</surname>
<given-names>J.N.</given-names>
</name>
</person-group>
<article-title>Behavioral phenotyping of rodents</article-title>
<source/>Comp. Med.
          <year>2003</year>
<volume>53</volume>
<fpage>140</fpage>
<lpage>146</lpage>
<pub-id pub-id-type="pmid">12784847</pub-id>
</element-citation>
</ref>
<ref id="B8-sensors-19-03274">
<label>8.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crawley</surname>
<given-names>J.N.</given-names>
</name>
</person-group>
<article-title>Behavioral phenotyping of transgenic and knockout mice: Experimental design and evaluation of general health, sensory functions, motor abilities, and specific behavioral tests</article-title>
<source/>Brain Res.
          <year>1999</year>
<volume>835</volume>
<fpage>18</fpage>
<lpage>26</lpage>
<pub-id pub-id-type="doi">10.1016/S0006-8993(98)01258-X</pub-id>
<pub-id pub-id-type="pmid">10448192</pub-id>
</element-citation>
</ref>
<ref id="B9-sensors-19-03274">
<label>9.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Crawley</surname>
<given-names>J.N.</given-names>
</name>
</person-group>
<source/>What’s Wrong with My Mouse? Behavioral Phenotyping of Transgenic and Knockout Mice
          <publisher-name>John Wiley &amp; Sons</publisher-name>
<publisher-loc>Hoboken, NJ, USA</publisher-loc>
<year>2007</year>
</element-citation>
</ref>
<ref id="B10-sensors-19-03274">
<label>10.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Tekalp</surname>
<given-names>A.M.</given-names>
</name>
<name>
<surname>Tekalp</surname>
<given-names>A.M.</given-names>
</name>
</person-group>
<source/>Digital Video Processing
          <publisher-name>Prentice Hall PTR</publisher-name>
<publisher-loc>Upper Saddle River, NJ, USA</publisher-loc>
<year>1995</year>
<volume>Volume 1</volume>
</element-citation>
</ref>
<ref id="B11-sensors-19-03274">
<label>11.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Bovik</surname>
<given-names>A.C.</given-names>
</name>
</person-group>
<source/>Handbook of Image and Video Processing
          <publisher-name>Academic Press</publisher-name>
<publisher-loc>Cambridge, MA, USA</publisher-loc>
<year>2010</year>
</element-citation>
</ref>
<ref id="B12-sensors-19-03274">
<label>12.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Borst</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Egelhaaf</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Principles of visual motion detection</article-title>
<source/>Trends Neurosci.
          <year>1989</year>
<volume>12</volume>
<fpage>297</fpage>
<lpage>306</lpage>
<pub-id pub-id-type="doi">10.1016/0166-2236(89)90010-6</pub-id>
<pub-id pub-id-type="pmid">2475948</pub-id>
</element-citation>
</ref>
<ref id="B13-sensors-19-03274">
<label>13.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hu</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Tan</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Maybank</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>A survey on visual surveillance of object motion and behaviors</article-title>
<source/>IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.)
          <year>2004</year>
<volume>34</volume>
<fpage>334</fpage>
<lpage>352</lpage>
<pub-id pub-id-type="doi">10.1109/TSMCC.2004.829274</pub-id>
</element-citation>
</ref>
<ref id="B14-sensors-19-03274">
<label>14.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Welch</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Foxlin</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Motion tracking survey</article-title>
<source/>IEEE Comput. Graph. Appl.
          <year>2002</year>
<volume>22</volume>
<fpage>24</fpage>
<lpage>38</lpage>
<pub-id pub-id-type="doi">10.1109/MCG.2002.1046626</pub-id>
</element-citation>
</ref>
<ref id="B15-sensors-19-03274">
<label>15.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>De-gui</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Sheng-sheng</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Jing-li</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<article-title>Motion tracking with fast adaptive background subtraction</article-title>
<source/>Wuhan Univ. J. Nat. Sci. A
          <year>2003</year>
<volume>8</volume>
<fpage>35</fpage>
<lpage>40</lpage>
</element-citation>
</ref>
<ref id="B16-sensors-19-03274">
<label>16.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Ding</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Object tracking and detecting based on adaptive background subtraction</article-title>
<source/>Procedia Eng.
          <year>2012</year>
<volume>29</volume>
<fpage>1351</fpage>
<lpage>1355</lpage>
<pub-id pub-id-type="doi">10.1016/j.proeng.2012.01.139</pub-id>
</element-citation>
</ref>
<ref id="B17-sensors-19-03274">
<label>17.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Saravanakumar</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Vadivel</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Ahmed</surname>
<given-names>C.G.S.</given-names>
</name>
</person-group>
<article-title>Multiple human object tracking using background subtraction and shadow removal techniques</article-title>
<source/>Proceedings of the 2010 International Conference on Signal and Image Processing (ICSIP)
          <conf-loc>Chennai, India</conf-loc>
<conf-date>15–17 December 2010</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2010</year>
</element-citation>
</ref>
<ref id="B18-sensors-19-03274">
<label>18.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Awan</surname>
<given-names>T.W.</given-names>
</name>
<name>
<surname>Soh</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Background subtraction-based multiple object tracking using particle filter</article-title>
<source/>Proceedings of the 2014 International Conference on Systems, Signals and Image Processing (IWSSIP)
          <conf-loc>Dubrovnik, Croatia</conf-loc>
<conf-date>12–15 May 2014</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2014</year>
</element-citation>
</ref>
<ref id="B19-sensors-19-03274">
<label>19.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Liang</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Motion human detection based on background subtraction</article-title>
<source/>Proceedings of the 2010 Second International Workshop on Education Technology and Computer Science (ETCS)
          <conf-loc>Wuhan, China</conf-loc>
<conf-date>6–7 March 2010</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2010</year>
<volume>Volume 1</volume>
</element-citation>
</ref>
<ref id="B20-sensors-19-03274">
<label>20.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Shuigen</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Zhen</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Hua</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Motion detection based on temporal difference method and optical flow field</article-title>
<source/>Proceedings of the Second International Symposium on Electronic Commerce and Security (ISECS’09)
          <conf-loc>Nanchang, China</conf-loc>
<conf-date>22–24 May 2009</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2009</year>
<volume>Volume 2</volume>
</element-citation>
</ref>
<ref id="B21-sensors-19-03274">
<label>21.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Singla</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Motion detection based on frame difference method</article-title>
<source/>Int. J. Inf. Comput. Technol.
          <year>2014</year>
<volume>4</volume>
<fpage>1559</fpage>
<lpage>1565</lpage>
</element-citation>
</ref>
<ref id="B22-sensors-19-03274">
<label>22.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lu</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>Q.H.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>An Improved Motion Detection Method for realtime Surveillance</article-title>
<source/>IAENG Int. J. Comput. Sci.
          <year>2008</year>
<volume>35</volume>
<pub-id pub-id-type="doi">10.1109/ICBNMT.2011.6155946</pub-id>
</element-citation>
</ref>
<ref id="B23-sensors-19-03274">
<label>23.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Jing</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Siong</surname>
<given-names>C.E.</given-names>
</name>
<name>
<surname>Rajan</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Foreground motion detection by difference-based spatial temporal entropy image</article-title>
<source/>Proceedings of the 2004 IEEE Region 10 Conference TENCON 2004
          <conf-loc>Chiang Mai, Thailand</conf-loc>
<conf-date>24 November 2004</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2004</year>
</element-citation>
</ref>
<ref id="B24-sensors-19-03274">
<label>24.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Shaikh</surname>
<given-names>S.H.</given-names>
</name>
<name>
<surname>Saeed</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Chaki</surname>
<given-names>N.</given-names>
</name>
</person-group>
<source/>Moving Object Detection Approaches, Challenges and Object Tracking. Moving Object Detection Using Background Subtraction
          <publisher-name>Springer</publisher-name>
<publisher-loc>Cham, Switzerland</publisher-loc>
<year>2014</year>
<fpage>5</fpage>
<lpage>14</lpage>
</element-citation>
</ref>
<ref id="B25-sensors-19-03274">
<label>25.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Denzler</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Schless</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Paulus</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Niemann</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Statistical approach to classification of flow patterns for motion detection</article-title>
<source/>Proceedings of the International Conference on Image Processing 1996
          <conf-loc>Lausanne, Switzerland</conf-loc>
<conf-date>19 September 1996</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>1996</year>
<volume>Volume 1</volume>
</element-citation>
</ref>
<ref id="B26-sensors-19-03274">
<label>26.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Paragios</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Tziritas</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Adaptive detection and localization of moving objects in image sequences</article-title>
<source/>Signal Process. Image Commun.
          <year>1999</year>
<volume>14</volume>
<fpage>277</fpage>
<lpage>296</lpage>
<pub-id pub-id-type="doi">10.1016/S0923-5965(98)00011-3</pub-id>
</element-citation>
</ref>
<ref id="B27-sensors-19-03274">
<label>27.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hu</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Xiao</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Fu</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Xie</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Tan</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Maybank</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>A system for learning statistical motion patterns</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>2006</year>
<volume>28</volume>
<fpage>1450</fpage>
<lpage>1464</lpage>
<pub-id pub-id-type="pmid">16929731</pub-id>
</element-citation>
</ref>
<ref id="B28-sensors-19-03274">
<label>28.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>El Abed</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Dubuisson</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Béréziat</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Comparison of statistical and shape-based approaches for non-rigid motion tracking with missing data using a particle filter</article-title>
<source/>International Conference on Advanced Concepts for Intelligent Vision Systems
          <publisher-name>Springer</publisher-name>
<publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
<year>2006</year>
</element-citation>
</ref>
<ref id="B29-sensors-19-03274">
<label>29.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chellappa</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Sankaranarayanan</surname>
<given-names>A.C.</given-names>
</name>
<name>
<surname>Veeraraghavan</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Turaga</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Statistical methods and models for video-based tracking, modeling, and recognition</article-title>
<source/>Found. Trends Signal Process.
          <year>2010</year>
<volume>3</volume>
<fpage>1</fpage>
<lpage>151</lpage>
<pub-id pub-id-type="doi">10.1561/2000000007</pub-id>
</element-citation>
</ref>
<ref id="B30-sensors-19-03274">
<label>30.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Paragios</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Deriche</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Geodesic active contours and level sets for the detection and tracking of moving objects</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>2000</year>
<volume>22</volume>
<fpage>266</fpage>
<lpage>280</lpage>
<pub-id pub-id-type="doi">10.1109/34.841758</pub-id>
</element-citation>
</ref>
<ref id="B31-sensors-19-03274">
<label>31.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pless</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Brodsky</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Aloimonos</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Detecting independent motion: The statistics of temporal continuity</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>2000</year>
<volume>22</volume>
<fpage>768</fpage>
<lpage>773</lpage>
<pub-id pub-id-type="doi">10.1109/34.868679</pub-id>
</element-citation>
</ref>
<ref id="B32-sensors-19-03274">
<label>32.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Isard</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Visual Motion Analysis by Probabilistic Propagation of Conditional Density</article-title>
<source/>Ph.D. Thesis
          <publisher-name>University of Oxford</publisher-name>
<publisher-loc>Oxford, UK</publisher-loc>
<year>1998</year>
</element-citation>
</ref>
<ref id="B33-sensors-19-03274">
<label>33.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Comaniciu</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Meer</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Mean shift: A robust approach toward feature space analysis</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>2002</year>
<volume>24</volume>
<fpage>603</fpage>
<lpage>619</lpage>
<pub-id pub-id-type="doi">10.1109/34.1000236</pub-id>
</element-citation>
</ref>
<ref id="B34-sensors-19-03274">
<label>34.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Lucas</surname>
<given-names>B.D.</given-names>
</name>
<name>
<surname>Kanade</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>An iterative image registration technique with an application to stereo vision</article-title>
<source/>Proceedings of the 7th International Joint Conference on Artificial Intelligence
          <conf-loc>Vancouver, BC, Canada</conf-loc>
<conf-date>24–28 August 1981</conf-date>
<fpage>674</fpage>
<lpage>679</lpage>
</element-citation>
</ref>
<ref id="B35-sensors-19-03274">
<label>35.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Horn</surname>
<given-names>P.B.K.</given-names>
</name>
<name>
<surname>Schunck</surname>
<given-names>B.G.</given-names>
</name>
</person-group>
<article-title>Determining optical flow</article-title>
<source/>Artif. Intell.
          <year>1981</year>
<volume>17</volume>
<fpage>185</fpage>
<lpage>203</lpage>
<pub-id pub-id-type="doi">10.1016/0004-3702(81)90024-2</pub-id>
</element-citation>
</ref>
<ref id="B36-sensors-19-03274">
<label>36.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>DWixson</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Detecting salient motion by accumulating directionally—Consistent flow</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <year>2000</year>
<volume>22</volume>
<fpage>774</fpage>
<lpage>780</lpage>
<pub-id pub-id-type="doi">10.1109/34.868680</pub-id>
</element-citation>
</ref>
<ref id="B37-sensors-19-03274">
<label>37.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shafie</surname>
<given-names>A.A.</given-names>
</name>
<name>
<surname>Hafiz</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Ali</surname>
<given-names>M.H.</given-names>
</name>
</person-group>
<article-title>Motion detection techniques using optical flow</article-title>
<source/>World Acad. Sci. Eng. Technol.
          <year>2009</year>
<volume>56</volume>
<fpage>559</fpage>
<lpage>561</lpage>
</element-citation>
</ref>
<ref id="B38-sensors-19-03274">
<label>38.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aslani</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Mahdavi-Nasab</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Optical flow based moving object detection and tracking for traffic surveillance</article-title>
<source/>Int. J. Electr. Comput. Energ. Electron. Commun. Eng.
          <year>2013</year>
<volume>7</volume>
<fpage>1252</fpage>
<lpage>1256</lpage>
</element-citation>
</ref>
<ref id="B39-sensors-19-03274">
<label>39.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barron</surname>
<given-names>J.L.</given-names>
</name>
<name>
<surname>Fleet</surname>
<given-names>D.J.</given-names>
</name>
<name>
<surname>Beauchemin</surname>
<given-names>S.S.</given-names>
</name>
</person-group>
<article-title>Performance of optical flow techniques</article-title>
<source/>Int. J. Comput. Vis.
          <year>1994</year>
<volume>12</volume>
<fpage>43</fpage>
<lpage>77</lpage>
<pub-id pub-id-type="doi">10.1007/BF01420984</pub-id>
</element-citation>
</ref>
<ref id="B40-sensors-19-03274">
<label>40.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sun</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Kuang</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Sheng</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Ouyang</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>W.</given-names>
</name>
</person-group>
<article-title>Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</article-title>
<source/>arXiv preprint
          <year>2017</year>
<pub-id pub-id-type="arxiv">1711.11152</pub-id>
</element-citation>
</ref>
<ref id="B41-sensors-19-03274">
<label>41.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Lu</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Deep visual tracking: Review and experimental comparison</article-title>
<source/>Pattern Recognit.
          <year>2018</year>
<volume>76</volume>
<fpage>323</fpage>
<lpage>338</lpage>
<pub-id pub-id-type="doi">10.1016/j.patcog.2017.11.007</pub-id>
</element-citation>
</ref>
<ref id="B42-sensors-19-03274">
<label>42.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Yeung</surname>
<given-names>D.Y.</given-names>
</name>
</person-group>
<article-title>Learning a deep compact image representation for visual tracking</article-title>
<source/>Proceedings of the Advances in Neural Information Processing System
          <conf-loc>Lake Tahoe, NV, USA</conf-loc>
<conf-date>5–10 December 2013</conf-date>
</element-citation>
</ref>
<ref id="B43-sensors-19-03274">
<label>43.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Feng</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Mei</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Hu</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>A Review of Visual Tracking with Deep Learning</article-title>
<source/>Adv. Intell. Syst. Res.
          <year>2016</year>
<volume>133</volume>
<fpage>231</fpage>
<lpage>234</lpage>
<pub-id pub-id-type="doi">10.2991/aiie-16.2016.54</pub-id>
</element-citation>
</ref>
<ref id="B44-sensors-19-03274">
<label>44.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Irschick</surname>
<given-names>D.J.</given-names>
</name>
<name>
<surname>Jayne</surname>
<given-names>B.C.</given-names>
</name>
</person-group>
<article-title>Comparative three-dimensional kinematics of the hindlimb for high-speed bipedal and quadrupedal locomotion of lizards</article-title>
<source/>J. Exp. Biol.
          <year>1999</year>
<volume>202</volume>
<fpage>1047</fpage>
<lpage>1065</lpage>
<pub-id pub-id-type="pmid">10101105</pub-id>
</element-citation>
</ref>
<ref id="B45-sensors-19-03274">
<label>45.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alexander</surname>
<given-names>R.M.</given-names>
</name>
</person-group>
<article-title>The gaits of bipedal and quadrupedal animals</article-title>
<source/>Int. J. Robot. Res.
          <year>1984</year>
<volume>3</volume>
<fpage>49</fpage>
<lpage>59</lpage>
<pub-id pub-id-type="doi">10.1177/027836498400300205</pub-id>
</element-citation>
</ref>
<ref id="B46-sensors-19-03274">
<label>46.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berillon</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Daver</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>D’août</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Nicolas</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>de La Villetanet</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Multon</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Digrandi</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Dubreuil</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Bipedal versus quadrupedal hind limb and foot kinematics in a captive sample of Papio anubis: Setup and preliminary results</article-title>
<source/>Int. J. Primatol.
          <year>2010</year>
<volume>31</volume>
<fpage>159</fpage>
<lpage>180</lpage>
<pub-id pub-id-type="doi">10.1007/s10764-010-9398-2</pub-id>
</element-citation>
</ref>
<ref id="B47-sensors-19-03274">
<label>47.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Ao</surname>
<given-names>L.J.</given-names>
</name>
<name>
<surname>Lu</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Leong</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>X.H.</given-names>
</name>
<name>
<surname>Zhu</surname>
<given-names>X.L.</given-names>
</name>
<name>
<surname>Sun</surname>
<given-names>T.F.D.</given-names>
</name>
<name>
<surname>Fei</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Jiu</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Quantitative gait analysis of long-term locomotion deficits in classical unilateral striatal intracerebral hemorrhage rat model</article-title>
<source/>Behav. Brain Res.
          <year>2013</year>
<volume>257</volume>
<fpage>166</fpage>
<lpage>177</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2013.10.007</pub-id>
<pub-id pub-id-type="pmid">24126041</pub-id>
</element-citation>
</ref>
<ref id="B48-sensors-19-03274">
<label>48.</label>
<element-citation publication-type="web">
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://nc3rs.org.uk/crackit/locowhisk-quantifying-rodent-locomotion-behaviours">https://nc3rs.org.uk/crackit/locowhisk-quantifying-rodent-locomotion-behaviours</ext-link></comment>
<date-in-citation content-type="access-date" iso-8601-date="2019-07-24">(accessed on 24 July 2019)</date-in-citation>
</element-citation>
</ref>
<ref id="B49-sensors-19-03274">
<label>49.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kain</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Stokes</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Gaudry</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Song</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Foley</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wilson</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>De Bivort</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Leg-tracking and automated behavioural classification in Drosophila</article-title>
<source/>Nat. Commun.
          <year>2013</year>
<volume>4</volume>
<fpage>1910</fpage>
<pub-id pub-id-type="doi">10.1038/ncomms2908</pub-id>
<pub-id pub-id-type="pmid">23715269</pub-id>
</element-citation>
</ref>
<ref id="B50-sensors-19-03274">
<label>50.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roy</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Bryant</surname>
<given-names>J.L.</given-names>
</name>
<name>
<surname>Cao</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Heck</surname>
<given-names>D.H.</given-names>
</name>
</person-group>
<article-title>High-precision, three-dimensional tracking of mouse whisker movements with optical motion capture technology</article-title>
<source/>Front. Behav. Neurosci.
          <year>2011</year>
<volume>5</volume>
<fpage>27</fpage>
<pub-id pub-id-type="doi">10.3389/fnbeh.2011.00027</pub-id>
<pub-id pub-id-type="pmid">21713124</pub-id>
</element-citation>
</ref>
<ref id="B51-sensors-19-03274">
<label>51.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tashman</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Anderst</surname>
<given-names>W.</given-names>
</name>
</person-group>
<article-title>In-vivo measurement of dynamic joint motion using high speed biplane radiography and CT: Application to canine ACL deficiency</article-title>
<source/>J. Biomech. Eng.
          <year>2003</year>
<volume>125</volume>
<fpage>238</fpage>
<lpage>245</lpage>
<pub-id pub-id-type="doi">10.1115/1.1559896</pub-id>
<pub-id pub-id-type="pmid">12751286</pub-id>
</element-citation>
</ref>
<ref id="B52-sensors-19-03274">
<label>52.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Harvey</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Roberto Bermejo</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Philip Zeigler</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Discriminative whisking in the head-fixed rat: Optoelectronic monitoring during tactile detection and discrimination tasks</article-title>
<source/>Somatosens. Mot. Res.
          <year>2001</year>
<volume>18</volume>
<fpage>211</fpage>
<lpage>222</lpage>
<pub-id pub-id-type="pmid">11562084</pub-id>
</element-citation>
</ref>
<ref id="B53-sensors-19-03274">
<label>53.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bermejo</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Houben</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Zeigler</surname>
<given-names>H.P.</given-names>
</name>
</person-group>
<article-title>Optoelectronic monitoring of individual whisker movements in rats</article-title>
<source/>J. Neurosci. Methods
          <year>1998</year>
<volume>83</volume>
<fpage>89</fpage>
<lpage>96</lpage>
<pub-id pub-id-type="doi">10.1016/S0165-0270(98)00050-8</pub-id>
<pub-id pub-id-type="pmid">9765121</pub-id>
</element-citation>
</ref>
<ref id="B54-sensors-19-03274">
<label>54.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kyme</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Meikle</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Baldock</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fulton</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Tracking and characterizing the head motion of unanaesthetized rats in positron emission tomography</article-title>
<source/>J. R. Soc. Interface
          <year>2012</year>
<volume>9</volume>
<fpage>3094</fpage>
<lpage>3107</lpage>
<pub-id pub-id-type="doi">10.1098/rsif.2012.0334</pub-id>
<pub-id pub-id-type="pmid">22718992</pub-id>
</element-citation>
</ref>
<ref id="B55-sensors-19-03274">
<label>55.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kyme</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Meikle</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Fulton</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Realtime 3D motion tracking for small animal brain PET</article-title>
<source/>Phys. Med. Biol.
          <year>2008</year>
<volume>53</volume>
<fpage>2651</fpage>
<lpage>2666</lpage>
<pub-id pub-id-type="doi">10.1088/0031-9155/53/10/014</pub-id>
<pub-id pub-id-type="pmid">18443388</pub-id>
</element-citation>
</ref>
<ref id="B56-sensors-19-03274">
<label>56.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kyme</surname>
<given-names>A.Z.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>V.W.</given-names>
</name>
<name>
<surname>Meikle</surname>
<given-names>S.R.</given-names>
</name>
<name>
<surname>Baldock</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fulton</surname>
<given-names>R.R.</given-names>
</name>
</person-group>
<article-title>Optimised motion tracking for positron emission tomography studies of brain function in awake rats</article-title>
<source/>PLoS ONE
          <year>2011</year>
<volume>6</volume>
<elocation-id>E21727</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0021727</pub-id>
<pub-id pub-id-type="pmid">21747951</pub-id>
</element-citation>
</ref>
<ref id="B57-sensors-19-03274">
<label>57.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pasquet</surname>
<given-names>M.O.</given-names>
</name>
<name>
<surname>Tihy</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Gourgeon</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Pompili</surname>
<given-names>M.N.</given-names>
</name>
<name>
<surname>Godsil</surname>
<given-names>B.P.</given-names>
</name>
<name>
<surname>Léna</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Dugué</surname>
<given-names>G.P.</given-names>
</name>
</person-group>
<article-title>Wireless inertial measurement of head kinematics in freely-moving rats</article-title>
<source/>Sci. Rep.
          <year>2016</year>
<volume>6</volume>
<fpage>35689</fpage>
<pub-id pub-id-type="doi">10.1038/srep35689</pub-id>
<pub-id pub-id-type="pmid">27767085</pub-id>
</element-citation>
</ref>
<ref id="B58-sensors-19-03274">
<label>58.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hamers</surname>
<given-names>F.P.</given-names>
</name>
<name>
<surname>Lankhorst</surname>
<given-names>A.J.</given-names>
</name>
<name>
<surname>van Laar</surname>
<given-names>T.J.</given-names>
</name>
<name>
<surname>Veldhuis</surname>
<given-names>W.B.</given-names>
</name>
<name>
<surname>Gispen</surname>
<given-names>W.H.</given-names>
</name>
</person-group>
<article-title>Automated quantitative gait analysis during overground locomotion in the rat: Its application to spinal cord contusion and transection injuries</article-title>
<source/>J. Neurotrauma
          <year>2001</year>
<volume>18</volume>
<fpage>187</fpage>
<lpage>201</lpage>
<pub-id pub-id-type="doi">10.1089/08977150150502613</pub-id>
<pub-id pub-id-type="pmid">11229711</pub-id>
</element-citation>
</ref>
<ref id="B59-sensors-19-03274">
<label>59.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dorman</surname>
<given-names>C.W.</given-names>
</name>
<name>
<surname>Krug</surname>
<given-names>H.E.</given-names>
</name>
<name>
<surname>Frizelle</surname>
<given-names>S.P.</given-names>
</name>
<name>
<surname>Funkenbusch</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Mahowald</surname>
<given-names>M.L.</given-names>
</name>
</person-group>
<article-title>A comparison of <italic>D</italic><italic>i</italic><italic>g</italic><italic>i</italic><italic>G</italic><italic>a</italic><italic>i</italic><italic>t</italic><sup>TM</sup> and <italic>T</italic><italic>r</italic><italic>e</italic><italic>a</italic><italic>d</italic><italic>S</italic><italic>c</italic><italic>a</italic><italic>n</italic><sup>TM</sup> imaging systems: Assessment of pain using gait analysis in murine monoarthritis</article-title>
<source/>J. Pain Res.
          <year>2014</year>
<volume>7</volume>
<fpage>25</fpage>
<pub-id pub-id-type="pmid">24516338</pub-id>
</element-citation>
</ref>
<ref id="B60-sensors-19-03274">
<label>60.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xiao</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Vemula</surname>
<given-names>S.R.</given-names>
</name>
<name>
<surname>Xue</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Khan</surname>
<given-names>M.M.</given-names>
</name>
<name>
<surname>Kuruvilla</surname>
<given-names>K.P.</given-names>
</name>
<name>
<surname>Marquez-Lona</surname>
<given-names>E.M.</given-names>
</name>
<name>
<surname>Cobb</surname>
<given-names>M.R.</given-names>
</name>
<name>
<surname>LeDoux</surname>
<given-names>M.S.</given-names>
</name>
</person-group>
<article-title>Motor phenotypes and molecular networks associated with germline deficiency of Ciz1</article-title>
<source/>Exp. Neurol.
          <year>2016</year>
<volume>283</volume>
<fpage>110</fpage>
<lpage>120</lpage>
<pub-id pub-id-type="doi">10.1016/j.expneurol.2016.05.006</pub-id>
<pub-id pub-id-type="pmid">27163549</pub-id>
</element-citation>
</ref>
<ref id="B61-sensors-19-03274">
<label>61.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Connell</surname>
<given-names>J.W.</given-names>
</name>
<name>
<surname>Allison</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Reid</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Quantitative gait analysis using a motorized treadmill system sensitively detects motor abnormalities in mice expressing ATPase defective spastin</article-title>
<source/>PLoS ONE
          <year>2016</year>
<volume>11</volume>
<elocation-id>E0152413</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0152413</pub-id>
<pub-id pub-id-type="pmid">27019090</pub-id>
</element-citation>
</ref>
<ref id="B62-sensors-19-03274">
<label>62.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sashindranath</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Daglas</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Medcalf</surname>
<given-names>R.L.</given-names>
</name>
</person-group>
<article-title>Evaluation of gait impairment in mice subjected to craniotomy and traumatic brain injury</article-title>
<source/>Behav. Brain Res.
          <year>2015</year>
<volume>286</volume>
<fpage>33</fpage>
<lpage>38</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2015.02.038</pub-id>
<pub-id pub-id-type="pmid">25721743</pub-id>
</element-citation>
</ref>
<ref id="B63-sensors-19-03274">
<label>63.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Neckel</surname>
<given-names>N.D.</given-names>
</name>
</person-group>
<article-title>Methods to quantify the velocity dependence of common gait measurements from automated rodent gait analysis devices</article-title>
<source/>J. Neurosci. Methods
          <year>2015</year>
<volume>253</volume>
<fpage>244</fpage>
<lpage>253</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.06.017</pub-id>
<pub-id pub-id-type="pmid">26129742</pub-id>
</element-citation>
</ref>
<ref id="B64-sensors-19-03274">
<label>64.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lambert</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Philpot</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Engberg</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Johns</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Wecker</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Gait analysis and the cumulative gait index (CGI): Translational tools to assess impairments exhibited by rats with olivocerebellar ataxia</article-title>
<source/>Behav. Brain Res.
          <year>2014</year>
<volume>274</volume>
<fpage>334</fpage>
<lpage>343</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2014.08.004</pub-id>
<pub-id pub-id-type="pmid">25116252</pub-id>
</element-citation>
</ref>
<ref id="B65-sensors-19-03274">
<label>65.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Takano</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Komaki</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Hikishima</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Konomi</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Fujiyoshi</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Tsuji</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Okano</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Toyama</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Nakamura</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>In vivo tracing of neural tracts in tiptoe-walking yoshimura mice by diffusion tensor tractography</article-title>
<source/>Neuroprotection and Regeneration of the Spinal Cord
          <publisher-name>Springer</publisher-name>
<publisher-loc>Tokyo, Japan</publisher-loc>
<year>2014</year>
<fpage>107</fpage>
<lpage>117</lpage>
</element-citation>
</ref>
<ref id="B66-sensors-19-03274">
<label>66.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hampton</surname>
<given-names>T.G.</given-names>
</name>
<name>
<surname>Amende</surname>
<given-names>I.</given-names>
</name>
</person-group>
<article-title>Treadmill gait analysis characterizes gait alterations in Parkinson’s disease and amyotrophic lateral sclerosis mouse models</article-title>
<source/>J. Mot. Behav.
          <year>2009</year>
<volume>42</volume>
<fpage>1</fpage>
<lpage>4</lpage>
<pub-id pub-id-type="doi">10.1080/00222890903272025</pub-id>
</element-citation>
</ref>
<ref id="B67-sensors-19-03274">
<label>67.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Beare</surname>
<given-names>J.E.</given-names>
</name>
<name>
<surname>Morehouse</surname>
<given-names>J.R.</given-names>
</name>
<name>
<surname>DeVries</surname>
<given-names>W.H.</given-names>
</name>
<name>
<surname>Enzmann</surname>
<given-names>G.U.</given-names>
</name>
<name>
<surname>Burke</surname>
<given-names>D.A.</given-names>
</name>
<name>
<surname>Magnuson</surname>
<given-names>D.S.</given-names>
</name>
<name>
<surname>Whittemore</surname>
<given-names>S.R.</given-names>
</name>
</person-group>
<article-title>Gait analysis in normal and spinal contused mice using the TreadScan system</article-title>
<source/>J. Neurotrauma
          <year>2009</year>
<volume>26</volume>
<fpage>2045</fpage>
<lpage>2056</lpage>
<pub-id pub-id-type="doi">10.1089/neu.2009.0914</pub-id>
<pub-id pub-id-type="pmid">19886808</pub-id>
</element-citation>
</ref>
<ref id="B68-sensors-19-03274">
<label>68.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gellhaar</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Marcellino</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Abrams</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Galter</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Chronic L-DOPA induces hyperactivity, normalization of gait and dyskinetic behavior in MitoPark mice</article-title>
<source/>Genes Brain Behav.
          <year>2015</year>
<volume>14</volume>
<fpage>260</fpage>
<lpage>270</lpage>
<pub-id pub-id-type="doi">10.1111/gbb.12210</pub-id>
<pub-id pub-id-type="pmid">25752644</pub-id>
</element-citation>
</ref>
<ref id="B69-sensors-19-03274">
<label>69.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Beare</surname>
<given-names>J.</given-names>
</name>
</person-group>
<source/>Kinematic Analysis of Treadmill Walking in Normal and Contused Mice Using the TreadScan System
          <publisher-name>University of Louisville</publisher-name>
<publisher-loc>Louisville, KY, USA</publisher-loc>
<year>2007</year>
</element-citation>
</ref>
<ref id="B70-sensors-19-03274">
<label>70.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>McMackin</surname>
<given-names>M.Z.</given-names>
</name>
<name>
<surname>Henderson</surname>
<given-names>C.K.</given-names>
</name>
<name>
<surname>Cortopassi</surname>
<given-names>G.A.</given-names>
</name>
</person-group>
<article-title>Neurobehavioral deficits in the KIKO mouse model of Friedreich’s ataxia</article-title>
<source/>Behav. Brain Res.
          <year>2017</year>
<volume>316</volume>
<fpage>183</fpage>
<lpage>188</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2016.08.053</pub-id>
<pub-id pub-id-type="pmid">27575947</pub-id>
</element-citation>
</ref>
<ref id="B71-sensors-19-03274">
<label>71.</label>
<element-citation publication-type="web">
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://cleversysinc.com/CleverSysInc/csi_products/gaitscan/">http://cleversysinc.com/CleverSysInc/csi_products/gaitscan/</ext-link></comment>
<date-in-citation content-type="access-date" iso-8601-date="2019-07-24">(accessed on 24 July 2019)</date-in-citation>
</element-citation>
</ref>
<ref id="B72-sensors-19-03274">
<label>72.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Adamah-Biassi</surname>
<given-names>E.B.</given-names>
</name>
<name>
<surname>Stepien</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Hudson</surname>
<given-names>R.L.</given-names>
</name>
<name>
<surname>Dubocovich</surname>
<given-names>M.L.</given-names>
</name>
</person-group>
<article-title>Automated Video Analysis System Reveals Distinct Diurnal Behaviors in C57BL/6 and C3H/HeN Mice</article-title>
<source/>Behav. Brain Res.
          <year>2013</year>
<volume>243</volume>
<fpage>306</fpage>
<lpage>312</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2013.01.003</pub-id>
<pub-id pub-id-type="pmid">23337734</pub-id>
</element-citation>
</ref>
<ref id="B73-sensors-19-03274">
<label>73.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kyzar</surname>
<given-names>E.J.</given-names>
</name>
<name>
<surname>Pham</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Roth</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Cachat</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Green</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Gaikwad</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Kalueff</surname>
<given-names>A.V.</given-names>
</name>
</person-group>
<article-title>Alterations in grooming activity and syntax in heterozygous SERT and BDNF knockout mice: The utility of behavior-recognition tools to characterize mutant mouse phenotypes</article-title>
<source/>Brain Res. Bull.
          <year>2012</year>
<volume>89</volume>
<fpage>168</fpage>
<lpage>176</lpage>
<pub-id pub-id-type="doi">10.1016/j.brainresbull.2012.08.004</pub-id>
<pub-id pub-id-type="pmid">22951260</pub-id>
</element-citation>
</ref>
<ref id="B74-sensors-19-03274">
<label>74.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Adamah-Biassi</surname>
<given-names>E.B.</given-names>
</name>
<name>
<surname>Stepien</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Hudson</surname>
<given-names>R.L.</given-names>
</name>
<name>
<surname>Dubocovich</surname>
<given-names>M.L.</given-names>
</name>
</person-group>
<article-title>Effects of the Melatonin Receptor Antagonist (MT2)/Inverse Agonist (MT1) Luzindole on Re-entrainment of Wheel Running Activity and Spontaneous Homecage Behaviors in C3H/HeN Mice</article-title>
<source/>FASEB J.
          <year>2012</year>
<volume>26</volume>
<fpage>1042</fpage>
<lpage>1045</lpage>
</element-citation>
</ref>
<ref id="B75-sensors-19-03274">
<label>75.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kyzar</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Gaikwad</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Roth</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Green</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pham</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Stewart</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Liang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Kobla</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Kalueff</surname>
<given-names>A.V.</given-names>
</name>
</person-group>
<article-title>Towards high-throughput phenotyping of complex patterned behaviors in rodents: Focus on mouse self-grooming and its sequencing</article-title>
<source/>Behav. Brain Res.
          <year>2011</year>
<volume>225</volume>
<fpage>426</fpage>
<lpage>431</lpage>
<pub-id pub-id-type="doi">10.1016/j.bbr.2011.07.052</pub-id>
<pub-id pub-id-type="pmid">21840343</pub-id>
</element-citation>
</ref>
<ref id="B76-sensors-19-03274">
<label>76.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ou-Yang</surname>
<given-names>T.H.</given-names>
</name>
<name>
<surname>Tsai</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Yen</surname>
<given-names>C.T.</given-names>
</name>
<name>
<surname>Lin</surname>
<given-names>T.T.</given-names>
</name>
</person-group>
<article-title>An infrared range camera-based approach for three-dimensional locomotion tracking and pose reconstruction in a rodent</article-title>
<source/>J. Neurosci. Methods
          <year>2011</year>
<volume>201</volume>
<fpage>116</fpage>
<lpage>123</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2011.07.019</pub-id>
<pub-id pub-id-type="pmid">21835202</pub-id>
</element-citation>
</ref>
<ref id="B77-sensors-19-03274">
<label>77.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Cupido</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Detecting Cerebellar Phenotypes with the Erasmus Ladder</article-title>
<source/>Ph.D. Thesis
          <publisher-name>Erasmus MC. Dept of Clin Genet</publisher-name>
<publisher-loc>Rotterdam, The Netherlands</publisher-loc>
<year>2009</year>
<volume>Volume 98</volume>
</element-citation>
</ref>
<ref id="B78-sensors-19-03274">
<label>78.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ha</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Cho</surname>
<given-names>Y.S.</given-names>
</name>
<name>
<surname>Chung</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Yoo</surname>
<given-names>Y.E.</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Bae</surname>
<given-names>Y.C.</given-names>
</name>
<etal></etal>
</person-group>
<article-title>Cerebellar Shank2 regulates excitatory synapse density, motor coordination, and specific repetitive and anxiety-like behaviors</article-title>
<source/>J. Neurosci.
          <year>2016</year>
<volume>36</volume>
<fpage>12129</fpage>
<lpage>12143</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1849-16.2016</pub-id>
<pub-id pub-id-type="pmid">27903723</pub-id>
</element-citation>
</ref>
<ref id="B79-sensors-19-03274">
<label>79.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Peter</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Michiel</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Stedehouder</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Reinelt</surname>
<given-names>C.M.</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Boele</surname>
<given-names>H.J.</given-names>
</name>
<name>
<surname>Kushner</surname>
<given-names>S.A.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>M.G.</given-names>
</name>
</person-group>
<article-title>Dysfunctional cerebellar Purkinje cells contribute to autism-like behaviour in Shank2-deficient mice</article-title>
<source/>Nat. Commun.
          <year>2016</year>
<volume>7</volume>
<fpage>12627</fpage>
<pub-id pub-id-type="doi">10.1038/ncomms12627</pub-id>
<pub-id pub-id-type="pmid">27581745</pub-id>
</element-citation>
</ref>
<ref id="B80-sensors-19-03274">
<label>80.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Zeeuw</surname>
<given-names>C.I.</given-names>
</name>
<name>
<surname>Hoogland</surname>
<given-names>T.M.</given-names>
</name>
</person-group>
<article-title>Reappraisal of Bergmann glial cells as modulators of cerebellar circuit function</article-title>
<source/>Front. Cell. Neurosci.
          <year>2015</year>
<volume>9</volume>
<fpage>246</fpage>
<pub-id pub-id-type="doi">10.3389/fncel.2015.00246</pub-id>
<pub-id pub-id-type="pmid">26190972</pub-id>
</element-citation>
</ref>
<ref id="B81-sensors-19-03274">
<label>81.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sepulveda-Falla</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Barrera-Ocampo</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Hagel</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Korwitz</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Vinueza-Veloz</surname>
<given-names>M.F.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Schonewille</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Velazquez-Perez</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Rodriguez-Labrada</surname>
<given-names>R.</given-names>
</name>
<etal></etal>
</person-group>
<article-title>Familial Alzheimer’s disease-associated presenilin-1 alters cerebellar activity and calcium homeostasis</article-title>
<source/>J. Clin. Investig.
          <year>2014</year>
<volume>124</volume>
<fpage>1552</fpage>
<lpage>1567</lpage>
<pub-id pub-id-type="doi">10.1172/JCI66407</pub-id>
<pub-id pub-id-type="pmid">24569455</pub-id>
</element-citation>
</ref>
<ref id="B82-sensors-19-03274">
<label>82.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Veloz</surname>
<given-names>M.F.V.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Bosman</surname>
<given-names>L.W.</given-names>
</name>
<name>
<surname>Potters</surname>
<given-names>J.W.</given-names>
</name>
<name>
<surname>Negrello</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Seepers</surname>
<given-names>R.M.</given-names>
</name>
<name>
<surname>Strydis</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Koekkoek</surname>
<given-names>S.K.</given-names>
</name>
<name>
<surname>De Zeeuw</surname>
<given-names>C.I.</given-names>
</name>
</person-group>
<article-title>Cerebellar control of gait and interlimb coordination</article-title>
<source/>Brain Struct. Funct.
          <year>2015</year>
<volume>220</volume>
<fpage>3513</fpage>
<lpage>3536</lpage>
<pub-id pub-id-type="doi">10.1007/s00429-014-0870-1</pub-id>
<pub-id pub-id-type="pmid">25139623</pub-id>
</element-citation>
</ref>
<ref id="B83-sensors-19-03274">
<label>83.</label>
<element-citation publication-type="web">
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://goo.gl/ippZrg">https://goo.gl/ippZrg</ext-link></comment>
<date-in-citation content-type="access-date" iso-8601-date="2019-07-24">(accessed on 24 July 2019)</date-in-citation>
</element-citation>
</ref>
<ref id="B84-sensors-19-03274">
<label>84.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vidal</surname>
<given-names>P.M.</given-names>
</name>
<name>
<surname>Karadimas</surname>
<given-names>S.K.</given-names>
</name>
<name>
<surname>Ulndreaj</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Laliberte</surname>
<given-names>A.M.</given-names>
</name>
<name>
<surname>Tetreault</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Forner</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Foltz</surname>
<given-names>W.D.</given-names>
</name>
<name>
<surname>Fehlings</surname>
<given-names>M.G.</given-names>
</name>
</person-group>
<article-title>Delayed decompression exacerbates ischemia-reperfusion injury in cervical compressive myelopathy</article-title>
<source/>JCI Insight
          <year>2017</year>
<volume>2</volume>
<pub-id pub-id-type="doi">10.1172/jci.insight.92512</pub-id>
<pub-id pub-id-type="pmid">28570271</pub-id>
</element-citation>
</ref>
<ref id="B85-sensors-19-03274">
<label>85.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tatenhorst</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Eckermann</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Dambeck</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Fonseca-Ornelas</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Walle</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>da Fonseca</surname>
<given-names>T.L.</given-names>
</name>
<name>
<surname>Koch</surname>
<given-names>J.C.</given-names>
</name>
<name>
<surname>Becker</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Tönges</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Bähr</surname>
<given-names>M.</given-names>
</name>
<etal></etal>
</person-group>
<article-title>Fasudil attenuates aggregation of a-synuclein in models of Parkinson disease</article-title>
<source/>Acta Neuropathol. Commun.
          <year>2016</year>
<volume>4</volume>
<fpage>39</fpage>
<pub-id pub-id-type="doi">10.1186/s40478-016-0310-y</pub-id>
<pub-id pub-id-type="pmid">27101974</pub-id>
</element-citation>
</ref>
<ref id="B86-sensors-19-03274">
<label>86.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhou</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Zheng</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Wen</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Xiao</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Gait analysis in three different 6-hydroxydopamine rat models of Parkinson’s disease</article-title>
<source/>Neurosci. Lett.
          <year>2015</year>
<volume>584</volume>
<fpage>184</fpage>
<lpage>189</lpage>
<pub-id pub-id-type="doi">10.1016/j.neulet.2014.10.032</pub-id>
<pub-id pub-id-type="pmid">25449863</pub-id>
</element-citation>
</ref>
<ref id="B87-sensors-19-03274">
<label>87.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>Y.J.</given-names>
</name>
<name>
<surname>Cheng</surname>
<given-names>F.C.</given-names>
</name>
<name>
<surname>Sheu</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Su</surname>
<given-names>H.L.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>C.J.</given-names>
</name>
<name>
<surname>Sheehan</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>H.C.</given-names>
</name>
</person-group>
<article-title>Detection of subtle neurological alterations by the Catwalk XT gait analysis system</article-title>
<source/>J. Neuroeng. Rehabil.
          <year>2014</year>
<volume>11</volume>
<fpage>62</fpage>
<pub-id pub-id-type="doi">10.1186/1743-0003-11-62</pub-id>
<pub-id pub-id-type="pmid">24739213</pub-id>
</element-citation>
</ref>
<ref id="B88-sensors-19-03274">
<label>88.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hou</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Nelson</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Nissim</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Parmer</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Thompson</surname>
<given-names>F.J.</given-names>
</name>
<name>
<surname>Bose</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Effect of combined treadmill training and magnetic stimulation on spasticity and gait impairments after cervical spinal cord injury</article-title>
<source/>J. Neurotrauma
          <year>2014</year>
<volume>31</volume>
<fpage>1088</fpage>
<lpage>1106</lpage>
<pub-id pub-id-type="doi">10.1089/neu.2013.3096</pub-id>
<pub-id pub-id-type="pmid">24552465</pub-id>
</element-citation>
</ref>
<ref id="B89-sensors-19-03274">
<label>89.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Knutsen</surname>
<given-names>P.M.</given-names>
</name>
<name>
<surname>Derdikman</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Ahissar</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Tracking whisker and head movements in unrestrained behaving rodents</article-title>
<source/>J. Neurophysiol.
          <year>2005</year>
<volume>93</volume>
<fpage>2294</fpage>
<lpage>2301</lpage>
<pub-id pub-id-type="doi">10.1152/jn.00718.2004</pub-id>
<pub-id pub-id-type="pmid">15563552</pub-id>
</element-citation>
</ref>
<ref id="B90-sensors-19-03274">
<label>90.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gravel</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Tremblay</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Leblond</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Rossignol</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>de Guise</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>A semi-automated software tool to study treadmill locomotion in the rat: From experiment videos to statistical gait analysis</article-title>
<source/>J. Neurosci. Methods
          <year>2010</year>
<volume>190</volume>
<fpage>279</fpage>
<lpage>288</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.05.006</pub-id>
<pub-id pub-id-type="pmid">20471995</pub-id>
</element-citation>
</ref>
<ref id="B91-sensors-19-03274">
<label>91.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lenhoff</surname>
<given-names>M.W.</given-names>
</name>
<name>
<surname>Santner</surname>
<given-names>T.J.</given-names>
</name>
<name>
<surname>Otis</surname>
<given-names>J.C.</given-names>
</name>
<name>
<surname>Peterson</surname>
<given-names>M.G.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>B.J.</given-names>
</name>
<name>
<surname>Backus</surname>
<given-names>S.I.</given-names>
</name>
</person-group>
<article-title>Bootstrap prediction and confidence bands: A superior statistical method for analysis of gait data</article-title>
<source/>Gait Posture
          <year>1999</year>
<volume>9</volume>
<fpage>10</fpage>
<lpage>17</lpage>
<pub-id pub-id-type="doi">10.1016/S0966-6362(98)00043-5</pub-id>
<pub-id pub-id-type="pmid">10575065</pub-id>
</element-citation>
</ref>
<ref id="B92-sensors-19-03274">
<label>92.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bender</surname>
<given-names>J.A.</given-names>
</name>
<name>
<surname>Simpson</surname>
<given-names>E.M.</given-names>
</name>
<name>
<surname>Ritzmann</surname>
<given-names>R.E.</given-names>
</name>
</person-group>
<article-title>Computer-assisted 3D kinematic analysis of all leg joints in walking insects</article-title>
<source/>PLoS ONE
          <year>2010</year>
<volume>5</volume>
<elocation-id>E13617</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0013617</pub-id>
<pub-id pub-id-type="pmid">21049024</pub-id>
</element-citation>
</ref>
<ref id="B93-sensors-19-03274">
<label>93.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nakamura</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Funaya</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Uezono</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Nakashima</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Ishida</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Suzuki</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Wakana</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Shibata</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Low-cost three-dimensional gait analysis system for mice with an infrared depth sensor</article-title>
<source/>Neurosci. Res.
          <year>2015</year>
<volume>100</volume>
<fpage>55</fpage>
<lpage>62</lpage>
<pub-id pub-id-type="doi">10.1016/j.neures.2015.06.006</pub-id>
<pub-id pub-id-type="pmid">26166585</pub-id>
</element-citation>
</ref>
<ref id="B94-sensors-19-03274">
<label>94.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Plagemann</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Ganapathi</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Koller</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Thrun</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Realtime identification and localization of body parts from depth images</article-title>
<source/>Proceedings of the 2010 IEEE International Conference on Robotics and Automation (ICRA)
          <conf-loc>Anchorage, Alaska</conf-loc>
<conf-date>3–8 May 2010</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2010</year>
<fpage>3108</fpage>
<lpage>3113</lpage>
</element-citation>
</ref>
<ref id="B95-sensors-19-03274">
<label>95.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mendes</surname>
<given-names>C.S.</given-names>
</name>
<name>
<surname>Bartos</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Márka</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Akay</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Márka</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Mann</surname>
<given-names>R.S.</given-names>
</name>
</person-group>
<article-title>Quantification of gait parameters in freely walking rodents</article-title>
<source/>BMC Biol.
          <year>2015</year>
<volume>13</volume>
<elocation-id>50</elocation-id>
<pub-id pub-id-type="doi">10.1186/s12915-015-0154-0</pub-id>
<pub-id pub-id-type="pmid">26197889</pub-id>
</element-citation>
</ref>
<ref id="B96-sensors-19-03274">
<label>96.</label>
<element-citation publication-type="web">
<article-title>MouseWalker</article-title>
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://biooptics.markalab.org/MouseWalker/">http://biooptics.markalab.org/MouseWalker/</ext-link></comment>
<date-in-citation content-type="access-date" iso-8601-date="2019-07-24">(accessed on 24 July 2019)</date-in-citation>
</element-citation>
</ref>
<ref id="B97-sensors-19-03274">
<label>97.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Mirbozorgi</surname>
<given-names>S.A.</given-names>
</name>
<name>
<surname>Ghovanloo</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Towards a kinect-based behavior recognition and analysis system for small animals</article-title>
<source/>Proceedings of the 2015 Biomedical Circuits and Systems Conference (BioCAS)
          <conf-loc>Atlanta, GA, USA</conf-loc>
<conf-date>22–24 October 2015</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2015</year>
</element-citation>
</ref>
<ref id="B98-sensors-19-03274">
<label>98.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Voigts</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Sakmann</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Celikel</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Unsupervised whisker tracking in unrestrained behaving animals</article-title>
<source/>J. Neurophysiol.
          <year>2008</year>
<volume>100</volume>
<fpage>504</fpage>
<lpage>515</lpage>
<pub-id pub-id-type="doi">10.1152/jn.00012.2008</pub-id>
<pub-id pub-id-type="pmid">18463190</pub-id>
</element-citation>
</ref>
<ref id="B99-sensors-19-03274">
<label>99.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nashaat</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Oraby</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Peña</surname>
<given-names>L.B.</given-names>
</name>
<name>
<surname>Dominiak</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Larkum</surname>
<given-names>M.E.</given-names>
</name>
<name>
<surname>Sachdev</surname>
<given-names>R.N.</given-names>
</name>
</person-group>
<article-title>Pixying behavior: A versatile realtime and post hoc automated optical tracking method for freely moving and head fixed animals</article-title>
<source/>eNeuro
          <year>2017</year>
<volume>4</volume>
<pub-id pub-id-type="doi">10.1523/ENEURO.0245-16.2017</pub-id>
</element-citation>
</ref>
<ref id="B100-sensors-19-03274">
<label>100.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Monteiro</surname>
<given-names>J.P.</given-names>
</name>
<name>
<surname>Oliveira</surname>
<given-names>H.P.</given-names>
</name>
<name>
<surname>Aguiar</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Cardoso</surname>
<given-names>J.S.</given-names>
</name>
</person-group>
<article-title>A depth-map approach for automatic mice behavior recognition</article-title>
<source/>Proceedings of the 2014 IEEE International Conference on Image Processing (ICIP)
          <conf-loc>Paris, France</conf-loc>
<conf-date>27–30 October 2014</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2014</year>
</element-citation>
</ref>
<ref id="B101-sensors-19-03274">
<label>101.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Petrou</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Webb</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Detailed tracking of body and leg movements of a freely walking female cricket during phonotaxis</article-title>
<source/>J. Neurosci. Methods
          <year>2012</year>
<volume>203</volume>
<fpage>56</fpage>
<lpage>68</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2011.09.011</pub-id>
<pub-id pub-id-type="pmid">21951620</pub-id>
</element-citation>
</ref>
<ref id="B102-sensors-19-03274">
<label>102.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Xu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Cai</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Ren</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>A video tracking system for limb motion measurement in small animals</article-title>
<source/>Proceedings of the 2010 International Conference on Optoelectronics and Image Processing (ICOIP)
          <conf-loc>Haikou, China</conf-loc>
<conf-date>11–12 November 2010</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2010</year>
<volume>Volume 1</volume>
</element-citation>
</ref>
<ref id="B103-sensors-19-03274">
<label>103.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hwang</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Choi</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Tracking the joints of arthropod legs using multiple images and inverse kinematics</article-title>
<source/>Int. J. Precis. Eng. Manuf.
          <year>2015</year>
<volume>16</volume>
<fpage>669</fpage>
<lpage>675</lpage>
<pub-id pub-id-type="doi">10.1007/s12541-015-0089-y</pub-id>
</element-citation>
</ref>
<ref id="B104-sensors-19-03274">
<label>104.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aristidou</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Lasenby</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>FABRIK: A fast, iterative solver for the inverse kinematics problem</article-title>
<source/>Graph. Models
          <year>2011</year>
<volume>73</volume>
<fpage>243</fpage>
<lpage>260</lpage>
<pub-id pub-id-type="doi">10.1016/j.gmod.2011.05.003</pub-id>
</element-citation>
</ref>
<ref id="B105-sensors-19-03274">
<label>105.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Gyory</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Rankov</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Gordon</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Perkon</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Mitchinson</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Grant</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Prescott</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>An algorithm for automatic tracking of rat whiskers</article-title>
<source/>Proceedings of the 20th International Conference on Pattern Recognition (ICPR 2010)
          <conf-loc>Istanbul, Turkey</conf-loc>
<conf-date>22 August 2010</conf-date>
<volume>Volume 2010</volume>
</element-citation>
</ref>
<ref id="B106-sensors-19-03274">
<label>106.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>da Silva Aragão</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Rodrigues</surname>
<given-names>M.A.B.</given-names>
</name>
<name>
<surname>de Barros</surname>
<given-names>K.M.F.T.</given-names>
</name>
<name>
<surname>Silva</surname>
<given-names>S.R.F.</given-names>
</name>
<name>
<surname>Toscano</surname>
<given-names>A.E.</given-names>
</name>
<name>
<surname>de Souza</surname>
<given-names>R.E.</given-names>
</name>
<name>
<surname>Manhães-de Castro</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Automatic system for analysis of locomotor activity in rodents—A reproducibility study</article-title>
<source/>J. Neurosci. Methods
          <year>2011</year>
<volume>195</volume>
<fpage>216</fpage>
<lpage>221</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.12.016</pub-id>
<pub-id pub-id-type="pmid">21182870</pub-id>
</element-citation>
</ref>
<ref id="B107-sensors-19-03274">
<label>107.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leroy</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Stroobants</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Aerts</surname>
<given-names>J.M.</given-names>
</name>
<name>
<surname>D’Hooge</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Berckmans</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Automatic analysis of altered gait in arylsulphatase A-deficient mice in the open field</article-title>
<source/>Behav. Res. Methods
          <year>2009</year>
<volume>41</volume>
<fpage>787</fpage>
<lpage>794</lpage>
<pub-id pub-id-type="doi">10.3758/BRM.41.3.787</pub-id>
<pub-id pub-id-type="pmid">19587193</pub-id>
</element-citation>
</ref>
<ref id="B108-sensors-19-03274">
<label>108.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Clack</surname>
<given-names>N.G.</given-names>
</name>
<name>
<surname>O’Connor</surname>
<given-names>D.H.</given-names>
</name>
<name>
<surname>Huber</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Petreanu</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Hires</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Peron</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Svoboda</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Myers</surname>
<given-names>E.W.</given-names>
</name>
</person-group>
<article-title>Automated tracking of whiskers in videos of head fixed rodents</article-title>
<source/>PlOS Comput. Biol.
          <year>2012</year>
<volume>8</volume>
<elocation-id>E1002591</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002591</pub-id>
<pub-id pub-id-type="pmid">22792058</pub-id>
</element-citation>
</ref>
<ref id="B109-sensors-19-03274">
<label>109.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kloefkorn</surname>
<given-names>H.E.</given-names>
</name>
<name>
<surname>Pettengill</surname>
<given-names>T.R.</given-names>
</name>
<name>
<surname>Turner</surname>
<given-names>S.M.</given-names>
</name>
<name>
<surname>Streeter</surname>
<given-names>K.A.</given-names>
</name>
<name>
<surname>Gonzalez-Rothi</surname>
<given-names>E.J.</given-names>
</name>
<name>
<surname>Fuller</surname>
<given-names>D.D.</given-names>
</name>
<name>
<surname>Allen</surname>
<given-names>K.D.</given-names>
</name>
</person-group>
<article-title>Automated Gait Analysis Through Hues and Areas (AGATHA): A method to characterize the spatiotemporal pattern of rat gait</article-title>
<source/>Ann. Biomed. Eng.
          <year>2017</year>
<volume>45</volume>
<fpage>711</fpage>
<lpage>725</lpage>
<pub-id pub-id-type="doi">10.1007/s10439-016-1717-0</pub-id>
<pub-id pub-id-type="pmid">27554674</pub-id>
</element-citation>
</ref>
<ref id="B110-sensors-19-03274">
<label>110.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dankert</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Hoopfer</surname>
<given-names>E.D.</given-names>
</name>
<name>
<surname>Anderson</surname>
<given-names>D.J.</given-names>
</name>
<name>
<surname>Perona</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Automated monitoring and analysis of social behavior in Drosophila</article-title>
<source/>Nat. Methods
          <year>2009</year>
<volume>6</volume>
<fpage>297</fpage>
<pub-id pub-id-type="doi">10.1038/nmeth.1310</pub-id>
<pub-id pub-id-type="pmid">19270697</pub-id>
</element-citation>
</ref>
<ref id="B111-sensors-19-03274">
<label>111.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Bishop</surname>
<given-names>C.M.</given-names>
</name>
</person-group>
<source/>Pattern Recognition and Machine Learning
          <publisher-name>Springer</publisher-name>
<publisher-loc>New York, NY, USA</publisher-loc>
<year>2007</year>
<fpage>738</fpage>
</element-citation>
</ref>
<ref id="B112-sensors-19-03274">
<label>112.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Otsu</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>A threshold selection method from gray level histograms</article-title>
<source/>IEEE Trans. Syst. Man Cybern.
          <year>1979</year>
<volume>9</volume>
<fpage>62</fpage>
<lpage>66</lpage>
<pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id>
</element-citation>
</ref>
<ref id="B113-sensors-19-03274">
<label>113.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>H.J.</given-names>
</name>
<name>
<surname>Shi</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Akdagli</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Most</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Yan</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Semi-Automated Tracking of Vibrissal Movements in Free-Moving Rodents Captured by High-Speed Videos</article-title>
<source/>World Acad. Sci. Eng. Technol. Int. J. Biol. Biomol. Agric. Food Biotechnol. Eng.
          <year>2015</year>
<volume>9</volume>
<fpage>565</fpage>
<lpage>569</lpage>
</element-citation>
</ref>
<ref id="B114-sensors-19-03274">
<label>114.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Palmér</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Åström</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Enqvist</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Ivica</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Petersson</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Rat Paw Tracking for Detailed Motion Analysis</article-title>
<source/>Proceedings of the Visual observation and analysis of Vertebrate And Insect Behavior 2014
          <conf-loc>Stockholm, Sweden</conf-loc>
<conf-date>24 August 2014</conf-date>
</element-citation>
</ref>
<ref id="B115-sensors-19-03274">
<label>115.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Palmér</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Tamtè</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Halje</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Enqvist</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Petersson</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>A system for automated tracking of motor components in neurophysiological research</article-title>
<source/>J. Neurosci. Methods
          <year>2012</year>
<volume>205</volume>
<fpage>334</fpage>
<lpage>344</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2012.01.008</pub-id>
<pub-id pub-id-type="pmid">22306061</pub-id>
</element-citation>
</ref>
<ref id="B116-sensors-19-03274">
<label>116.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Giovannucci</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Pnevmatikakis</surname>
<given-names>E.A.</given-names>
</name>
<name>
<surname>Deverett</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Pereira</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Fondriest</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Brady</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>S.H.</given-names>
</name>
<name>
<surname>Abbas</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Parés</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Masip</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Automated gesture tracking in head-fixed mice</article-title>
<source/>J. Neurosci. Methods
          <year>2018</year>
<volume>300</volume>
<fpage>184</fpage>
<lpage>195</lpage>
<pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.07.014</pub-id>
<pub-id pub-id-type="pmid">28728948</pub-id>
</element-citation>
</ref>
<ref id="B117-sensors-19-03274">
<label>117.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Mathis</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Mamidanna</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Cury</surname>
<given-names>K.M.</given-names>
</name>
<name>
<surname>Abe</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Murthy</surname>
<given-names>V.N.</given-names>
</name>
<name>
<surname>Mathis</surname>
<given-names>M.W.</given-names>
</name>
<name>
<surname>Bethge</surname>
<given-names>M.</given-names>
</name>
</person-group>
<source/>DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning
          <publisher-name>Nature Publishing Group</publisher-name>
<publisher-loc>London, UK</publisher-loc>
<year>2018</year>
</element-citation>
</ref>
<ref id="B118-sensors-19-03274">
<label>118.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Insafutdinov</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Pishchulin</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Andres</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Andriluka</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Schiele</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Deepercut: A deeper, stronger, and faster multi-person pose estimation model</article-title>
<source/>Proceedings of the European Conference on Computer Vision
          <publisher-name>Springer</publisher-name>
<publisher-loc>Cham, Switzerland</publisher-loc>
<year>2016</year>
</element-citation>
</ref>
<ref id="B119-sensors-19-03274">
<label>119.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Arac</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Dobkin</surname>
<given-names>B.H.</given-names>
</name>
<name>
<surname>Carmichael</surname>
<given-names>S.T.</given-names>
</name>
<name>
<surname>Golshani</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>DeepBehavior: A deep learning toolbox for automated analysis of animal and human behavior imaging data</article-title>
<source/>Front. Syst. Neurosci.
          <year>2019</year>
<volume>13</volume>
<fpage>20</fpage>
<pub-id pub-id-type="doi">10.3389/fnsys.2019.00020</pub-id>
<pub-id pub-id-type="pmid">31133826</pub-id>
</element-citation>
</ref>
<ref id="B120-sensors-19-03274">
<label>120.</label>
<element-citation publication-type="web">
<comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://github.com/Russell91/TensorBox">https://github.com/Russell91/TensorBox</ext-link></comment>
<date-in-citation content-type="access-date" iso-8601-date="2019-07-24">(accessed on 24 July 2019)</date-in-citation>
</element-citation>
</ref>
<ref id="B121-sensors-19-03274">
<label>121.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Redmon</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Divvala</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Girshick</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Farhadi</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>You only look once: Unified, real-time object detection</article-title>
<source/>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
          <conf-loc>Las Vegas, NV, USA</conf-loc>
<conf-date>27–30 June 2016</conf-date>
</element-citation>
</ref>
<ref id="B122-sensors-19-03274">
<label>122.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Cao</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Hidalgo</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Wei</surname>
<given-names>S.E.</given-names>
</name>
<name>
<surname>Sheikh</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Realtime multi-person 2d pose estimation using part affinity fields</article-title>
<source/>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
          <conf-loc>Honolulu, HI, USA</conf-loc>
<conf-date>21–26 July 2017</conf-date>
</element-citation>
</ref>
<ref id="B123-sensors-19-03274">
<label>123.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dirks</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Groenink</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Verdouw</surname>
<given-names>M.P.</given-names>
</name>
<name>
<surname>Gugten</surname>
<given-names>J.v.d.</given-names>
</name>
<name>
<surname>Hijzen</surname>
<given-names>T.H.</given-names>
</name>
<name>
<surname>Olivier</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Behavioral analysis of transgenic mice overexpressing corticotropin-releasing hormone in paradigms emulating aspects of stress, anxiety, and depression</article-title>
<source/>Int. J. Comp. Psychol.
          <year>2001</year>
<volume>14</volume>
<fpage>123</fpage>
<lpage>135</lpage>
</element-citation>
</ref>
<ref id="B124-sensors-19-03274">
<label>124.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>de Paula Nascimento-Castro</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Wink</surname>
<given-names>A.C.</given-names>
</name>
<name>
<surname>da Fônseca</surname>
<given-names>V.S.</given-names>
</name>
<name>
<surname>Bianco</surname>
<given-names>C.D.</given-names>
</name>
<name>
<surname>Winkelmann-Duarte</surname>
<given-names>E.C.</given-names>
</name>
<name>
<surname>Farina</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Rodrigues</surname>
<given-names>A.L.S.</given-names>
</name>
<name>
<surname>Gil-Mohapel</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>de Bem</surname>
<given-names>A.F.</given-names>
</name>
<name>
<surname>Brocardo</surname>
<given-names>P.S.</given-names>
</name>
</person-group>
<article-title>Antidepressant effects of probucol on early-symptomatic YAC128 transgenic mice for Huntington’s disease</article-title>
<source/>Neural Plast.
          <year>2018</year>
<volume>2018</volume>
<fpage>4056383</fpage>
<pub-id pub-id-type="doi">10.1155/2018/4056383</pub-id>
<pub-id pub-id-type="pmid">30186318</pub-id>
</element-citation>
</ref>
<ref id="B125-sensors-19-03274">
<label>125.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yun</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Donovan</surname>
<given-names>M.H.</given-names>
</name>
<name>
<surname>Ross</surname>
<given-names>M.N.</given-names>
</name>
<name>
<surname>Richardson</surname>
<given-names>D.R.</given-names>
</name>
<name>
<surname>Reister</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Farnbauch</surname>
<given-names>L.A.</given-names>
</name>
<name>
<surname>Fischer</surname>
<given-names>S.J.</given-names>
</name>
<name>
<surname>Riethmacher</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Gershenfeld</surname>
<given-names>H.K.</given-names>
</name>
<name>
<surname>Lagace</surname>
<given-names>D.C.</given-names>
</name>
<etal></etal>
</person-group>
<article-title>Stress-induced anxiety- and depressive-like phenotype associated with transient reduction in neurogenesis in adult nestin-CreERT2/diphtheria toxin fragment A transgenic mice</article-title>
<source/>PLoS ONE
          <year>2016</year>
<volume>11</volume>
<elocation-id>E0147256</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0147256</pub-id>
<pub-id pub-id-type="pmid">26795203</pub-id>
</element-citation>
</ref>
<ref id="B126-sensors-19-03274">
<label>126.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Krishnan</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Nestler</surname>
<given-names>E.J.</given-names>
</name>
</person-group>
<source/>Animal Models of Depression: Molecular Perspectives. Molecular and Functional Models in Neuropsychiatry
          <publisher-name>Springer</publisher-name>
<publisher-loc>Berlin/Heidelberg, Geramny</publisher-loc>
<year>2011</year>
<fpage>121</fpage>
<lpage>147</lpage>
</element-citation>
</ref>
<ref id="B127-sensors-19-03274">
<label>127.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shimogawa</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Sakaguchi</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Kikuchi</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Tsuchimochi</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Sano</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Torikoshi</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Ito</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Aoyama</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Iihara</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Takahashi</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Therapeutic effects of combined cell transplantation and locomotor training in rats with brain injury</article-title>
<source/>NPJ Regen. Med.
          <year>2019</year>
<volume>4</volume>
<fpage>13</fpage>
<pub-id pub-id-type="doi">10.1038/s41536-019-0075-6</pub-id>
<pub-id pub-id-type="pmid">31231547</pub-id>
</element-citation>
</ref>
<ref id="B128-sensors-19-03274">
<label>128.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lazar</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Moreno</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Jacob</surname>
<given-names>H.J.</given-names>
</name>
<name>
<surname>Kwitek</surname>
<given-names>A.E.</given-names>
</name>
</person-group>
<article-title>Impact of genomics on research in the rat</article-title>
<source/>Genome Res.
          <year>2005</year>
<volume>15</volume>
<fpage>1717</fpage>
<lpage>1728</lpage>
<pub-id pub-id-type="doi">10.1101/gr.3744005</pub-id>
<pub-id pub-id-type="pmid">16339370</pub-id>
</element-citation>
</ref>
<ref id="B129-sensors-19-03274">
<label>129.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lemieux</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Josset</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Roussel</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Couraud</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Bretzner</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Speed-dependent modulation of the locomotor behavior in adult mice reveals attractor and transitional gaits</article-title>
<source/>Front. Neurosci.
          <year>2016</year>
<volume>10</volume>
<fpage>42</fpage>
<pub-id pub-id-type="doi">10.3389/fnins.2016.00042</pub-id>
<pub-id pub-id-type="pmid">26941592</pub-id>
</element-citation>
</ref>
<ref id="B130-sensors-19-03274">
<label>130.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lister</surname>
<given-names>R.G.</given-names>
</name>
</person-group>
<article-title>The use of a plus-maze to measure anxiety in the mouse</article-title>
<source/>Psychopharmacology
          <year>1987</year>
<volume>92</volume>
<fpage>180</fpage>
<lpage>185</lpage>
<pub-id pub-id-type="doi">10.1007/BF00177912</pub-id>
<pub-id pub-id-type="pmid">3110839</pub-id>
</element-citation>
</ref>
<ref id="B131-sensors-19-03274">
<label>131.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fraser</surname>
<given-names>L.M.</given-names>
</name>
<name>
<surname>Brown</surname>
<given-names>R.E.</given-names>
</name>
<name>
<surname>Hussin</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Fontana</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Whittaker</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>O’Leary</surname>
<given-names>T.P.</given-names>
</name>
<name>
<surname>Lederle</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Holmes</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Ramos</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Measuring anxiety- and locomotion-related behaviours in mice: A new way of using old tests</article-title>
<source/>Psychopharmacology
          <year>2010</year>
<volume>211</volume>
<fpage>99</fpage>
<lpage>112</lpage>
<pub-id pub-id-type="doi">10.1007/s00213-010-1873-0</pub-id>
<pub-id pub-id-type="pmid">20454890</pub-id>
</element-citation>
</ref>
<ref id="B132-sensors-19-03274">
<label>132.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Seibenhener</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Wooten</surname>
<given-names>M.C.</given-names>
</name>
</person-group>
<article-title>Use of the Open Field Maze to measure locomotor and anxiety-like behavior in mice</article-title>
<source/>JoVE (J. Vis. Exp.)
          <year>2015</year>
<volume>96</volume>
<fpage>E52434</fpage>
<pub-id pub-id-type="doi">10.3791/52434</pub-id>
</element-citation>
</ref>
<ref id="B133-sensors-19-03274">
<label>133.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tatem</surname>
<given-names>K.S.</given-names>
</name>
<name>
<surname>Quinn</surname>
<given-names>J.L.</given-names>
</name>
<name>
<surname>Phadke</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Yu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Gordish-Dressman</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Nagaraju</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Behavioral and locomotor measurements using an open field activity monitoring system for skeletal muscle diseases</article-title>
<source/>JoVE (J. Vis. Exp.)
          <year>2014</year>
<volume>91</volume>
<fpage>E51785</fpage>
<pub-id pub-id-type="doi">10.3791/51785</pub-id>
</element-citation>
</ref>
<ref id="B134-sensors-19-03274">
<label>134.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lira</surname>
<given-names>F.S.</given-names>
</name>
<name>
<surname>Esteves</surname>
<given-names>A.M.</given-names>
</name>
<name>
<surname>Pimentel</surname>
<given-names>G.D.</given-names>
</name>
<name>
<surname>Rosa</surname>
<given-names>J.C.</given-names>
</name>
<name>
<surname>Frank</surname>
<given-names>M.K.</given-names>
</name>
<name>
<surname>Mariano</surname>
<given-names>M.O.</given-names>
</name>
<name>
<surname>Budni</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Quevedo</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>dos Santos</surname>
<given-names>R.V.</given-names>
</name>
<name>
<surname>De Mello</surname>
<given-names>M.T.</given-names>
</name>
</person-group>
<article-title>Sleep pattern and locomotor activity are impaired by doxorubicin in non-tumor-bearing rats</article-title>
<source/>Sleep Sci.
          <year>2016</year>
<volume>9</volume>
<fpage>232</fpage>
<lpage>235</lpage>
<pub-id pub-id-type="doi">10.1016/j.slsci.2016.10.006</pub-id>
<pub-id pub-id-type="pmid">28123667</pub-id>
</element-citation>
</ref>
<ref id="B135-sensors-19-03274">
<label>135.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schramm-Sapyta</surname>
<given-names>N.L.</given-names>
</name>
<name>
<surname>Cha</surname>
<given-names>Y.M.</given-names>
</name>
<name>
<surname>Chaudhry</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Wilson</surname>
<given-names>W.A.</given-names>
</name>
<name>
<surname>Swartzwelder</surname>
<given-names>H.S.</given-names>
</name>
<name>
<surname>Kuhn</surname>
<given-names>C.M.</given-names>
</name>
</person-group>
<article-title>Differential anxiogenic, aversive, and locomotor effects of THC in adolescent and adult rats</article-title>
<source/>Psychopharmacology
          <year>2007</year>
<volume>191</volume>
<fpage>867</fpage>
<lpage>877</lpage>
<pub-id pub-id-type="doi">10.1007/s00213-006-0676-9</pub-id>
<pub-id pub-id-type="pmid">17211649</pub-id>
</element-citation>
</ref>
<ref id="B136-sensors-19-03274">
<label>136.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Javadi-Paydar</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Nguyen</surname>
<given-names>J.D.</given-names>
</name>
<name>
<surname>Vandewater</surname>
<given-names>S.A.</given-names>
</name>
<name>
<surname>Dickerson</surname>
<given-names>T.J.</given-names>
</name>
<name>
<surname>Taffe</surname>
<given-names>M.A.</given-names>
</name>
</person-group>
<article-title>Locomotor and reinforcing effects of pentedrone, pentylone and methylone in rats</article-title>
<source/>Neuropharmacology
          <year>2018</year>
<volume>134</volume>
<fpage>57</fpage>
<lpage>64</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuropharm.2017.09.002</pub-id>
<pub-id pub-id-type="pmid">28882561</pub-id>
</element-citation>
</ref>
<ref id="B137-sensors-19-03274">
<label>137.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Walker</surname>
<given-names>R.B.</given-names>
</name>
<name>
<surname>Fitz</surname>
<given-names>L.D.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>L.M.</given-names>
</name>
<name>
<surname>McDaniel</surname>
<given-names>Y.M.</given-names>
</name>
</person-group>
<article-title>The effect on ephedrine prodrugs on locomotor activity in rats</article-title>
<source/>Gen. Pharmacol.
          <year>1996</year>
<volume>27</volume>
<fpage>109</fpage>
<lpage>111</lpage>
<pub-id pub-id-type="doi">10.1016/0306-3623(95)00127-1</pub-id>
<pub-id pub-id-type="pmid">8742505</pub-id>
</element-citation>
</ref>
<ref id="B138-sensors-19-03274">
<label>138.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wellman</surname>
<given-names>P.J.</given-names>
</name>
<name>
<surname>Davis</surname>
<given-names>K.W.</given-names>
</name>
<name>
<surname>Clifford</surname>
<given-names>P.S.</given-names>
</name>
<name>
<surname>Rothman</surname>
<given-names>R.B.</given-names>
</name>
<name>
<surname>Blough</surname>
<given-names>B.E.</given-names>
</name>
</person-group>
<article-title>Changes in feeding and locomotion induced by amphetamine analogs in rats</article-title>
<source/>Drug Alcohol Depend.
          <year>2009</year>
<volume>100</volume>
<fpage>234</fpage>
<lpage>239</lpage>
<pub-id pub-id-type="doi">10.1016/j.drugalcdep.2008.10.005</pub-id>
<pub-id pub-id-type="pmid">19062203</pub-id>
</element-citation>
</ref>
<ref id="B139-sensors-19-03274">
<label>139.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Kong</surname>
<given-names>Q.</given-names>
</name>
</person-group>
<article-title>Locomotor activity: A distinctive index in morphine self-administration in rats</article-title>
<source/>PLoS ONE
          <year>2017</year>
<volume>12</volume>
<elocation-id>E0174272</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0174272</pub-id>
<pub-id pub-id-type="pmid">28380023</pub-id>
</element-citation>
</ref>
<ref id="B140-sensors-19-03274">
<label>140.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wiechman</surname>
<given-names>B.E.</given-names>
</name>
<name>
<surname>Wood</surname>
<given-names>T.E.</given-names>
</name>
<name>
<surname>Spratto</surname>
<given-names>G.R.</given-names>
</name>
</person-group>
<article-title>Locomotor activity in morphine-treated rats: Effects of and comparisons between cocaine, procaine, and lidocaine</article-title>
<source/>Pharmacol. Biochem. Behav.
          <year>1981</year>
<volume>15</volume>
<fpage>425</fpage>
<lpage>433</lpage>
<pub-id pub-id-type="doi">10.1016/0091-3057(81)90273-2</pub-id>
<pub-id pub-id-type="pmid">7197366</pub-id>
</element-citation>
</ref>
<ref id="B141-sensors-19-03274">
<label>141.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Walker</surname>
<given-names>Q.D.</given-names>
</name>
<name>
<surname>Schramm-Sapyta</surname>
<given-names>N.L.</given-names>
</name>
<name>
<surname>Caster</surname>
<given-names>J.M.</given-names>
</name>
<name>
<surname>Waller</surname>
<given-names>S.T.</given-names>
</name>
<name>
<surname>Brooks</surname>
<given-names>M.P.</given-names>
</name>
<name>
<surname>Kuhn</surname>
<given-names>C.M.</given-names>
</name>
</person-group>
<article-title>Novelty-induced locomotion is positively associated with cocaine ingestion in adolescent rats; anxiety is correlated in adults</article-title>
<source/>Pharmacol. Biochem. Behav.
          <year>2009</year>
<volume>91</volume>
<fpage>398</fpage>
<lpage>408</lpage>
<pub-id pub-id-type="doi">10.1016/j.pbb.2008.08.019</pub-id>
<pub-id pub-id-type="pmid">18790706</pub-id>
</element-citation>
</ref>
<ref id="B142-sensors-19-03274">
<label>142.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Marin</surname>
<given-names>M.T.</given-names>
</name>
<name>
<surname>Zancheta</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Paro</surname>
<given-names>A.H.</given-names>
</name>
<name>
<surname>Possi</surname>
<given-names>A.P.</given-names>
</name>
<name>
<surname>Cruz</surname>
<given-names>F.C.</given-names>
</name>
<name>
<surname>Planeta</surname>
<given-names>C.S.</given-names>
</name>
</person-group>
<article-title>Comparison of caffeine-induced locomotor activity between adolescent and adult rats</article-title>
<source/>Eur. J. Pharmacol.
          <year>2011</year>
<volume>660</volume>
<fpage>363</fpage>
<lpage>367</lpage>
<pub-id pub-id-type="doi">10.1016/j.ejphar.2011.03.052</pub-id>
<pub-id pub-id-type="pmid">21497160</pub-id>
</element-citation>
</ref>
<ref id="B143-sensors-19-03274">
<label>143.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dipoppa</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Ranson</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Krumin</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Pachitariu</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Carandini</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Harris</surname>
<given-names>K.D.</given-names>
</name>
</person-group>
<article-title>Vision and locomotion shape the interactions between neuron types in mouse visual cortex</article-title>
<source/>Neuron
          <year>2018</year>
<volume>98</volume>
<fpage>602</fpage>
<lpage>615</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id>
<pub-id pub-id-type="pmid">29656873</pub-id>
</element-citation>
</ref>
<ref id="B144-sensors-19-03274">
<label>144.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dadarlat</surname>
<given-names>M.C.</given-names>
</name>
<name>
<surname>Stryker</surname>
<given-names>M.P.</given-names>
</name>
</person-group>
<article-title>Locomotion enhances neural encoding of visual stimuli in mouse V1</article-title>
<source/>J. Neurosci.
          <year>2017</year>
<volume>37</volume>
<fpage>3764</fpage>
<lpage>3775</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.2728-16.2017</pub-id>
<pub-id pub-id-type="pmid">28264980</pub-id>
</element-citation>
</ref>
<ref id="B145-sensors-19-03274">
<label>145.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tresch</surname>
<given-names>M.C.</given-names>
</name>
<name>
<surname>Kiehn</surname>
<given-names>O.</given-names>
</name>
</person-group>
<article-title>Synchronization of motor neurons during locomotion in the neonatal rat: Predictors and mechanisms</article-title>
<source/>J. Neurosci.
          <year>2002</year>
<volume>22</volume>
<fpage>9997</fpage>
<lpage>10008</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-22-09997.2002</pub-id>
<pub-id pub-id-type="pmid">12427857</pub-id>
</element-citation>
</ref>
<ref id="B146-sensors-19-03274">
<label>146.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vinck</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Batista-Brito</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Knoblich</surname>
<given-names>U.</given-names>
</name>
<name>
<surname>Cardin</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding</article-title>
<source/>Neuron
          <year>2015</year>
<volume>86</volume>
<fpage>740</fpage>
<lpage>754</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.028</pub-id>
<pub-id pub-id-type="pmid">25892300</pub-id>
</element-citation>
</ref>
<ref id="B147-sensors-19-03274">
<label>147.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dunn</surname>
<given-names>T.W.</given-names>
</name>
<name>
<surname>Mu</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Narayan</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Randlett</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Naumann</surname>
<given-names>E.A.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>C.T.</given-names>
</name>
<name>
<surname>Schier</surname>
<given-names>A.F.</given-names>
</name>
<name>
<surname>Freeman</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Engert</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Ahrens</surname>
<given-names>M.B.</given-names>
</name>
</person-group>
<article-title>Brain-wide mapping of neural activity controlling zebrafish exploratory locomotion</article-title>
<source/>eLife
          <year>2016</year>
<volume>5</volume>
<fpage>E12741</fpage>
<pub-id pub-id-type="doi">10.7554/eLife.12741</pub-id>
<pub-id pub-id-type="pmid">27003593</pub-id>
</element-citation>
</ref>
<ref id="B148-sensors-19-03274">
<label>148.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hesse</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Locomotor therapy in neurorehabilitation</article-title>
<source/>NeuroRehabilitation
          <year>2001</year>
<volume>16</volume>
<fpage>133</fpage>
<lpage>139</lpage>
<pub-id pub-id-type="pmid">11790898</pub-id>
</element-citation>
</ref>
<ref id="B149-sensors-19-03274">
<label>149.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Turner</surname>
<given-names>D.L.</given-names>
</name>
<name>
<surname>Murguialday</surname>
<given-names>A.R.</given-names>
</name>
<name>
<surname>Birbaumer</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Hoffmann</surname>
<given-names>U.</given-names>
</name>
<name>
<surname>Luft</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Neurophysiology of robot-mediated training and therapy: A perspective for future use in clinical populations</article-title>
<source/>Front. Neurol.
          <year>2013</year>
<volume>4</volume>
<fpage>184</fpage>
<pub-id pub-id-type="doi">10.3389/fneur.2013.00184</pub-id>
<pub-id pub-id-type="pmid">24312073</pub-id>
</element-citation>
</ref>
<ref id="B150-sensors-19-03274">
<label>150.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shik</surname>
<given-names>M.L.</given-names>
</name>
<name>
<surname>Orlovsky</surname>
<given-names>G.N.</given-names>
</name>
</person-group>
<article-title>Neurophysiology of locomotor automatism</article-title>
<source/>Physiol. Rev.
          <year>1976</year>
<volume>56</volume>
<fpage>465</fpage>
<lpage>501</lpage>
<pub-id pub-id-type="doi">10.1152/physrev.1976.56.3.465</pub-id>
<pub-id pub-id-type="pmid">778867</pub-id>
</element-citation>
</ref>
<ref id="B151-sensors-19-03274">
<label>151.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wernig</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Locomotor programs versus ‘conventional’ physical therapy? Locomotor training</article-title>
<source/>Spinal Cord
          <year>2012</year>
<volume>50</volume>
<fpage>641</fpage>
<pub-id pub-id-type="doi">10.1038/sc.2011.190</pub-id>
<pub-id pub-id-type="pmid">22249326</pub-id>
</element-citation>
</ref>
<ref id="B152-sensors-19-03274">
<label>152.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leech</surname>
<given-names>K.A.</given-names>
</name>
<name>
<surname>Kinnaird</surname>
<given-names>C.R.</given-names>
</name>
<name>
<surname>Holleran</surname>
<given-names>C.L.</given-names>
</name>
<name>
<surname>Kahn</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Hornby</surname>
<given-names>T.G.</given-names>
</name>
</person-group>
<article-title>Effects of locomotor exercise intensity on gait performance in individuals with incomplete spinal cord injury</article-title>
<source/>Phys. Ther.
          <year>2016</year>
<volume>96</volume>
<fpage>1919</fpage>
<lpage>1929</lpage>
<pub-id pub-id-type="doi">10.2522/ptj.20150646</pub-id>
<pub-id pub-id-type="pmid">27313241</pub-id>
</element-citation>
</ref>
<ref id="B153-sensors-19-03274">
<label>153.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van Meer</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Raber</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Mouse behavioural analysis in systems biology</article-title>
<source/>Biochem. J.
          <year>2005</year>
<volume>389</volume>
<fpage>593</fpage>
<lpage>610</lpage>
<pub-id pub-id-type="doi">10.1042/BJ20042023</pub-id>
<pub-id pub-id-type="pmid">16035954</pub-id>
</element-citation>
</ref>
<ref id="B154-sensors-19-03274">
<label>154.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>O’Hara</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lui</surname>
<given-names>Y.M.</given-names>
</name>
<name>
<surname>Draper</surname>
<given-names>B.A.</given-names>
</name>
</person-group>
<article-title>Unsupervised learning of human expressions, gestures, and actions</article-title>
<source/>Face Gesture
          <publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2011</year>
</element-citation>
</ref>
<ref id="B155-sensors-19-03274">
<label>155.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Lala</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Mohammad</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Nishida</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Unsupervised gesture recognition system for learning manipulative actions in virtual basketball</article-title>
<source/>Proceedings of the 1st International Conference on Human-Agent Interaction
          <conf-loc>Sapporo, Japan</conf-loc>
<conf-date>7–9 August 2013</conf-date>
</element-citation>
</ref>
<ref id="B156-sensors-19-03274">
<label>156.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Rhodin</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Salzmann</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Fua</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Unsupervised geometry-aware representation for 3d human pose estimation</article-title>
<source/>Proceedings of the European Conference on Computer Vision (ECCV)
          <conf-loc>Munich, Germany</conf-loc>
<conf-date>8–14 September 2018</conf-date>
</element-citation>
</ref>
<ref id="B157-sensors-19-03274">
<label>157.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simão</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Neto</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Gibaru</surname>
<given-names>O.</given-names>
</name>
</person-group>
<article-title>Unsupervised gesture segmentation by motion detection of a real-time data stream</article-title>
<source/>IEEE Trans. Ind. Inform.
          <year>2016</year>
<volume>13</volume>
<fpage>473</fpage>
<lpage>481</lpage>
<pub-id pub-id-type="doi">10.1109/TII.2016.2613683</pub-id>
</element-citation>
</ref>
<ref id="B158-sensors-19-03274">
<label>158.</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dubost</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Adams</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Yilmaz</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Bortsova</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>van Tulder</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Ikram</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Niessen</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Vernooij</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>de Bruijne</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Weakly Supervised Object Detection with 2D and 3D Regression Neural Networks</article-title>
<source/>arXiv
          <year>2019</year>
<pub-id pub-id-type="arxiv">1906.01891</pub-id>
</element-citation>
</ref>
<ref id="B159-sensors-19-03274">
<label>159.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>C.H.</given-names>
</name>
<name>
<surname>Tyagi</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Agrawal</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Drover</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>MV</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Stojanov</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Rehg</surname>
<given-names>J.M.</given-names>
</name>
</person-group>
<article-title>Unsupervised 3D Pose Estimation with Geometric Self-Supervision</article-title>
<source/>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
          <publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2019</year>
</element-citation>
</ref>
<ref id="B160-sensors-19-03274">
<label>160.</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Asadi-Aghbolaghi</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Clapes</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Bellantonio</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Escalante</surname>
<given-names>H.J.</given-names>
</name>
<name>
<surname>Ponce-López</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Baró</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Guyon</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Kasaei</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Escalera</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>A survey on deep learning based approaches for action and gesture recognition in image sequences</article-title>
<source/>Proceedings of the 2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)
          <conf-loc>Washington, DC, USA</conf-loc>
<conf-date>30 May–3 June 2017</conf-date>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2017</year>
</element-citation>
</ref>
<ref id="B161-sensors-19-03274">
<label>161.</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Toshev</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Szegedy</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Deeppose: Human pose estimation via deep neural networks</article-title>
<source/>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
          <publisher-name>IEEE</publisher-name>
<publisher-loc>Piscataway, NJ, USA</publisher-loc>
<year>2014</year>
</element-citation>
</ref>
</ref-list>
</back>
<floats-group>
<fig id="sensors-19-03274-f001" orientation="portrait" position="float">
<label>Figure 1</label>
<caption>
<p>Frontal view of a mouse with its moving limbs marked.</p>
</caption>
<graphic xlink:href="sensors-19-03274-g001"></graphic>
</fig>
<fig id="sensors-19-03274-f002" orientation="portrait" position="float">
<label>Figure 2</label>
<caption>
<p>Lateral view of a mouse with its moving limbs marked.</p>
</caption>
<graphic xlink:href="sensors-19-03274-g002"></graphic>
</fig>
<fig id="sensors-19-03274-f003" orientation="portrait" position="float">
<label>Figure 3</label>
<caption>
<p>Categories and their hierarchy of the approaches covered in this survey paper.</p>
</caption>
<graphic xlink:href="sensors-19-03274-g003"></graphic>
</fig>
</floats-group>
</article>
</pmc-articleset>