<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="research-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Inf Fusion</journal-id>
<journal-id journal-id-type="iso-abbrev">Inf Fusion</journal-id>
<journal-title-group>
<journal-title>An International Journal on Information Fusion</journal-title>
</journal-title-group>
<issn pub-type="ppub">1566-2535</issn>
<issn pub-type="epub">1872-6305</issn>
<publisher>
<publisher-name>Elsevier B.V.</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">32834795</article-id>
<article-id pub-id-type="pmc">7366126</article-id>
<article-id pub-id-type="publisher-id">S1566-2535(20)30318-3</article-id>
<article-id pub-id-type="doi">10.1016/j.inffus.2020.07.006</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" id="au0001">
<name>
<surname>Zhang</surname>
<given-names>Yu-Dong</given-names>
</name>
<xref ref-type="aff" rid="aff0001">a</xref>
<xref ref-type="aff" rid="aff0002">b</xref>
<xref ref-type="corresp" rid="cor0001">⁎</xref>
</contrib>
<contrib contrib-type="author" id="au0002">
<name>
<surname>Dong</surname>
<given-names>Zhengchao</given-names>
</name>
<xref ref-type="aff" rid="aff0003">c</xref>
<xref ref-type="aff" rid="aff0004">d</xref>
</contrib>
<contrib contrib-type="author" id="au0003">
<name>
<surname>Wang</surname>
<given-names>Shui-Hua</given-names>
</name>
<xref ref-type="aff" rid="aff0002">b</xref>
<xref ref-type="aff" rid="aff0006">f</xref>
<xref ref-type="aff" rid="aff0007">g</xref>
</contrib>
<contrib contrib-type="author" id="au0004">
<name>
<surname>Yu</surname>
<given-names>Xiang</given-names>
</name>
<xref ref-type="aff" rid="aff0001">a</xref>
</contrib>
<contrib contrib-type="author" id="au0005">
<name>
<surname>Yao</surname>
<given-names>Xujing</given-names>
</name>
<xref ref-type="aff" rid="aff0001">a</xref>
</contrib>
<contrib contrib-type="author" id="au0006">
<name>
<surname>Zhou</surname>
<given-names>Qinghua</given-names>
</name>
<xref ref-type="aff" rid="aff0001">a</xref>
</contrib>
<contrib contrib-type="author" id="au0007">
<name>
<surname>Hu</surname>
<given-names>Hua</given-names>
</name>
<xref ref-type="aff" rid="aff0003">c</xref>
<xref ref-type="aff" rid="aff0005">e</xref>
</contrib>
<contrib contrib-type="author" id="au0008">
<name>
<surname>Li</surname>
<given-names>Min</given-names>
</name>
<xref ref-type="aff" rid="aff0003">c</xref>
<xref ref-type="aff" rid="aff0008">h</xref>
</contrib>
<contrib contrib-type="author" id="au0009">
<name>
<surname>Jiménez-Mesa</surname>
<given-names>Carmen</given-names>
</name>
<xref ref-type="aff" rid="aff0009">i</xref>
</contrib>
<contrib contrib-type="author" id="au0010">
<name>
<surname>Ramirez</surname>
<given-names>Javier</given-names>
</name>
<xref ref-type="aff" rid="aff0009">i</xref>
</contrib>
<contrib contrib-type="author" id="au0011">
<name>
<surname>Martinez</surname>
<given-names>Francisco J.</given-names>
</name>
<xref ref-type="aff" rid="aff0009">i</xref>
</contrib>
<contrib contrib-type="author" id="au0012">
<name>
<surname>Gorriz</surname>
<given-names>Juan Manuel</given-names>
</name>
<xref ref-type="aff" rid="aff0009">i</xref>
<xref ref-type="aff" rid="aff0010">j</xref>
</contrib>
<aff id="aff0001"><label>a</label>School of Informatics, University of Leicester, Leicester, LE1 7RH, Leicestershire, UK</aff>
<aff id="aff0002"><label>b</label>Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia</aff>
<aff id="aff0003"><label>c</label>Department of Psychiatry, Columbia University, USA</aff>
<aff id="aff0004"><label>d</label>New York State Psychiatric Institute, New York, NY 10032, USA</aff>
<aff id="aff0005"><label>e</label>Department of Neurology, The Second Affiliated Hospital of Soochow University, China</aff>
<aff id="aff0006"><label>f</label>School of Architecture Building and Civil engineering, Loughborough University, Loughborough, LE11 3TU, UK</aff>
<aff id="aff0007"><label>g</label>School of Mathematics and Actuarial Science, University of Leicester, LE1 7RH, UK</aff>
<aff id="aff0008"><label>h</label>School of Internet of Things, Hohai University, Changzhou, China</aff>
<aff id="aff0009"><label>i</label>Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain</aff>
<aff id="aff0010"><label>j</label>Department of Psychiatry, University of Cambridge, Cambridge CB21TN, UK</aff>
</contrib-group>
<author-notes>
<corresp id="cor0001"><label>⁎</label>Corresponding author.</corresp>
</author-notes>
<pub-date pub-type="pmc-release">
<day>17</day>
<month>7</month>
<year>2020</year>
</pub-date>
<!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
<pub-date pub-type="ppub">
<month>12</month>
<year>2020</year>
</pub-date>
<pub-date pub-type="epub">
<day>17</day>
<month>7</month>
<year>2020</year>
</pub-date>
<volume>64</volume>
<fpage>149</fpage>
<lpage>187</lpage>
<history>
<date date-type="received">
<day>30</day>
<month>4</month>
<year>2020</year>
</date>
<date date-type="rev-recd">
<day>6</day>
<month>7</month>
<year>2020</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>7</month>
<year>2020</year>
</date>
</history>
<permissions>
<copyright-statement>© 2020 Elsevier B.V. All rights reserved.</copyright-statement>
<copyright-year>2020</copyright-year>
<copyright-holder>Elsevier B.V.</copyright-holder>
<license>
<license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
</license>
</permissions>
<abstract abstract-type="author-highlights" id="abs0001">
<title>Highlights</title>
<p>
<list id="celist0001" list-type="simple">
<list-item id="celistitem0001">
<label>•</label>
<p id="para0001">We analysed over 450 references from all well-famed databases.</p>
</list-item>
<list-item id="celistitem0002">
<label>•</label>
<p id="para0002">We provided a comprehensive survey on multimodal data fusion in neuroimaging.</p>
</list-item>
<list-item id="celistitem0003">
<label>•</label>
<p id="para0003">This review encompassed current challenges &amp; applications, strengths &amp;limitations.</p>
</list-item>
<list-item id="celistitem0004">
<label>•</label>
<p id="para0004">Fundamental fusion rules, and fusion quality assessment methods were reviewed.</p>
</list-item>
<list-item id="celistitem0005">
<label>•</label>
<p id="para0005">Atlas-based fusion segmentation, quantification, &amp; applications were reviewed.</p>
</list-item>
</list>
</p>
</abstract>
<abstract id="abs0002">
<p>Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamental limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and various sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread education and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging.</p>
</abstract>
<kwd-group id="keys0001">
<title>Keywords</title>
<kwd>Multimodal data fusion</kwd>
<kwd>Neuroimaging</kwd>
<kwd>Magnetic resonance imaging</kwd>
<kwd>PET</kwd>
<kwd>SPECT</kwd>
<kwd>Fusion rules</kwd>
<kwd>Assessment</kwd>
<kwd>Applications</kwd>
<kwd>Partial volume effect</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="sec0001">
<label>1</label>
<title>Introduction</title>
<p id="para0006">Neuroimaging has been playing pivotal roles in clinical diagnosis and basic biomedical research in the past decades. As described in the following section, the most widely used imaging modalities are magnetic resonance imaging (MRI), computerized tomography (CT), positron emission tomography (PET), and single-photon emission computed tomography (SPECT). Among them, MRI itself is a non-radioactive, non-invasive, and versatile technique that has derived many unique imaging modalities, such as diffusion-weighted imaging, diffusion tensor imaging, susceptibility-weighted imaging, and spectroscopic imaging. PET is also versatile, as it may use different radiotracers to target different molecules or to trace different biologic pathways of the receptors in the body.</p>
<p id="para0007">Therefore, these individual imaging modalities (the use of one imaging modality), with their characteristics in signal sources, energy levels, spatial resolutions, and temporal resolutions, provide complementary information on anatomical structure, pathophysiology, metabolism, structural connectivity, functional connectivity, etc. Over the past decades, everlasting efforts have been made in developing individual modalities and improving their technical performance. Directions of improvements include data acquisition and data processing aspects to increase spatial and/or temporal resolutions, improve signal-to-noise ratio and contrast to noise ratio, and reduce scan time. On application aspects, individual modalities have been widely used to meet clinical and scientific challenges. At the same time, technical developments and biomedical applications of the concert, integrated use of multiple neuroimaging modalities are trending up in both research and clinical institutions. The driving force of this trend is twofold. First, all individual modalities have their limitations. For example, some lesions in MS can appear normal in T1-weighted or T2-weighted MR images but show pathological changes in DWI or SWI images <xref ref-type="bibr" rid="bib0001">[1]</xref>. Second, a disease, disorder, or lesion may manifest itself in different forms, symptoms, or etiology; or on the other hand, different diseases may share some common symptoms or appearances <xref ref-type="bibr" rid="bib0002">[2,</xref>
<xref ref-type="bibr" rid="bib0003">3]</xref>. Therefore, an individual image modality may not be able to reveal a complete picture of the disease; and multimodal imaging modality (the use of multiple imaging modalities) may lead to a more comprehensive understanding, identify factors, and develop biomarkers of the disease.</p>
<p id="para0008">In the narrow sense, a multimodal imaging study would mean the use of multiple imaging devices such as PET and MRI scanners, different imaging modes such as structural MRI, diffusion-weighted imaging, and magnetic resonance spectroscopy, or even different contract mechanism such as with or without contract agents in a single examination or experiment of a subject. This practice has been widely used in clinical diagnosis and medical research. For example, a routine protocol of MRI examination of a stroke patient may include T1-weighted, T1-weighted high-resolution structural MRI scans, diffusion-weighted imaging, SWI, etc <xref ref-type="bibr" rid="bib0004">[4,</xref>
<xref ref-type="bibr" rid="bib0005">5]</xref>. A protocol of an MRI study of a psychiatric disorder may contain a combination of structural MRI, functional MRI, MR spectroscopic imaging, etc <xref ref-type="bibr" rid="bib0006">[6,</xref>
<xref ref-type="bibr" rid="bib0007">7]</xref>.</p>
<p id="para0009">In the broad sense, a multimodal imaging study may mean the use of multimodal imaging data obtained separately, from different subjects, and/or from different clinical or research sites. This practice offers the advantages of large and diverse datasets. However, it also comes with challenges of sophisticated models, complicated data normalization (that includes correction of errors and variations imbedded in data from different institutions), data fusion, and data integration <xref ref-type="bibr" rid="bib0008">[8,</xref>
<xref ref-type="bibr" rid="bib0009">9]</xref>.</p>
<p id="para0010">In recent years, the quantity of peer-reviewed journal articles on neuroimaging has been increasing steadily. A database (PubMed) query using the keywords in titles of “neuroimaging” OR “brain imaging” returned more than 39,000 articles from 2010 to the present time when this paper was drafted in Feb 2020 (<xref ref-type="fig" rid="fig0001">Fig. 1</xref>
). These publications include not only applications of multimodal neuroimaging in clinical examinations and biomedical research but also methodological studies in imaging processing and fusion of multimodal neuroimaging.<fig id="fig0001"><label>Fig. 1</label><caption><p>Numbers of peer-reviewed papers with the keywords of “neuroimaging” or “brain imaging” in titles (the numbers and the bar graph were generated by PubMed in Feb 2020).</p></caption><alt-text id="alt0001">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"></graphic></fig></p>
<p id="para0011">Therefore, this present paper will focus on the following two main aspects: (1) we will review some of the recent, typical papers that exhibit the strength and limitations of the neuroimaging modalities and the corresponding analysis methods, and in particular, the needs for improved image fusion methods and (2) we will review recent methodological development in data preprocessing and data fusion in multimodal neuroimaging. We note that although we tried to cover all neuroimaging modalities, we inevitably paid more attention to MRI modalities. This is not only due to the most practical application and versatility of the MRI but also due to the limitations of our expertise. <xref ref-type="fig" rid="fig0002">Fig. 2</xref>
shows the taxonomy of this review.<fig id="fig0002"><label>Fig. 2</label><caption><p>Taxonomy of this review.</p></caption><alt-text id="alt0002">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"></graphic></fig></p>
<p id="para0012">The main contents of the paper are organized as follows. Chapter 2 will give a brief introduction to neuroimaging, and challenges of multimodal imaging; Chapter 3 introduces the commonly used neuroimaging modalities, which include computerized tomography, positron emission tomography, single-proton emission computed tomography, and magnetic resonance imaging, which has many modalities in its own right. For each modality, we will concisely describe its signal source, energy level, spatial resolution, temporal resolution, and major applications; Chapter 4 describe applications of neuroimaging in three major areas: the developing brains, the degenerative brains, and mental disorders. In each part, we will first briefly describe what the clinical and/or biomedical problems are, we then review recent papers on how neuroimaging has been used to address these problems, and we point out what the unmet needs and challenges;</p>
<p id="para0013">Chapters 5 to 9 are devoted to the multimodal neuroimaging fusion, covering some important procedures in data fusion. The topics are not necessarily complete and their order of presentation is not necessarily coherent with the pipeline of fusion processing. Chapter 5 reviews the fundamental methods, which covers types, rules, atlas-based segmentation, decomposition, reconstruction, and quantification; Chapter 6 reviews subjective and objective assessment of data fusion in multimodal neuroimaging; Chapter 7 reviews the advantages of data fusion in improving the spatial/temporal resolution, distortion correction, and contrast; it also reviews the benefits of these advantages in fusing structural and functional images; Chapter 8 reviews atlas-based segmentations in multimodal imaging fusion; Chapter 9 reviews the quantification in multimodal neuroimaging fusion. While the focus of this part is given to PET and SPECT, some of the approaches and principles discussed here, such as partial volume correction and attenuation (relaxation), can be applied to quantitative MRI modalities, such as DTI, ASL, quantitative susceptibility mapping (QSM), etc. Chapter 10 concludes the paper.</p>
</sec>
<sec id="sec0002">
<label>2</label>
<title>Multimodal imaging data fusion: challenges in neuroimaging</title>
<p id="para0014">In this part, we will review the current challenges of neuroimaging, including limited spatial/temporal resolution, lack of quantification, and imaging distortions. These challenges often create fundamental limitations on individual modalities of neuroimaging, while some challenges also exist in current multi-modal neuroimaging. This part will mainly cover the challenges of individual neuroimaging modalities that led to the development and ongoing research of multimodal neuroimaging methods.</p>
<sec id="sec0003">
<label>2.1</label>
<title>Individual modality imaging</title>
<p id="para0015">Neuroimaging can be divided into structural imaging and functional imaging according to the imaging mode. Structural imaging is used to show the structure of the brain to aid the diagnosis of some brain diseases, such as brain tumors or brain trauma. Functional imaging is used to show how the brain metabolizes while carrying out certain tasks, including sensory, motor, and cognitive functions. Functional imaging is mainly used in neuroscience and psychological research, but it is gradually becoming a new way of clinical-neurological diagnosis <xref ref-type="bibr" rid="bib0010">[10]</xref>.</p>
<p id="para0016">The amount of information obtainable through single-mode imaging is limited and often cannot reflect the complex specificity of organisms. For instance, although CT imaging is effective in identifying normal structures and abnormal diseased tissues according to their density and thus can provide clear anatomical structure information, it cannot image soft tissue well. Generally speaking, MRI imaging has good soft-tissue contrast resolution for most sequences, but its display of bone structure is relatively poor. PET imaging and SPECT imaging are not limited by the detection depth, with high imaging sensitivity, and are easy to quantify, but their spatial resolutions are low <xref ref-type="bibr" rid="bib0011">[11]</xref>. Optical imaging refers to the detection of fluorescence or bioluminescent dyes using the principle of light emission. This technique has high sensitivity, no radioactivity, good specificity, and low cost. Optical imaging allows dynamical monitoring of the replication process of virus bacteria in organisms. However, it has low spatial resolution and limited imaging depth <xref ref-type="bibr" rid="bib0012">[12,</xref>
<xref ref-type="bibr" rid="bib0013">13]</xref>. Therefore, it can be observed that various imaging technologies all have both benefits and drawbacks, as shown in <xref ref-type="fig" rid="fig0003">Fig. 3</xref>
, and it is difficult to provide comprehensive and accurate information through utilizing individual modality imaging.<fig id="fig0003"><label>Fig. 3</label><caption><p>Advantages and disadvantages of various individual modality imaging.</p></caption><alt-text id="alt0003">Fig. 3</alt-text><graphic xlink:href="gr3_lrg"></graphic></fig></p>
</sec>
<sec id="sec0004">
<label>2.2</label>
<title>Low spatial/temporal resolution</title>
<p id="para0017">The nowadays most commonly used noninvasive functional imaging methods and their spatial and temporal range are illustrated in <xref ref-type="fig" rid="fig0004">Fig. 4</xref>
. It can be distinctly observed that among these most advanced methods, functional MRI (fMRI) reaches the highest range of spatial resolution. fMRI can assess the whole brain and image the hemodynamic processes at the layered and columnar levels of the human cortex, under the condition of a high-intensity magnetic field (i.e., submillimeter level) <xref ref-type="bibr" rid="bib0014">[14]</xref>. However, it has a relatively lower temporal resolution in terms of imaging the neuronal population dynamics. Electroencephalogram (EEG) and Magnetoencephalography (MEG) can both measure electromagnetic changes in the scale of milliseconds. However, their spatial resolution/uncertainty is more than several millimeters <xref ref-type="bibr" rid="bib0015">[15,</xref>
<xref ref-type="bibr" rid="bib0016">16]</xref>. The microscopic level of neuroscience is often beyond the reach of noninvasive imaging techniques due to the requirement of high spatial or temporal resolution.<fig id="fig0004"><label>Fig. 4</label><caption><p>Functional neuroimaging modalities.</p></caption><alt-text id="alt0004">Fig. 4</alt-text><graphic xlink:href="gr4_lrg"></graphic></fig></p>
</sec>
<sec id="sec0005">
<label>2.3</label>
<title>Non-quantitative</title>
<p id="para0018">The majority of imaging modalities are non-quantitative and have to gain complement information from other data. This additional information allows the normalization of signals, acquirement of absolute units, and inter-subject comparison. As an example, the fMRI signal is a measure of neuronal activity incited hemodynamic changes caused by a combination of complex physical and physiological processes. In different subjects or brain areas, the same level of neuronal activity can evoke different corresponding fMRI signals. As a consequence, fMRI signals can only be considered as roughly proportional to the activity of neurons. Four years later, in 2008, the studies from Ances have found that the cerebral blood flow (CBF) is relevant to fMRI signal variations in individual's brain regions, patients’ age groups, and health conditions. This broad relevance brings fMRI signal high sensitivity <xref ref-type="bibr" rid="bib0017">[17]</xref>. Various approaches have been proposed to explain the ensuing sensitivity differences. Amongst these approaches, the so-called calibrated BOLD approach proposed and improved by Blockley, Chiarelli and Hoge over the years has been the most widely used <xref ref-type="bibr" rid="bib0018">[18]</xref>, <xref ref-type="bibr" rid="bib0019">[19]</xref>, <xref ref-type="bibr" rid="bib0020">[20]</xref>. However, the more reliable absolute quantitative results of CBF with improved spatial and temporal resolutions are provided by the Arterial spin labeling (ASL) technique by the UMICH fMRI lab <xref ref-type="bibr" rid="bib0021">[21]</xref>.</p>
</sec>
<sec id="sec0006">
<label>2.4</label>
<title>Distortion</title>
<p id="para0019">Some neuroimaging modalities are prone to geometric distortions. Echo‐planar imaging (EPI) is a fast imaging approach that could obtain the complete <italic>k</italic>‐space data set just in a single acquisition. Due to its unmatched acquisition speed, it has revolutionized the field of neuroimaging and has served as the standard readout module for most fMRI and dMRI acquisition. Nevertheless, EPI suffers from distortion and intensity loss mainly caused by field inhomogeneities, leading to relatively poor image quality <xref ref-type="bibr" rid="bib0022">[22]</xref>. In general, the imperfection of equipment may result in information loss, noise amplification and artifacts, resulting in the distortion of images.</p>
<p id="para0020">To conclude, in practical application, utilizing individual modality imaging often has limitations, such as low sensitivity and specificity, low spatial/temporal/contrast resolution, distortion and so on. Because of these deficiencies, we need to introduce the usage of multimodal neuroimaging to eliminate those shortcomings in some degree.</p>
</sec>
</sec>
<sec id="sec0007">
<label>3</label>
<title>Multimodal imaging data fusion: imaging technologies</title>
<p id="para0021">Neuroimaging, more commonly known as brain imaging, is referred to as different types of technologies to display the function, pathology, and structure of the nervous system. There are mainly two types of neuroimaging: the functional imaging that directly or indirectly visualizes the processing of information by the central neural system of the brain and the structural imaging that shows the structure information of the brain. Neuroimaging is used for a patient who is found a neurological disorder by a physician to have a more in-depth investigation. There are different types of imaging modalities, such as Magnetic resonance imaging (MRI), Single-Photon Emission Computed Tomography, Computerized Tomography, Positron Emission Tomography, Pneumoencephalography, Functional Magnetic Resonance Imaging (fMRI). According to the types of possible diseases, patients will be investigated by different methods.</p>
<sec id="sec0008">
<label>3.1</label>
<title>Computerized tomography</title>
<p id="para0022">Computerized Tomography (CT), also known as computerized x-ray imaging, combines a series of X-ray signals obtained from multiple angles around the body and creates cross-sectional images by computer processing. Different from the conventional X-ray <xref ref-type="bibr" rid="bib0023">[23]</xref> that uses a fixed X-ray tube, the CT scanner uses a motorized x-ray source, which rotates around a gantry, a circular frame in a donut-shaped structure. CT scan images, therefore, can provide more information than conventional X-rays. CT can be recommended for disease or injury of various parts of the body, such as lesions or tumors of the abdomen, different types of heart disease, injuries, tumors or clots of the head <xref ref-type="bibr" rid="bib0024">[24]</xref>, <xref ref-type="bibr" rid="bib0025">[25]</xref>, <xref ref-type="bibr" rid="bib0026">[26]</xref>. <xref ref-type="fig" rid="fig0005">Fig. 5</xref>
<xref ref-type="bibr" rid="bib0027">[27]</xref> shows an example of a CT scan of a brain.<fig id="fig0005"><label>Fig. 5</label><caption><p>An example of a CT brain scan.</p></caption><alt-text id="alt0005">Fig. 5</alt-text><graphic xlink:href="gr5_lrg"></graphic></fig></p>
</sec>
<sec id="sec0009">
<label>3.2</label>
<title>Positron emission tomography</title>
<p id="para0023">Positron Emission Tomography, shortened as PET, is a combination of nuclear medicine and biochemical analysis that is mostly used for the diagnosis of brain or heart conditions and cancer. Instead of detecting the amount of a radioactive substance existing in body tissues of a specific location to check the tissue's function, PET detects the biochemical changes within body tissues. The biochemical changes can reveal the onset of a disease process before other imaging processes can visualize the anatomical changes related to the disease. During PET studies, only a tiny amount of radioactive substance is needed for the examination of targeted tissues. PET scans not only can be used to detect the presence of disease or other conditions of organs or tissues but can also be used to evaluate the function of organs, like the heart or the brain. The most common application of PET scan is cancer detection and treatment. <xref ref-type="fig" rid="fig0006">Fig. 6</xref>
(a) shows a PET image of the brain, and <xref ref-type="fig" rid="fig0006">Fig. 6</xref>(b) shows a PET scan of the kidney <xref ref-type="bibr" rid="bib0027">[27]</xref>.<fig id="fig0006"><label>Fig. 6</label><caption><p>Common scan modalities.</p></caption><alt-text id="alt0006">Fig. 6</alt-text><graphic xlink:href="gr6_lrg"></graphic></fig></p>
</sec>
<sec id="sec0010">
<label>3.3</label>
<title>Single-photon emission computed tomography</title>
<p id="para0024">Single-photon emission computed tomography, commonly known as SPECT, uses gamma rays as the tracer to detect blood flow in organs or tissue. Therefore, a gamma-emitting radioisotope, such as isotope gallium, should be injected into the bloodstream of the patient for SPECT. The computer collects the gamma rays from the tracer and shows it on the CT cross-section. It bears similarity with the traditional nuclear medicine planar imaging but provides 3D information as multiple images of cross-sectional slices through the patient. The information can be manipulated or reformatted freely according to diagnostic or research requirements. Besides detecting blood flow, SPECT scanning is also applied for the presurgical evaluation of medically controlled seizures. <xref ref-type="fig" rid="fig0006">Fig. 6</xref>(c) <xref ref-type="bibr" rid="bib0027">[27]</xref> shows a SPECT scan of the brain</p>
</sec>
<sec id="sec0011">
<label>3.4</label>
<title>Magnetic resonance imaging</title>
<p id="para0025">Magnetic resonance imaging (MRI) utilizes strong magnetic field, magnetic field gradients, and radio waves to generate pictures of the anatomy and the physiological processes of the body <xref ref-type="bibr" rid="bib0028">[28]</xref>. Different from PET or CT, MRI does not need the injection of ionizing radioisotopes or involve X-rays. As all radiation instances can cause ionization that leads to cancer, MRI, without exposing the body to radiation, becomes a better choice than CT and one of the safest medical procedures. MRI is widely used in hospitals and clinics for the medical diagnosis of different body regions, including the brain, spinal cord, bones and joints, breasts, heart and blood vessels, and other internal organs, such as the liver, womb or prostate gland <xref ref-type="bibr" rid="bib0029">[29]</xref>, <xref ref-type="bibr" rid="bib0030">[30]</xref>, <xref ref-type="bibr" rid="bib0031">[31]</xref>. Besides, MRI can also be used for non-living objects <xref ref-type="bibr" rid="bib0032">[32]</xref>. <xref ref-type="fig" rid="fig0006">Fig. 6</xref>(d) shows an MRI brain image <xref ref-type="bibr" rid="bib0033">[33]</xref>.</p>
<sec id="sec0012">
<label>3.4.1</label>
<title>T1, T2, and proton density</title>
<p id="para0026">T1, T2 and Proton density (PD) are three basic types of MRI imaging. T1, T2 and PD, which all vary with sequence parameters, can simultaneously determine the contrast of the MR images <xref ref-type="bibr" rid="bib0034">[34]</xref>. Via selecting different pulse sequences with different timings, we can decide the contrast in the region being imaged. There are also other types of sequences, such as fluid attenuates inversion recovery (FLAIR) and short tau inversion recovery (STIR). In this section, we will only mention the three main types. <xref ref-type="fig" rid="fig0007">Fig. 7</xref>
shows the relationship between <italic>M<sub>z</sub></italic> and <italic>M<sub>xy</sub></italic>, of which <italic>xy</italic> refers to plane <xref ref-type="bibr" rid="bib0035">[35]</xref>.<fig id="fig0007"><label>Fig. 7</label><caption><p>The relationship between <italic>M<sub>z</sub></italic> and <italic>M<sub>xy</sub></italic>.</p></caption><alt-text id="alt0007">Fig. 7</alt-text><graphic xlink:href="gr7_lrg"></graphic></fig></p>
<p id="para0027">Higher <italic>M<sub>z</sub></italic> at the time of applying the 90° RF pulse brings the larger transverse signal (<italic>M<sub>xy</sub></italic>). The time to repetition (TR) is defined as the determent of the length of time between 90° RF pulses. The Echo time <xref ref-type="bibr" rid="bib0026">[26]</xref> is defined as the time between the excitation pulse and the peak of the signal.</p>
<p id="para0028">T1, the longitudinal relaxation time, is defined as a time constant that stands for the magnetization to recover from 0 to 63% of their maximum <italic>M<sub>z</sub></italic> in a static magnetic field. T1 values of hydrogen nuclei are different for different molecules and different tissues. T1 relaxation is defined as the recovery of the longitudinal magnetization. T1 is commonly applied for detecting fatty tissue, general morphological information, and characterizing focal liver lesions.</p>
<p id="para0029">T2, the transverse relaxation time, is defined as the time for transverse magnetization <italic>M<sub>xy</sub></italic> to decay to 37% of its initial <italic>M<sub>xy</sub></italic>. Similar to T1 values, we also have different T2 values of Hydrogen nuclei in different molecules and different tissues. T2 weighted imaging is suitable for revealing cerebral white matter lesions and assessing edema, inflammation and zonal anatomy in the prostate and uterus.</p>
<p id="para0030">Different from T1 and T2, which mainly focuses on the magnetic characteristics of the hydrogen nuclei, PD is more related to the number of nuclei in the region being imaged. PD weighted images are obtained by a short echo time and a long repetition time, which can provide a more apparent distinction between the gray matter and white matter. PD weighted imaging is specifically useful for detecting joint disease and injury. <xref ref-type="fig" rid="fig0008">Fig. 8</xref>
(a) and (b) show a T1 weighted brain image and T2 weighted brain image, respectively <xref ref-type="bibr" rid="bib0027">[27]</xref>.<fig id="fig0008"><label>Fig. 8</label><caption><p>Two types of weighted images of the same brain.</p></caption><alt-text id="alt0008">Fig. 8</alt-text><graphic xlink:href="gr8_lrg"></graphic></fig></p>
</sec>
<sec id="sec0013">
<label>3.4.2</label>
<title>Functional magnetic resonance imaging</title>
<p id="para0031">Functional magnetic resonance imaging or functional MRI (fMRI) measures brain activity by detecting changes associated with blood flow. As it has been proved that when an area of the brain is in use, the blood flow to that area increases, which means that the neuronal activation and cerebral blood flow are matched. fMRI is a particular type of imaging technology used to map the neuron activities in the spinal cord and brain of humans or animals by visualizing the change in blood flow, which is related to the energy use by brain cells. fMRI also includes resting-state fMRI <xref ref-type="bibr" rid="bib0036">[36]</xref> or task-less fMRI, which can provide subjects’ baseline BOLD service <xref ref-type="bibr" rid="bib0037">[37]</xref>.</p>
</sec>
<sec id="sec0014">
<label>3.4.3</label>
<title>Diffusion weighted/tensor imaging</title>
<p id="para0032">Diffusion Weighted/Tensor Imaging (DWI) generates image contrast from the differences in the magnitude of diffusion of water molecules within the brain. Diffusion in biology is defined as the passive movement of molecules from a higher concentration region to a lower concentration region, which is also known as Brownian motion <xref ref-type="bibr" rid="bib0038">[38]</xref>. Diffusion within the brain is affected by many factors, such as temperature, type of molecule under investigation, and the microenvironmental architecture in which the diffusion takes place. Based on the MRI sequences of which diffusion is sensitive to, the image contrast can be generated according to the difference in diffusion rates. DWI is highly effective for the early diagnosis of ischemic tissue injury, even before the pathology can be shown by the traditional MR sequence. Therefore, DWI provides the time window for tissue salvaging interventions.</p>
</sec>
<sec id="sec0015">
<label>3.4.4</label>
<title>Perfusion and susceptibility weighted imaging</title>
<p id="para0033">Perfusion weight imaging (PWI) is defined as a variety of MRI techniques that are able to provide insights into the perfusion of tissues by blood <xref ref-type="bibr" rid="bib0039">[39]</xref>. PWI can be used for the evaluation of ischaemic conditions, neoplasms, and neurodegenerative diseases. Perfusion MRI mainly has three main techniques: Dynamic susceptibility contrast (DSC), Dynamic contrast-enhanced (DCE), and Arterial spin labeling (ASL).</p>
<p id="para0034">Susceptibility weighted imaging (SWI), previously known as BOLD venographic imaging, is a type of MRI sequence that is extremely sensitive to venous blood, hemorrhage, and iron storage. As an fMRI technique, SWI can explore the susceptibility differences between tissues and detect differences based on the phase image. An enhanced contrast magnitude image can be obtained by combing the magnitude and phase data. SWI is commonly used in traumatic brain injuries (TBI) and high-resolution brain venographies as it is sensitive to venous blood.</p>
</sec>
<sec id="sec0016">
<label>3.4.5</label>
<title>Magnetic resonance fingerprinting</title>
<p id="para0035">Magnetic resonance fingerprinting (MRF) <xref ref-type="bibr" rid="bib0040">[40]</xref> is new MRI technique that integrates MR physics theory and computer pattern recognition technology and realizes fast and multi-parameter parallel quantization imaging. The technique consists of three modules. First, the fingerprinting signals are excited and acquired from the subject in the MR scanner by the pseudorandom temporal varied pulse sequence to reflect the physiological property of tissue. Second, the evolution of fingerprinting signals with different physiological parameter combinations are predicted by the computer simulation using the Bloch equation; and a fingerprint dictionary indexed by the quantized parameters is constructed. Finally, the pattern recognition technology is applied to find the matched fingerprinting entries for the measured fingerprinting signals, so as to obtain the corresponding quantization parameters and realize quantization MR imaging. Different from most of the conventional MRI modalities, which provide qualitative contrast-based images that are determined not only by the tissue properties but also by experimental conditions, MRF provides quantitative images of tissue properties that reflects pathological conditions of the subject. <xref ref-type="fig" rid="fig0009">Fig. 9</xref>
shows digital phantom experiments of conventional MRI and MRF. The upper row shows a digital brain with T1 values (left), T1-weighted MRIs with different experimental parameters (middle), and MRF reconstructed T1 map (right), respectively; The lower row shows a digital brain with T2 values (left), T2-weighted MRIs with different experimental parameters (middle), and MRF reconstructed T2 map, respectively. The experiments demonstrated that the contrasts of the conventionally “weighted” MRI images depend on both the tissue properties and experimental parameters, but MRF can reconstruct the parameter images independent of experimental parameters.<fig id="fig0009"><label>Fig. 9</label><caption><p>Digital phantom experiments of conventional T1 weighted and T2 weighted MRIs, and MRF.</p></caption><alt-text id="alt0009">Fig. 9</alt-text><graphic xlink:href="gr9_lrg"></graphic></fig></p>
<p id="para0036">Currently, the applications of MRF have been limited to biomedical research and the fusion of MRF with other neuroimaging modalities has not been reported. Given its parametric and quantitative features, the MRF technique will play an important role not only in neuroimaging but also in fusion of multimodal neuroimaging.</p>
</sec>
</sec>
<sec id="sec0017">
<label>3.5</label>
<title>Comparison of imaging methods</title>
<p id="para0037">
<xref ref-type="table" rid="tbl0001">Table 1</xref>
lists the main advantages, disadvantages and applications of each neuroimaging technology.<table-wrap id="tbl0001" position="float"><label>Table 1</label><caption><p>Comparison of various imaging methods.</p></caption><alt-text id="alt0025">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Imaging methods</th><th valign="top">Advantages</th><th valign="top">Disadvantages</th><th valign="top">Applications</th></tr></thead><tbody><tr><td valign="top">Computerized Tomography (CT)</td><td valign="top">• Painless, noninvasive and accurate</td><td valign="top">• Radiation</td><td valign="top">• Brain tumors.</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Blood clots and blood</td></tr><tr><td valign="top"></td><td valign="top">• Image bone, soft tissue</td><td valign="top"></td><td valign="top">vessel defects. enlarged ventricles</td></tr><tr><td valign="top"></td><td valign="top">and blood vessels all at the same time</td><td valign="top"></td><td valign="top"></td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top">• Not recommended for pregnant women</td><td valign="top">• Abnormalities in the</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">nerves or muscles of the eye</td></tr><tr><td valign="top"></td><td valign="top">• Fast and simple</td><td valign="top"></td><td valign="top"></td></tr><tr><td colspan="4"><hr/></td></tr><tr><td valign="top">Positron Emission Tomography (PET)</td><td valign="top">• Double the diagnostic clarity compared to CT</td><td valign="top">• Not recommended for pregnant women</td><td valign="top">• Cancer</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Heart disease</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top">• Diabetics require certain precautions.</td><td valign="top">• Brain disorders</td></tr><tr><td valign="top"></td><td valign="top">• Easy<bold>,</bold>Nondisruptive</td><td valign="top"></td><td valign="top"></td></tr><tr><td colspan="4"><hr/></td></tr><tr><td valign="top">Single-photon Emission Computed Tomography (SPET)</td><td valign="top">• More available and widely used</td><td valign="top">• Long scan times</td><td valign="top">• Functional brain imaging</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top">• Low-resolution and prone to artifacts and attenuation</td><td valign="top"></td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Functional cardiac imaging</td></tr><tr><td valign="top"></td><td valign="top">• Less expensive than PET</td><td valign="top"></td><td valign="top"></td></tr><tr><td colspan="4"><hr/></td></tr><tr><td valign="top">Magnetic Resonance Imaging (MRI)</td><td valign="top">• No radiation</td><td valign="top">• Expensive</td><td valign="top">• Anomalies of the brain and spinal cord</td></tr><tr><td valign="top"></td><td valign="top">• Apparent, detailed images of soft-tissue structures compared to other imaging techniques</td><td valign="top">• Cannot find all cancers</td><td valign="top"></td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Tumors, cysts, and other anomalies in various parts of the body</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top">• Cannot always distinguish between malignant or benign tumors</td><td valign="top"></td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Breast cancer screening for women who face a high risk of breast cancer</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Injuries or abnormalities of the joints, such as the back and knee</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Certain types of heart conditions</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Diseases of the liver and other abdominal organs</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• The evaluation of pelvic pain in women, with causes including fibroids and endometriosis</td></tr><tr><td valign="top"></td><td valign="top"></td><td valign="top"></td><td valign="top">• Suspected uterine anomalies in women undergoing infertility evaluation</td></tr></tbody></table></table-wrap></p>
</sec>
<sec id="sec0018">
<label>3.6</label>
<title>Databases</title>
<p id="para0038">In this section, we listed some public databases, as shown in <xref ref-type="table" rid="tbl0002">Table 2</xref>
. The International Cat Association (TICA) database is an extensive database that contains different types of medical images of cancers, including lung cancer, breast cancer, and kidney cancer. ATLAS is a public database of Harvard University, which mainly contains image data of Cerebrovascular Disease, Neoplastic Disease, Degenerative Disease, Inflammatory or Infectious Disease. CTisus has numerous MRI, CT, X-rays of different organs and tissues. The Open Access Series of Imaging Studies (OASIS) dataset contains 2000 MR sessions, which includes: T1 weighted image, T2 weighted image, FLAIR, ASL, SWI, time of flight, resting-state BOLD and DTI sequences, PET images from three types of traces, PIB, AV45 and FDG. The Alzheimer's Disease Neuroimaging Initiative (ADNI) database contains several types of data like MR, PET from a group of volunteers and dementia patients. The Federal Interagency Traumatic Brain Injury Research (FITBIR) shares the data for Traumatic Brain Injury (TBI) research.<table-wrap id="tbl0002" position="float"><label>Table 2</label><caption><p>Public datasets.</p></caption><alt-text id="alt0026">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Database Name</th><th valign="top">Web Address</th></tr></thead><tbody><tr><td valign="top">TCIA</td><td valign="top"><ext-link ext-link-type="uri" id="interref0001" xlink:href="http://www.cancerimagingarchive.net/">http://www.cancerimagingarchive.net/</ext-link></td></tr><tr><td valign="top">ATLAS</td><td valign="top"><ext-link ext-link-type="uri" id="interref0002" xlink:href="http://www.med.harvard.edu/aanlib/home.html">http://www.med.harvard.edu/aanlib/home.html</ext-link></td></tr><tr><td valign="top">CTisus</td><td valign="top"><ext-link ext-link-type="uri" id="interref0003" xlink:href="http://www.ctisus.com/">http://www.ctisus.com/</ext-link></td></tr><tr><td valign="top">OASIS</td><td valign="top"><ext-link ext-link-type="uri" id="interref0004" xlink:href="https://www.oasis-brains.org/">https://www.oasis-brains.org/</ext-link></td></tr><tr><td valign="top">ADNI</td><td valign="top"><ext-link ext-link-type="uri" id="interref0005" xlink:href="http://adni.loni.usc.edu/">http://adni.loni.usc.edu/</ext-link></td></tr><tr><td valign="top">FITBIR</td><td valign="top"><ext-link ext-link-type="uri" id="interref0006" xlink:href="https://fitbir.nih.gov/">https://fitbir.nih.gov/</ext-link></td></tr></tbody></table></table-wrap></p>
</sec>
</sec>
<sec id="sec0019">
<label>4</label>
<title>Multimodal imaging data fusion: diseases</title>
<p id="para0039">In this part, we will review recent advancements in the application of multimodal neuroimaging in some clinical and research areas such as early brain development, neurodegenerative diseases, psychiatric disorders, and neurological diseases. It is not our intention to cover all aspects or provide a complete review of these areas. Instead, we focus on the aspects related to the development and applications of the multimodal neuroimaging techniques that meet the expectations and challenges of biomedicine. As such, each of the areas will begin with a brief description of background information such as clinical features, pathology, diagnosis, treatment of the diseases; then a general introduction of the roles, applications, and current status of the medical imaging techniques to the disease; the major part will be a review of recent papers that used one or more imaging modalities and used image fusion in multiple imaging modalities.</p>
<sec id="sec0020">
<label>4.1</label>
<title>Developing brains</title>
<p id="para0040">Recent studies show that the human brain experiences a rapid development in the first eight years and continues to develop and change into adulthood. During this long period, the brain develops in size, neuroanatomy, and functions. This period is significant for a person's physical and mental health, intellectual and emotional development, and learning, working, and life success <xref ref-type="bibr" rid="bib0041">[41]</xref>, <xref ref-type="bibr" rid="bib0042">[42]</xref>, <xref ref-type="bibr" rid="bib0043">[43]</xref>.</p>
<p id="para0041">Many factors have influences on the brain development of young children, which will have an impact on cognitive abilities and mental health in later life. These influencing factors include genes, maternal stress, and drug abuse, exposure to toxic environments, infectious diseases, socioeconomic status of the family, etc. Approximately one-third of genes in the human genome are expressed primarily in the brain and will affect brain development. Many psychiatric and mental disorders, such as autism, ADHD, bipolar, and schizophrenia, are highly heritable or have genetic risk factors. Maternal stress and drug abuse are associated with preterm birth and low birth weight and increased risk of neurodevelopmental disorders and mental disorders in children <xref ref-type="bibr" rid="bib0044">[44]</xref>. The nutritional status of a child, which is affected by the socioeconomic status of the family, has a significant impact on neurocognitive development <xref ref-type="bibr" rid="bib0045">[45]</xref>.</p>
<p id="para0042">Neuroimaging techniques have been used to study normal and/or abnormal development of the brain, enhancing our understanding of neuroanatomy, connectivity, and functionality of the brain. These techniques also reveal the etiological associations of abnormal brain development with risk factors and contribute to the development of intervention procedures for diseased children <xref ref-type="bibr" rid="bib0042">[42]</xref>. Young children are more sensitive to radiation than adults are, so the use of PET and CT is limited. Thanks to the in vivo nature and versatility of MRI, not only young children but also newborn babies can be imaged, offering the opportunity to study white matter development and cognition in babies <xref ref-type="bibr" rid="bib0046">[46]</xref>, <xref ref-type="bibr" rid="bib0047">[47]</xref>, <xref ref-type="bibr" rid="bib0048">[48]</xref>. MRI has become the most important pediatric neuroimaging modality and has been widely used to study normal and abnormal brain development, allowing repeated longitudinal observation of the changes of brains of the same individuals before and after birth <xref ref-type="bibr" rid="bib0049">[49]</xref>. In the following, our review will focus on the major MRI modalities in pediatric imaging, which include structural, functional, and diffusion tensor imaging.</p>
<p id="para0043">Early pediatric brain MRI studies focused on the anatomical aspects using T1-weighted and T2-weighted images. Qualitative studies provided information about changing patterns of gray matter and white matter differentiation and myelination in the first months of birth <xref ref-type="bibr" rid="bib0050">[50]</xref> and early childhood <xref ref-type="bibr" rid="bib0051">[51]</xref>. Quantitative studies also revealed the changes in water contents, T1, and T2 relaxation times in both gray matter and white matter; age-related changes in gray matter, white matter, and CSF volumes. All these reflect ongoing maturation and remodeling of the central nervous system <xref ref-type="bibr" rid="bib0052">[52,</xref>
<xref ref-type="bibr" rid="bib0053">53]</xref>.</p>
<p id="para0044">Compared with adult cohorts, brain MR imaging of young children is challenging because of several factors. Young children are less cooperative than adults with scanning procedures, which can be long, noisy, and uncomfortable when lying still for long; the images are often plagued with motion artifacts. The brain changes rapidly with age in early life after birth; the brain is not well myelinated; the contrast between gray matter and white matter is low. These pose difficulties to the optimized parameters for data acquisition protocol and also the standard parameters or criteria for the postprocessing procedures, such as the segmentation of the brain to determine cortical thickness <xref ref-type="bibr" rid="bib0054">[54]</xref>. As a result, the physical properties, such as relaxation times, water content, diffusion coefficients, of the developing brain are not very well characterized. Other technical challenges exist to scan young children <xref ref-type="bibr" rid="bib0055">[55]</xref>.</p>
<p id="para0045">Knowledge of the variations of biophysical properties, such as T1 and T2 relaxation times, water contents in GM and WM during the early life of children, is of critical importance to the understanding of neurodevelopment of young children and also to the development of diagnostic protocols of abnormal brain development. The measurement of these biophysical properties is challenging due to the prolonged scan time. Recent technical development of magnetic resonance fingerprinting (MRF) allows rapid and quantitative analysis of multiple tissue properties <xref ref-type="bibr" rid="bib0040">[40]</xref>. For example, MRF can provide T1, T2, and proton density maps of the brain in contrast to the conventional T1-weighted, T2-weighted, or proton density-weighted images. A recent paper reported an application of the MRF to study the T1, T2, and MWF of children aged from 0 to 5 years old <xref ref-type="bibr" rid="bib0056">[56]</xref>. This study was able to record different patterns of variations of tissue biophysical parameters over different age stages. MRF techniques were also used to parametrically characterize brain tumors in children and young adults <xref ref-type="bibr" rid="bib0057">[57]</xref>. In a broad sense, the parametric information in MRF opened doors to studies of the correlations between brain tissue properties and brain development, impairment, and physiopathology. Techniques of image fusion can play an essential role in the processing, interpretation, and application of MRF data.</p>
</sec>
<sec id="sec0021">
<label>4.2</label>
<title>Degenerative brains</title>
<p id="para0046">Degenerative brain diseases are caused by the decline of neuronal function and the reduction of numbers of neurons in the central nervous system (CNS). Known degenerative brain diseases include mild cognition impairment, Alzheimer's disease (AD), Parkinson's disease, etc. The patients of these diseases suffer from losses of functions in memory, speech, movement, etc. Most of these diseases (except for some mild cognitive impairment subtypes) are progressive, i.e. the symptoms deteriorate as the brains age. As the population is rapidly aging, degenerative brain diseases post enormous impacts on individuals, families, and society. The etiology of these diseases is still unknown, and there is currently no cure. In the following sessions, we will review advances of neuroimaging on MCI, ADs, and PDs.</p>
<sec id="sec0022">
<label>4.2.1</label>
<title>Mild cognitive impairment</title>
<p id="para0047">Mild cognitive impairment (MCI) is a clinical transition between normal aging and dementia or Alzheimer's disease (AD), in which individuals have memory or other cognitive impairments beyond their age, but not to the extent of dementia. Patients with MCI often only have minor difficulties in functional ability.</p>
<p id="para0048">In studies based on people older than 65 years of age, the incidence of MCI is estimated to be at 10–20% <xref ref-type="bibr" rid="bib0058">[58]</xref>, and the Mayo clinical study on aging shows an 11.1% incidence of amnestic MCI (aMCI) and 4.9% incidence of non-amnestic MCI (naMCI) in undiagnosed patients aged 70-89 years <xref ref-type="bibr" rid="bib0059">[59]</xref>. Several longitudinal studies have shown that most MCI patients have a significantly higher risk of developing to dementia compared to the general U.S. population (1-2%/ year) <xref ref-type="bibr" rid="bib0060">[60]</xref>, the community population (5-15%/ year), and the clinical patients (10-15%/ year) <xref ref-type="bibr" rid="bib0061">[61]</xref>, <xref ref-type="bibr" rid="bib0062">[62]</xref>, <xref ref-type="bibr" rid="bib0063">[63]</xref>. The latter data suggest that cognitive impairment tends to develop more rapidly for the patients that display serious symptoms. Although some studies have shown that the incidence of MCI reversals to normal cognitive function is as high as 25-30%, recent studies suggest that the incidence may be lower. In addition, cognitive reversals over a short period of follow-up study showed that they did not prevent subsequent disease progression.</p>
<p id="para0049">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0050">Magnetic resonance imaging (MRI) techniques have been used in the clinical identification of MCI and various types of dementia to predict the progression of MCI to dementia. For MRI measurements of brain structure, linear, area, or volume measurements can be used. The results showed that the area of MCI brain atrophy was consistent with AD, but to a lesser extent, between the normal elderly (control group) and AD patients <xref ref-type="bibr" rid="bib0064">[64]</xref>, <xref ref-type="bibr" rid="bib0065">[65]</xref>, <xref ref-type="bibr" rid="bib0066">[66]</xref>, <xref ref-type="bibr" rid="bib0067">[67]</xref>. Similar results were found using voxel-based measurement and analysis, with abnormal changes in not only gray matter but also white matter <xref ref-type="bibr" rid="bib0068">[68,</xref>
<xref ref-type="bibr" rid="bib0069">69]</xref>. The previous diagnosis of AD by structural MRI was mainly based on the degree of brain atrophy, especially in the medial temporal lobe. The structural MRI studies showed the atrophy along the hippocampal pathway (entorhinal cortex, hippocampus and posterior cingulate cortex), which was consistent with the loss of early memory. As the disease progresses, the temporal, frontal, and apical lobes shrink with neuronal loss, causing abnormalities in language, practice, vision, and behavior <xref ref-type="bibr" rid="bib0070">[70,</xref>
<xref ref-type="bibr" rid="bib0071">71]</xref>. However, no definitive biomarkers have been identified by structural MRI alone to distinguish MCI and AD, to stage MCI, and to predict MCI conversion to AD or not <xref ref-type="bibr" rid="bib0072">[72,</xref>
<xref ref-type="bibr" rid="bib0073">73]</xref>.</p>
<p id="para0051">MRI-based functional imaging has been applied to the understanding of and to the discrimination between AD and MCI. These techniques include perfusion-weighted imaging (PWI), diffusion-weighted imaging (DWI), diffusion tensor imaging (DTI), and blood oxygen-dependent fMRI (including task execution and resting state) <xref ref-type="bibr" rid="bib0072">[72]</xref>. Functional MRI allows the delineation of microstructural brain changes, which is complementary to structural MRI that can depict the global changes of the brain in MCI. An MRI-based functional imaging study that employed PWI, DTI and proton MRS showed significant abnormalities in parameters derived from the three imaging modalities for AD patients. PWI and DTI parameters showed a significant, but a lower degree of abnormalities in some areas for MCI patients. fMRI has also been used to distinguish AD and MCI and to predict the transition from cognitive normal to MCI and from MCI to AD. Recent studies show that BOLD-fMRI can detect changes in brain function before MCI progresses to AD, making it an important technique to study the neural mechanism of MCI <xref ref-type="bibr" rid="bib0074">[74,</xref>
<xref ref-type="bibr" rid="bib0075">75]</xref>.</p>
<p id="para0052">Proton magnetic resonance spectroscopy (<sup>1</sup>HMRS) is a noninvasive imaging method that can detect biochemical and metabolic changes in brain tissue <italic>in vivo</italic> and conduct quantitative analysis. Early MRS studies show abnormal concentrations of N-acetylaspartate (NAA), creatine, and choline are associated with the status of memory and cognition impairment and have a promise for assessing cognitive status, evaluating response to medicine, and monitoring progression during treatment <xref ref-type="bibr" rid="bib0076">[76]</xref>, <xref ref-type="bibr" rid="bib0077">[77]</xref>, <xref ref-type="bibr" rid="bib0078">[78]</xref>. In recent years, with advances in the technical development of MR hardware and pulse sequences, the roles of glutamate, the excitatory neurotransmitter, and GABA, the inhibitory neurotransmitter, in MCI patients became the main focus <xref ref-type="bibr" rid="bib0079">[79]</xref>, <xref ref-type="bibr" rid="bib0080">[80]</xref>, <xref ref-type="bibr" rid="bib0081">[81]</xref>. For example, with ultra-high field 7 Tesla MR scanner, abnormal concentrations of GABA, glutamate, NAA, glutathione, and myo-inositol (mI) in different brain regions were detected <xref ref-type="bibr" rid="bib0082">[82]</xref>. The manifestations of <sup>1</sup>HMRS in MCI patients were mainly shown in decreased NAA/Cr ratio and increased mI/Cr ratio. The pathological results showed neuronal deletion and glial proliferation, and the changes in metabolite concentration were consistent with the pathological results <xref ref-type="bibr" rid="bib0082">[82,</xref>
<xref ref-type="bibr" rid="bib0083">83]</xref>.</p>
<p id="para0053">
<bold><italic>Multimodal imaging</italic></bold>
</p>
<p id="para0054">PET and SPECT provide insight into blood perfusion and metabolism in tissues and organs, as well as explore changes in function. The nuclear medical images of aMCI patients showed decreased perfusion and metabolism in the hippocampus, temporoparietal lobe, and posterior cingulate gyrus. Studies using PET, SPECT, and MRI have shown that glucose metabolism in the hippocampus, glucose metabolism rate in the bilateral temporal-parietal lobe, and blood perfusion in patients with aMCI are lower than those in normal elderly. These studies have also shown that low glucose metabolism in the temporal-parietal lobe is a reliable indicator of conversion to AD <xref ref-type="bibr" rid="bib0084">[84]</xref>, <xref ref-type="bibr" rid="bib0085">[85]</xref>, <xref ref-type="bibr" rid="bib0086">[86]</xref>. Excessive deposition of β-amyloid peptide in the brain and the cascade reaction caused by it are the early onset of AD. Therefore, early detection of β-amyloid peptide in the brain can help identify patients with aMCI, and monitor the progression of the disease and treatment effect. It was found that the <sup>11</sup>C-PiB-PET could attach to Aβ in the brain. PET imaging showed the amount and location of Aβ deposition in the brain, which was expected to be an early diagnostic method for AD <xref ref-type="bibr" rid="bib0087">[87]</xref>, <xref ref-type="bibr" rid="bib0088">[88]</xref>, <xref ref-type="bibr" rid="bib0089">[89]</xref>.</p>
<p id="para0055">Multimodal imaging techniques involving MRI-based imaging and PET-based imaging have been frequently used for prediction, characterization, and classification of MCI <xref ref-type="bibr" rid="bib0090">[90,</xref>
<xref ref-type="bibr" rid="bib0091">91]</xref>. In facilitating these complex tasks, imaging fusion methods based on artificial intelligence, neural network, deep learning and graph theory have been used <xref ref-type="bibr" rid="bib0092">[92]</xref>, <xref ref-type="bibr" rid="bib0093">[93]</xref>, <xref ref-type="bibr" rid="bib0094">[94]</xref>. Brain network studies based on multimodal MRI and graph theory analysis have found that the topological properties of AD and aMCI affected brain networks have undergone abnormal changes, which mainly manifested as the imbalance between functional differentiation and integration. This approach provided a new way to reveal topological mechanisms and pathophysiological mechanisms of brain networks <xref ref-type="bibr" rid="bib0093">[93,</xref>
<xref ref-type="bibr" rid="bib0095">95]</xref>. In addition, the combination of graph theory analysis and classification analysis suggests that the brain network topology attribute can be used as an imaging marker of AD and has a good clinical application prospect.</p>
</sec>
<sec id="sec0023">
<label>4.2.2</label>
<title>Alzheimer's disease</title>
<p id="para0056">Alzheimer's disease (AD) is a neurodegenerative disorder and the most common cause of dementia. AD is characterized by progressive memory loss, aphasia, loss of use, loss of recognition, impairment of visual-spatial skills, executive dysfunction, and personality and behavior changes <xref ref-type="bibr" rid="bib0096">[96,</xref>
<xref ref-type="bibr" rid="bib0097">97]</xref>. It has become one of the major diseases that seriously threaten the health and quality of life of the elderly <xref ref-type="bibr" rid="bib0098">[98]</xref>. The onset of AD is slow or insidious, with patients and their families often unable to tell when it starts. It is more common in the elderly over the age of 70 (the average male is 73, and the average female is 75 years old), with more females than males (female to male ratio of 3:1) <xref ref-type="bibr" rid="bib0099">[99]</xref>.</p>
<p id="para0057">There is currently no cure for AD, but large numbers of novel compounds are currently under development that have the potential to modify the course of the disease and to assess the efficacy of these proposed treatments. There is a pressing need for imaging biomarkers to improve understanding of the disease and to assess the efficacy of these proposed treatments.</p>
<p id="para0058">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0059">Structural MRI (sMRI) is the most widely used imaging modality for the study of AD. The techniques for analyzing sMRI are classified into volume-based and surface-based methods <xref ref-type="bibr" rid="bib0100">[100]</xref>. Previous studies have shown that hippocampal volume atrophy and whole-brain atrophy independently predicted the progression of AD <xref ref-type="bibr" rid="bib0101">[101]</xref>. Hippocampal damage or atrophy occurs in the early stage of AD, which is an important structural basis for the clinical manifestations of AD. Although global hippocampal atrophy in AD was well accepted, the differences were often detected large sample-size studies <xref ref-type="bibr" rid="bib0102">[102]</xref>.</p>
<p id="para0060">De Winter et al. studied 48 elderly AD patients with depression and 52 healthy control elderly people and examined all the subjects with sMRI and neuropsychology <xref ref-type="bibr" rid="bib0103">[103]</xref>. They found that there was no significant difference in the positive rate of Aβ between the depression group and the healthy control group. However, the hippocampal volume in the depression group was significantly smaller than that in the healthy control group. There is significant hippocampal atrophy in elderly depression patients, and hippocampal atrophy has nothing to do with Aβ, which challenges the reliability of hippocampal atrophy in the clinical diagnosis of AD. It is suggested that hippocampal atrophy not only occurs in AD but also in senile depression. The study of sMRI indicates that the brain atrophy shown by brain morphology and structure has reference value for the diagnosis of AD. However, the diagnosis of AD still needs to be confirmed by combining clinical manifestations, neuropsychological assessments, and other examination methods. It also indicates that follow-up is needed for suspected depression in patients with AD. The above studies showed the limitations of structural MRI and the necessity of the multimodal approach in the study of AD <xref ref-type="bibr" rid="bib0104">[104]</xref>.</p>
<p id="para0061">Other MRI modalities, including functional MRI, DWI, PWI, have also been widely used in the study of neurodegenerative diseases. We will review recent advances of the resting-state functional magnetic resonance imaging (rs-fMRI) as an example. As opposed to the conventional task-based fMRI, rs-fMRI does not require the subject to perform any task or be subjected to any external excitation. The rs-fMRI captures the low-frequency oscillations signals that are related to the spontaneous neural activity of the brain by analyzing the brain blood oxygen level dependent (BOLD) signal. Sophisticated methods of analysis of the rs-fMRI data depict the functional connectivity of the brain. The rs-fMRI has been used to reveal how the networks of the functional connectivity are correlated to the brain functions of individuals with cognitive impairment. Zamboni et al. found that the recognition task of AD patients was related to the increased activation of the lateral prefrontal area, which also overlapped with the functional connection enhancement area indicated by the rs-fMRI <xref ref-type="bibr" rid="bib0105">[105]</xref>. Zhou et al. predicted the pathological changes of AD by using the calculation model of resting brain function network and studied five different brain regions vulnerable to neurodegenerative diseases through the use of task state fMRI <xref ref-type="bibr" rid="bib0106">[106]</xref>. They found that the brain network of AD patients may have the phenomenon of weak functional connectivity and their ability to transmit information of functional brain network decline. Wang et al. found that the functional brain network of MCI patients had different degrees of functional connectivity disorder. The evaluation of overall functional brain connectivity of patients plays an important role in the early diagnosis and treatment of AD <xref ref-type="bibr" rid="bib0107">[107]</xref>. Abnormal brain connectivity can be a biomarker of the disease.</p>
<p id="para0062">Many neuropsychiatric diseases and dementia can change the default mode network (DMN) of the brain. Identification of the change in the connectivity of DMN is constructive for the early recognition of AD. Jin et al. collected 8 patients with aMCI and 8 healthy people to analyze rs-fMRI data by independent component analysis (ICA) <xref ref-type="bibr" rid="bib0108">[108]</xref>. They found that the functional activities of the lateral prefrontal cortex, left medial temporal lobe, left middle temporal gyrus and right angular gyrus in aMCI patients decreased, while the activity of the middle and medial prefrontal cortex and the left parietal cortex increased. Further studies found that the functional activities of the left lateral prefrontal cortex, left middle temporal gyrus and right angular gyrus were positively correlated with memory, especially delayed memory <xref ref-type="bibr" rid="bib0109">[109]</xref>. Although there was no significant difference between the two groups in the degree of medial temporal lobe atrophy, the functional activities of the left medial temporal lobe decreased. This decrease suggests that the functional changes of DMN may occur in the early stage of AD, i.e. aMCI, and the functional changes may occur before the obvious change of brain structure.</p>
<p id="para0063">
<bold><italic>Multimodal imaging</italic></bold>
</p>
<p id="para0064">Due to severe overlap in symptoms and findings of individual imaging modalities of the neurodegenerative diseases, it is difficult to identify the biomarkers that could be used to differentiate the types of these diseases and/or to stage the progress of a disease. Therefore, multimodal neuroimaging techniques are used to overcome the challenges <xref ref-type="bibr" rid="bib0110">[110]</xref>. As pointed out in <xref ref-type="bibr" rid="bib0111">[111]</xref>, individual modalities of MRI and EEG lack precision in AD diagnosis and staging. By employing both imaging modalities, with the MRI measuring the cortical thickness and the EEG measuring the rhythmic activities, the authors found joint markers that identified the subjects of Alzheimer's disease with an accuracy of 84.7%, a significant increase from those of individual modalities. While some studies of multimodal imaging confirmed correlations of findings among individual modalities as in a study of sMRI and fMRI <xref ref-type="bibr" rid="bib0112">[112]</xref>, multimodal imaging studies can also be used to dissociate the tau deposition and brain atrophy in early ADs using PET and MRI. The study found that the tau load had little effect on the gray matter atrophy, and this might imply that tau protein deposit precedes and predicts brain atrophy. The multimodal imaging studies require statistical and analytical models, advanced computing algorithms, and especially, novel data fusion methods <xref ref-type="bibr" rid="bib0113">[113]</xref>, <xref ref-type="bibr" rid="bib0114">[114]</xref>, <xref ref-type="bibr" rid="bib0115">[115]</xref>, <xref ref-type="bibr" rid="bib0116">[116]</xref>, which will be reviewed in detail in the following sections.</p>
</sec>
<sec id="sec0024">
<label>4.2.3</label>
<title>Parkinson's disease</title>
<p id="para0065">Parkinson's disease (PD) is a chronic progressive degenerative disease of the central nervous system, which is commonly seen in elderly patients. Typical clinical manifestations of PD include static tremor, myotonia, bradykinesia, and abnormal posture and pace <xref ref-type="bibr" rid="bib0117">[117]</xref>. With the continuous increase of the aging population, the incidence and disability rates of the population are also increasing year by year. The results of epidemiological surveys indicate that the prevalence of PD in people over 65 is about 1.7%, and the prevalence of PD in people over 80 is as high as 4% <xref ref-type="bibr" rid="bib0118">[118]</xref>, <xref ref-type="bibr" rid="bib0119">[119]</xref>, <xref ref-type="bibr" rid="bib0120">[120]</xref>. PD is more and more harmful to the health of the middle-aged and elderly, especially involving the central nervous system.</p>
<p id="para0066">Due to the lack of objective basis and diagnostic criteria for the diagnosis of PD, the previous clinical diagnosis of PD was mainly based on the clinical symptoms, resulting in a low coincidence rate between the clinical diagnosis and pathology of PD and in a significant lag behind the pathological changes of brain microstructure. With the increasingly standardized diagnosis and treatment of PD, neuroimaging examination has become an indispensable part of the diagnosis. This differential diagnosis of PD can help identify different movement disorders, locate anatomical dysfunction sites, and determine the causes of the lesion, which will improve clinical evaluation and prognosis <xref ref-type="bibr" rid="bib0121">[121,</xref>
<xref ref-type="bibr" rid="bib0122">122]</xref>.</p>
<p id="para0067">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0068">Structural cranial MRI can distinguish white matter from gray matter by setting different imaging parameters while avoiding radiation. It is better than cranial CT in revealing white matter lesions, small infarcts, subacute intracerebral hemorrhage, and lesions in the brain stem, subcortical regions, and posterior fossa. On structural MRI such as T1, T2-weighted, and fluid-attenuated inversion recovery (FLAIR) images, PD patients usually exhibit broadening of the ventricles (caused by extrapyramidal atrophy) and widened sulci (diffuse brain cortical atrophy) <xref ref-type="bibr" rid="bib0123">[123]</xref>. The quantitative measurements of cortical atrophy can be measured based on voxel morphometric assessment. When the compact belt of the substantia nigra shrinks and the short T2 signal of the substantia nigra disappears, the width of the dense belt of the substantia nigra, the ratio of the width of the dense belt of the substantia nigra to the diameter of the midbrain, the caudex nucleus, the putamen nucleus, the thalamus and other areas of interest are measured. In evaluating the extent of atrophy, physiological changes such as age increase and relevant clinical supporting evidence should be taken into account <xref ref-type="bibr" rid="bib0124">[124]</xref>.</p>
<p id="para0069">Neuromelanin-sensitive MRI is used to detect neuromelanin, a surrogate biomarker for the PD. Neuromelanin is a dark pigment found in neurons in the substantia nigra pars compacta. The concentration of neuromelanin increases with age but is found to be around 50% higher in PD patients compared with age-matched non-PD subjects, due to the death of cells in the substantia nigra. Neuromelanin-sensitive MRI allows the visualization of the neuromelanin-containing neurons in the substantia nigra, pars compacta. With the use of morphological analysis and signal intensity (contrast to noise ratio), the width and CNR of the lateral and central substantia nigra were found to be significantly lower in the PD subjects than in the control group and untreated essential tremor (ET) group <xref ref-type="bibr" rid="bib0125">[125,</xref>
<xref ref-type="bibr" rid="bib0126">126]</xref>. Therefore, this imaging technique can be potentially used as a biomarker to differentiate ET from the <italic>de novo</italic> tremor-dominant PD subtype. The neuromelanin levels were quantitatively assessed using neuromelanin-sensitive MRI and quantitative susceptibility mapping (QSM) <xref ref-type="bibr" rid="bib0127">[127]</xref>, an MRI modality for measuring the absolute concentrations of iron, calcium, and other substances in tissues based on changes of local susceptibility <xref ref-type="bibr" rid="bib0128">[128]</xref>. While the neuromelanin imaging found significantly lower neuromelanin levels in the PD group than the health controls (HC), which is in agreement with the neuromelanin MRI only study, the QSM values were significantly higher in the PS group than in the HC group. This result suggested the usefulness of QSM in detecting PD <xref ref-type="bibr" rid="bib0127">[127]</xref>.</p>
<p id="para0070">Resting functional magnetic resonance imaging (fMRI) is a technique that collects the blood oxygen level dependent signal changes of patients in the awake and resting states to obtain the functional activity level of the brain in the baseline state. In recent years, fMRI has been widely used in clinical studies of all motor disorders or neurodegenerative diseases, including PD <xref ref-type="bibr" rid="bib0129">[129]</xref>, <xref ref-type="bibr" rid="bib0130">[130]</xref>, <xref ref-type="bibr" rid="bib0131">[131]</xref>. Resting-state functional MRI (rs-fMRI) can calculate a variety of brain activity attributes, such as local consistency, range of low-frequency fluctuation, and amplitude of low-frequency fluctuation etc. By observing the correlation between time-dependent signals of blood oxygen levels in different voxels or areas of interest, we can further evaluate the synchronization of functional activity in different brain areas, i.e. functional connectivity <xref ref-type="bibr" rid="bib0129">[129,</xref>
<xref ref-type="bibr" rid="bib0132">132,</xref>
<xref ref-type="bibr" rid="bib0133">133]</xref>. In recent years, calculation methods based on independent component analysis, Granger causality analysis and Graph Theory can help to find complex pattern changes in the brain network of PD patients.</p>
<p id="para0071">
<bold><italic>Other imaging modalities and multimodal imaging</italic></bold>
</p>
<p id="para0072">Other imaging modalities, including PET, SPECT, EEG, CT, have been applied to study the functional and structural abnormalities and changes of PD patients, and they provide much complementary information to MR-based imaging modalities mentioned earlier. PET studies investigated cerebral glucose metabolism with or without medications, with or without brain stimulations <xref ref-type="bibr" rid="bib0134">[134]</xref>, <xref ref-type="bibr" rid="bib0135">[135]</xref>, <xref ref-type="bibr" rid="bib0136">[136]</xref>. Metabolic and brain chemical changes related to dopamine neurons in PD patients were also studied using SPECT, which cannot be assessed by other MRI modalities including proton magnetic resonance spectroscopy (MRS) <xref ref-type="bibr" rid="bib0137">[137]</xref>. By jointly applying SPECT and DTI, this study identified regions and connections of the brain that differentiate PD patients and healthy controls. Different from the imaging modalities in that study, a recent study employed PET scans with two different tracers and rs-fMRI to investigate variations of metabolism and functional connectivity of the PD patients <xref ref-type="bibr" rid="bib0138">[138]</xref>. It identified correlations between motor impairments with hypometabolism and hypoconnectivity in multiple brain regions. With the use of different modalities under similar aims, results from these studies can provide complementary information for the impaired regions. The data from them can be integrated and analyzed using data fusion like the work in <xref ref-type="bibr" rid="bib0139">[139]</xref>, in which data of anatomical MRI, rs-fMRI, and DTI were analyzed for more accurate and reliable biomarkers of PD.</p>
</sec>
</sec>
<sec id="sec0025">
<label>4.3</label>
<title>Mental disorders</title>
<p id="para0073">Mental disorders are conditions that affect a person's thinking, mode, behavior, relationship with others, and functions of daily life and work. Major psychiatric disorders include depressive disorders, bipolar disorders, obsessive-compulsive disorders, schizophrenic disorders, autistic spectrum disorders, attention deficit, and hyperactivity disorder. It is estimated that nearly one-fifth of adults aged 18 or older in the United States live with a psychiatric disorder <xref ref-type="bibr" rid="bib0140">[140]</xref>. The World Health Organization estimates that mental disorders affect one-fourth of the worldwide population <xref ref-type="bibr" rid="bib0141">[141]</xref>. The high prevalence of mental disorders have a significant impact on the wellbeing of societies and the development of the world economy <xref ref-type="bibr" rid="bib0142">[142]</xref>, <xref ref-type="bibr" rid="bib0143">[143]</xref>, <xref ref-type="bibr" rid="bib0144">[144]</xref>
</p>
<p id="para0074">Unlike the diagnosis of other diseases, such as cancer and diabetes, there are currently no medical tests that can determine mental illness. The diagnosis of mental illness is determined by a psychiatrist using official criteria such as The Diagnostic and Statistical Manual of Mental Disorders, fifth edition (DSM-5) according to the feeling, symptoms, and behaviors of the patient. However, neuroimaging techniques have been used to detect, identify, differentiate, and understand the abnormalities, differences, etiologies, and biomarkers of psychiatric disorders <xref ref-type="bibr" rid="bib0145">[145]</xref>, <xref ref-type="bibr" rid="bib0146">[146]</xref>, <xref ref-type="bibr" rid="bib0147">[147]</xref>, <xref ref-type="bibr" rid="bib0148">[148]</xref>, <xref ref-type="bibr" rid="bib0149">[149]</xref>.</p>
<sec id="sec0026">
<label>4.3.1</label>
<title>Depression</title>
<p id="para0075">Depression is a common mood disorder, which can be caused by a variety of reasons. The main clinical feature is marked with persistent depression of mood, which is incompatible with the situation. In severe cases, suicidal thoughts and behaviors may occur. Most cases tend to show recurrence; and most can be relieved each time, while some may have residual symptoms or progress to chronic depression. At least 10% of clinical depression patients also show manic episodes and should be diagnosed as bipolar disorder <xref ref-type="bibr" rid="bib0150">[150,</xref>
<xref ref-type="bibr" rid="bib0151">151]</xref>. What we commonly call depression is clinical or major depression, which affects 16% of the population at some point in their lives <xref ref-type="bibr" rid="bib0152">[152]</xref>. In addition to the severe emotional and social costs of depression, the economic costs are also enormous. According to the World Health Organization, depression has become the fourth most serious disease in the world and is expected to become the second most serious disease after coronary heart disease by 2020 <xref ref-type="bibr" rid="bib0153">[153]</xref>.</p>
<p id="para0076">So far, the etiology and pathogenesis of depression are not clear, and there are no obvious signs or laboratory indicators of abnormality. Although there have been many basic and clinical studies on depression, no critical breakthrough has been made in the three most important clinical problems: pathogenesis, objective diagnosis, and efficient treatment. A key breakthrough in these issues is to find and establish a stable biological marker from gene to clinical phenotype and then further study its pathogenesis, establish objective diagnostic methods and develop efficient clinical therapy.</p>
<p id="para0077">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0078">Up to now, in the clinical research field of mental illness, especially depression, the most sought after biological markers may potentially be provided by the study of neuroimaging, especially brain MRI. Brain MRI examination have characteristics of good clinical applicability, non-invasive, simple operation, universal, relatively stable results and easy to repeat, but its sensitivity and specificity need to be improved. Brain MRI research has become an intermediate mechanism from molecular research to clinical phenotype. Through this mechanism research, we can not only explore how genes, molecules, and proteins affect the brain structure and function of patients with depression but also use MRI as an objective diagnostic tool for the most urgent clinical needs. Over the past 20 years, the application of multi-mode MRI technology to study the brain structural and functional characteristics of depression, especially to establish clear biological marker targets around the characteristics of emotional circuits, has become one of the major scientific frontiers in the basic and clinical research of neuroscience.</p>
<p id="para0079">Many studies have found that patients with depression have abnormalities in brain structure and function of emotional circuits, as well as in neurotransmitters associated with these circuits <xref ref-type="bibr" rid="bib0149">[149,</xref>
<xref ref-type="bibr" rid="bib0154">154]</xref>. MRI studies in recent years found that the depressive mood is associated with three brain regions, namely in the amygdala and the ventral striatum as the primary mood areas, the orbital gyrus, medial prefrontal cortex and cingulate gyrus as the emotional auto-regulation areas, and the dorsolateral and ventrolateral prefrontal cortex as the center of the active emotional regulation area <xref ref-type="bibr" rid="bib0154">[154]</xref>, <xref ref-type="bibr" rid="bib0155">[155]</xref>, <xref ref-type="bibr" rid="bib0156">[156]</xref>.</p>
<p id="para0080">
<bold><italic>Multimodal MRI</italic></bold>
</p>
<p id="para0081">Multimodal MRI techniques used in mental disorders seek to find correlated, complementary, and/or converging image features from multiple image modalities and applied sophisticated analytical methods to identify robust biomarkers for the types of depression.</p>
<p id="para0082">A study employed DTI, magnetic resonance spectroscopy (MRS), rs-fMRI, and magnetoencephalography (MEG) and revealed patterns of abnormalities of patients with major depressive disorders. These patterns included factors in the neurotransmitters (glutamate concentration), white matter fibers (fractional anisotropy), and functional excitations (fMRI) <xref ref-type="bibr" rid="bib0157">[157]</xref>. A multimodal MRI study involves structural MRI and ASL to assess grey matter volume and regional cerebral blood flow in MDD patients. This multimodal study revealed negative correlations between the extent of depressive symptoms and CBF in the bilateral para-hippocampus and between depressive symptoms and CBF in the right middle frontal cortex <xref ref-type="bibr" rid="bib0158">[158]</xref>. In addition to confirming the correlations among findings of individual modalities, some multimodal MRI studies, however, found disrelations among individual findings in the MDD group <xref ref-type="bibr" rid="bib0159">[159]</xref>. Further multimodal data analysis involving MRI imaging data and clinical, neurobiological metrics of the patients may resolve the disparities.</p>
<p id="para0083">Among the methods of the multimodal MRI image fusion in depressive disorders, support vector machine (SVM) <xref ref-type="bibr" rid="bib0160">[160]</xref> and linked independent component analysis <xref ref-type="bibr" rid="bib0161">[161]</xref> was recently used, respectively, to identify biomarkers for the classification and prediction of symptom loads of heterogeneous MDD cohorts. Imaging data and neurobiological data were included in the data fusion. In both studies, the results did not show strong support for the hypothesis and did not provide sufficient evidence for the sought biomarkers <xref ref-type="bibr" rid="bib0160">[160,</xref>
<xref ref-type="bibr" rid="bib0161">161]</xref>.</p>
</sec>
<sec id="sec0027">
<label>4.3.2</label>
<title>Obsessive-compulsive disorder</title>
<p id="para0084">Obsessive-compulsive disorder (OCD) is a group of neuropsychiatric disorders with obsessive thinking and compulsive behavior as the main clinical manifestations. It is characterized by the co-existence of conscious compulsion and anti-compulsion, and the repeated intrusion of thoughts or impulses into the daily life of patients that are often meaningless and involuntary. Although patient perceives that these thoughts or impulses are their own and resist them to the utmost degree, he or she is still unable to control them. The intense conflict between the two causes the patient great anxiety and pain, which affects his other study, work, interpersonal communication, and even daily life.</p>
<p id="para0085">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0086">Voxel-based morphometry was widely used in the sMRI studies of OCD. These studies measure the structures and volumes of regions of interest in various OCD groups and healthy control groups. OCD patients were found to have lower grey matter volumes in specific regions of the brain. For children with OCD, these regions include the bilateral frontal lobe, cingulate cortex, and temporal-parietal junction <xref ref-type="bibr" rid="bib0162">[162]</xref>. For adults with OCD, these regions are the left and right orbitofrontal cortex <xref ref-type="bibr" rid="bib0067">[67]</xref>. Lower volumes were also seen in white matter in the cingulate and occipital cortex, right frontal and parietal and left temporal regions <xref ref-type="bibr" rid="bib0162">[162]</xref> and in a small area of the parietal cortex for patients with OCD <xref ref-type="bibr" rid="bib0163">[163]</xref>. fMRI studies can provide information about the pathophysiology of OCD <xref ref-type="bibr" rid="bib0164">[164,</xref>
<xref ref-type="bibr" rid="bib0165">165]</xref>. However, whether this information from single fMRI modality alone could be of clinical value in the diagnosis of individual patients is not clear <xref ref-type="bibr" rid="bib0166">[166]</xref>. The pathophysiological feature of OCD, as revealed by fMRI studies, suggest that abnormal brain metabolites may be implied in OCDs. Abnormalities in brain metabolite concentrations in patients with OCD were investigated using proton MRS <xref ref-type="bibr" rid="bib0167">[167]</xref>, <xref ref-type="bibr" rid="bib0168">[168]</xref>, <xref ref-type="bibr" rid="bib0169">[169]</xref>, <xref ref-type="bibr" rid="bib0170">[170]</xref>. Among about ten detectable metabolites, glutamate, glutamine, and GABA are of particular interest, as they are involved in neurotransmission. Recent studies show that OCD patients had an elevated GABA level and a higher GABA/glutamate ratio in the anterior cingulate cortex <xref ref-type="bibr" rid="bib0171">[171]</xref>, and they had lower GABA concentration in the prefrontal lobe, as compared to healthy control groups <xref ref-type="bibr" rid="bib0172">[172]</xref>. The roles of glutamate and glutamine in OCD are in the focus of research interest, but the findings lacked reasonable consistency <xref ref-type="bibr" rid="bib0173">[173]</xref>, <xref ref-type="bibr" rid="bib0174">[174]</xref>, <xref ref-type="bibr" rid="bib0175">[175]</xref>. The heterogeneity of structural neuroimaging findings of OCD may reflect the heterogeneity of the disease itself.</p>
<p id="para0087">
<bold><italic>Multimodal MRI</italic></bold>
</p>
<p id="para0088">Multimodal MRI studies in OCD provide complementary, correlated and/or integrated information of findings from individual modalities <xref ref-type="bibr" rid="bib0176">[176,</xref>
<xref ref-type="bibr" rid="bib0177">177]</xref>. Early structural MRI study suggested that the volume reduction of superior temporal gyrus (STG) is associated with the pathophysiology of OCD <xref ref-type="bibr" rid="bib0178">[178]</xref>. A functional MRI study found increased low-frequency fluctuations in neural activities in STG <xref ref-type="bibr" rid="bib0179">[179]</xref>. A correlation between these findings was found in a combined structural MRI and fMRI study, which shows that the volume of the superior temporal sulcus is strongly correlated with functional connectivity between several brain regions that may form a neuro-network <xref ref-type="bibr" rid="bib0177">[177,</xref>
<xref ref-type="bibr" rid="bib0180">180]</xref>. The simultaneous 1H-MRS and DTI study, to investigate metabolic and white matter integrity alterations in OCD, found that the level of Glx to Cr ratio in the anterior cingulate cortex was higher in the OCD group than the healthy control group <xref ref-type="bibr" rid="bib0181">[181]</xref>. The study also found from DTI analysis that the FA values in the left cingulate bundle of the OCD group were significantly higher than the healthy controls. A limitation of this study is that the Glx level, which is a combination of glutamate and glutamine, was measured instead of measuring glutamate and glutamine individually. It has been recognized that it is difficult to distinguish these two structurally similar metabolites using 3 Tesla scanners and ultra-high magnetic field (e.g. 7T) scanners are required.</p>
</sec>
<sec id="sec0028">
<label>4.3.3</label>
<title>Schizophrenia</title>
<p id="para0089">Schizophrenia is a group of serious psychosis with unknown etiology, which usually starts slowly or sub-acute in the young and middle-aged individuals <xref ref-type="bibr" rid="bib0182">[182]</xref>. Clinically, it often manifests as a syndrome with different symptoms, including abnormalities in sensory perception, thinking, emotion, and behavior, as well as uncoordinated mental activities <xref ref-type="bibr" rid="bib0183">[183]</xref>. Schizophrenia is a multifactor disease <xref ref-type="bibr" rid="bib0184">[184]</xref>. Although the understanding of etiology is not clear at present, effects of the susceptible quality of individual psychology and the adverse factors of external social environment on the occurrence and development of the disease have been widely recognized. Both susceptible quality and external adverse factors may lead to the occurrence of disease through the joint action of internal biological factors. The course of schizophrenia is generally protracted, showing repeated attack, aggravation, or deterioration. Some patients eventually show recession and mental disability, but some patients can maintain recovery or basic recovery after treatment <xref ref-type="bibr" rid="bib0185">[185]</xref>.</p>
<p id="para0090">
<bold><italic>Magnetic resonance imaging</italic></bold>
</p>
<p id="para0091">Structural MRI was widely used to study the morphology and volumetry of the brains of schizophrenia patients. Studies found that the average brain volume of schizophrenia patients was smaller than that of healthy people <xref ref-type="bibr" rid="bib0186">[186,</xref>
<xref ref-type="bibr" rid="bib0187">187]</xref>. The abnormal volume and structure of white matter usually appear before the onset of the disease, and these abnormalities tend to be stable during the development of the disease <xref ref-type="bibr" rid="bib0188">[188]</xref>; the change of gray matter volume is more evident after the onset of the disease and decreases progressively over time <xref ref-type="bibr" rid="bib0189">[189]</xref>. According to a longitudinal study, gray matter deficiency in schizophrenia mainly occurs in the first five years <xref ref-type="bibr" rid="bib0190">[190]</xref>.</p>
<p id="para0092">Quiet complement to the structural MRI, DTI reveals the abnormalities of white matter microstructure of schizophrenia patients <xref ref-type="bibr" rid="bib0191">[191]</xref>. Decreased fractional anisotropy in white matter tracts, different cortical regions, and subcortical regions was found in schizophrenia patients in some studies. However, controversial findings were also reported <xref ref-type="bibr" rid="bib0192">[192,</xref>
<xref ref-type="bibr" rid="bib0193">193]</xref>. These inconsistencies might be attributed, in part, to small sample sizes. A large-scale DTI study involving more 4,000 subjects found widespread white matter microstructural differences between schizophrenia patients and healthy controls <xref ref-type="bibr" rid="bib0194">[194]</xref>. Significantly reduced fractional anisotropy values were found in 20 of the 25 investigated regions within the white matter. Furthermore, significantly higher mean diffusivity and radial diffusivity were also observed in schizophrenia patients than in healthy controls.</p>
<p id="para0093">Functional MRI techniques are used to detect the deficits in neural networks of patients with schizophrenia <xref ref-type="bibr" rid="bib0195">[195]</xref>. Brain network studies show that the functional connectivity of the default mode network (DMN) in schizophrenic patients has changed. Although the research structures are inconsistent, most studies show that DMN functional connectivity is enhanced in schizophrenia, and functional connectivity in the prefrontal cortex is weakened (especially in the prefrontal cortex) <xref ref-type="bibr" rid="bib0196">[196]</xref>. In addition, the functional connections of auditory/linguistic networks and basal nuclei are related to auditory hallucinations and delusional symptoms. The study of brain structure networks found that the number of frontal and temporal core nodes decreased and the average shortest path increased, indicating decrease in global efficiency. Wang et al constructed a network of DTI images of 79 schizophrenia patients and 96 age-matched normal subjects <xref ref-type="bibr" rid="bib0197">[197]</xref>. They found that: compared with the normal subjects, the global efficiency of the schizophrenic group decreased; the local efficiency of the core nodes distributed in the frontal cortex, the paralimbic system, the limbic system and the left putamen decreased; and the global efficiency of the network was negatively correlated with the PANSS score. Research shows that the change in brain structure network started at the beginning of the disease. More severe symptoms indicate lower the global or local efficiency of the network, and the slower the speed of information integration.</p>
<p id="para0094">Magnetic resonance spectroscopy studies found that the brain metabolism of schizophrenia patients was abnormal <xref ref-type="bibr" rid="bib0198">[198]</xref>, <xref ref-type="bibr" rid="bib0199">[199]</xref>, <xref ref-type="bibr" rid="bib0200">[200]</xref>. The levels of N-acetyl-aspartic acid (NAA) in the hippocampus, frontal lobe, temporal lobe, and thalamus of schizophrenic patients decreased; the levels of NAA in the thalamus of high-risk groups also decreased, while the level of NAA in temporal lobe decreased. It was also found that the increase of glutamate level in the hippocampus and medial temporal lobe was related to the decrease of executive function.</p>
<p id="para0095">
<bold><italic>Multimodal imaging</italic></bold>
</p>
<p id="para0096">The heterogeneities of findings of individual imaging modalities have been driving the multimodal neuroimaging approach to the search of the more consistent and precise biomarkers for the deficits, abnormalities in functions of schizophrenia patients. A concerted use of three MRI modalities, namely, resting-state of fMRI, structural MRI, and diffusion MRI, was able to simultaneously reveal abnormalities from these three kinds of MRI images and, thereby, identify the cortico-striato-thalamic circuits that might be related the cognitive impairments in schizophrenia <xref ref-type="bibr" rid="bib0201">[201]</xref>. In addition to the multimodal imaging investigation of structural and functional brain abnormalities in schizophrenia, proton MRS has also been used in combination with fMRI to investigate cognitive impairment in schizophrenia at both neurometabolic and functional levels <xref ref-type="bibr" rid="bib0202">[202]</xref>. The combined proton MRS and fMRI study are particularly useful for short-term longitudinal studies on the effects of medication, which is invariant to brain structural change. In this study <xref ref-type="bibr" rid="bib0202">[202]</xref>, the relationship between Glx/Cr levels and BOLD response significantly changed after six weeks of medication for schizophrenia patients, although factors that confound interpretations of the results remain. More examples of multimodal imaging studies on schizophrenia are given in recent review articles <xref ref-type="bibr" rid="bib0203">[203,</xref>
<xref ref-type="bibr" rid="bib0204">204]</xref>.</p>
</sec>
</sec>
</sec>
<sec id="sec0029">
<label>5</label>
<title>Multimodal imaging data fusion: methodology</title>
<p id="para0097">Multimodal fusion has gradually entered the center of research interest as an approach to tackle the challenges of neuroimaging. The first main reason is that there exists a great complementarity between different imaging modes. For instance, the images obtained by positron emission tomography imaging (PET) or single-photon emission tomography imaging (SPECT) do not contain high resolution, three-dimensional anatomical information. On the other hand, high-resolution structural images can be obtained via the use of CT and/or MRI. These images complement each other to provide a complete picture of the targeted organs’ anatomy, physiology, and pathology. The fusion of these images is of great significance for the relevant clinical and pre-clinical studies.</p>
<p id="para0098">Another outstanding merit of utilizing multimodal fusion is that it efficiently enhances the spatial and temporal resolution in the characterization of brain processes. In other words, multimodal imaging may allow the combination of the hyper-temporal resolution of one imaging mode with the hyper-spatial resolution of another, taking advantage of the spatial-temporal complementarity. Take a study in 2014 as an example, Ke Zhang et al have successfully measured the cerebral blood flow with a combination of Arterial Spin Labeling (ASL), MRI and PET <xref ref-type="bibr" rid="bib0205">[205]</xref>. Apart from this, utilizing EEG together with fMRI to improve spatial and temporal resolution has also been studied by many scientists in neuroscience <xref ref-type="bibr" rid="bib0206">[206]</xref>, <xref ref-type="bibr" rid="bib0207">[207]</xref>, <xref ref-type="bibr" rid="bib0208">[208]</xref>, <xref ref-type="bibr" rid="bib0209">[209]</xref>
</p>
<p id="para0099">It is worth mentioning that, in both narrow and wide senses, multimodal data fusion has a high capacity of generalization. A typical instance is the alignment of functional MRI, EEG, and fNIRS images to an anatomical coordinate system. The coordinate system can act as a template to standardize reported results. The alignment of the images to a single coordinate system not only allows comparison with other studies but also allows the combination of functional and structural information <xref ref-type="bibr" rid="bib0210">[210]</xref>.</p>
<p id="para0100">PET/CT combination is a multimodal absolute quantification approach where CT provides structural data information on bones, which also serves as the main absorber for the γ-rays in PET. This combination allows decay correction, in which the accumulation of radioactive isotopes in human tissues becomes apparent, and the amount of radioactive decay can be absolutely quantified <xref ref-type="bibr" rid="bib0211">[211,</xref>
<xref ref-type="bibr" rid="bib0212">212]</xref>.</p>
<p id="para0101">Multimodal imaging also has the benefit of utilizing data from one modality to improve the data quality of another modality, such as correcting the geometric distortions of EPI images by acquiring a B0 field map or obtaining EPI with different parameters <xref ref-type="bibr" rid="bib0213">[213,</xref>
<xref ref-type="bibr" rid="bib0214">214]</xref>. Another classic case is in a combined MR-PET study, where the motion information provided by high-temporal resolution MRI data was used to help the reconstruction of the PET data <xref ref-type="bibr" rid="bib0010">[10]</xref>.</p>
<sec id="sec0030">
<label>5.1</label>
<title>Forms of multimodality fusion</title>
<p id="para0102">The long history of neuroimaging has led to the development of an assortment of imaging technologies and modalities, as seen in Section 3. The existing research and apparatus provided a solid foundation for multimodal fusion, leading to the rapid development of many fusion techniques. In this part, we classify reviewed methodologies into four primary forms: multimodal, multi-focus, multi-temporal, and multi-view.</p>
<sec id="sec0031">
<label>5.1.1</label>
<title>Multimodal fusion</title>
<p id="para0103">Modern medical imaging methods aim at revealing possible dysfunctions in patients. For example, neuroimaging methods are often used to image the structure of nervous system on a macroscopic level, which in turn helps explore the neuroglial basis of behavior and cognition of patients. However, how to combine different medical imaging methods to provide images with better quality or clearer structures remains to be the heated research interest in the field. Therefore, multimodal image fusion is proposed to minimize the gap.</p>
<p id="para0104">In a narrow sense, multimodal image fusion, a technique to improve the interpretation of the structure and functions of target organ or region, generally combines two or even more images collected from different imaging instruments. In order to achieve the goal of simultaneous acquisition, researchers have developed particular instrumentations to allow data of one modality to be acquired with low or neglectable inference from another modality. For instance, the EEG-fMRI combination fuses data acquired from EEG instruments like amplifiers and MRI scanners. Also, the novel instrumentations range from simple arrangements to relatively complex technological innovations. However, in some combinations, simultaneous acquisition of data was impossible due to the physical interactions of the imaging devices.</p>
<p id="para0105">Multimodal image fusion, in a broader sense, also combines data but collected with the same instrument. In this case, MRI is widely used due to its versatility in the generation of different tissue contrasts with the well-studied phenomenon of magnetic resonance. There is also research that combines two and more contrasts in the same acquisition, which has been routinely used for many years. Multiple contrasts can also be acquired by PET (Positron emission tomography) when radioactive compounds injected are different.</p>
</sec>
<sec id="sec0032">
<label>5.1.2</label>
<title>Multi-focus fusion</title>
<p id="para0106">Multi-focus fusion, which keeps interested objects staying in focus, is a technique to fuse images acquired with different focal length. Generally, regions containing objects of interest were segmented from individual images, and then fusion is applied to form fused images. Many multi-focus image fusion methods have been developed during the past decades. These methods, however, can be classified into two categories: spatial domain and frequency domain.</p>
<p id="para0107">Spatial domain methods work on image pixels. Methods like Intensity Hue Saturation <xref ref-type="bibr" rid="bib0215">[215]</xref> and Principle Component Analysis (PCA) belong to the category of the spatial domain <xref ref-type="bibr" rid="bib0216">[216]</xref>. A method based on Discrete Cosine Transform proposed by Phamila and Amutha <xref ref-type="bibr" rid="bib0217">[217]</xref> turns out to be more efficient compared to other existing DCT methods <xref ref-type="bibr" rid="bib0218">[218]</xref>. In the proposed method, original images are divided into 8*8 blocks, while DCT coefficients of each block are calculated. AC coefficients, together with DC coefficients, are two components that form DCT coefficients. The blocks with higher AC coefficient values are selected for fusion since higher AC coefficient value indicates higher variance and more fine-grained images. The analysis of performance is based on metrics of mean square error (MSE), Petrovic's metric, and Peak Signal to Noise Ratio (PSNR). The reason why this method is so efficient is likely because it does not require complex floating-point operations.</p>
<p id="para0108">A region-based method, which incorporated segmentation into the fusion process, was proposed by S. Li in <xref ref-type="bibr" rid="bib0219">[219]</xref>. The algorithm is composed of three steps: segmentation, clarity measurement, and fusion of images. Prior to these steps, images are first fused by averaging. Objects of interest are then extracted by normalization-based cuts segmentation. The normalized criterion measures similarity and dissimilarity between different images. After measuring the spatial frequency, the fusion of corresponding regions of source images can be performed. In order to evaluate the performance, mutual information and the Petrovic metric are considered. The Petrovic metric examines the quality of edge-based information transferred from source images to fused images. Compared to pixel-based fusion methods, which suffer from problems including blurring effect, sensitivity to noise and pixel misregistration, the proposed method has lower complexity while being more robust and reliable.</p>
<p id="para0109">Based on the scale-invariant feature transform (SIFT), Yu et al. proposed that local image features such as SIFT can be used for image fusion <xref ref-type="bibr" rid="bib0220">[220]</xref>. The characteristic of SIFT is the robustness to spatial variations, including scale, rotation and translation. Generally, there are two phases in SIFT: feature points detection and feature point description. The proposed algorithm acquired the initial decision map by dense SIFT descriptor, which was used to measure the activity level of the source image patches. The decision map is further improved with feature matching and local focus measure comparison. It was pointed out that local features such as dense SIFT can also be used to match pixels that were misregistered between multiple source images.</p>
<p id="para0110">In methods based on frequency, images are transformed into the frequency-domain for fusion. Wavelet-based methods, including Discrete Wavelet Transform (DWT) <xref ref-type="bibr" rid="bib0221">[221]</xref>, Haar wavelet <xref ref-type="bibr" rid="bib0222">[222]</xref> and pyramid-based methods such as the Laplacian pyramid <xref ref-type="bibr" rid="bib0223">[223]</xref>, fall under the category of frequency-domain methods. A DWT based method was proposed in <xref ref-type="bibr" rid="bib0224">[224]</xref>. Principal component analysis (PCA) was used for approximating coefficients of input images. The principal components were evaluated to obtain multiscale coefficients. The weights for the fusion rule were then acquired by averaging those components. Besides the promising performance, the proposed method was widely applied in medical image fusion for CT and MRI images. In <xref ref-type="bibr" rid="bib0225">[225]</xref>, the authors proposed the Daubechies complex wavelet transform (DCxWT) to fuse multimodal medical image. By using the maximum selection rule, the complex wavelet coefficients of source images are fused. Source images at different levels are decomposed by DCxWT, followed by Inverse DCxWT to form the fused image. Compared to the other five methods, including Dual tree complex wavelet transform (DTCWT) based fusion and PCA based fusion, the proposed method achieved the best performance in terms of five measurements, including standard deviation, entropy, edge strength, fusion symmetry, and fusion factor. It was proved that the proposed method was robust to noises, including speckle, salt and pepper, while maintaining the property of shift-invariance.</p>
</sec>
<sec id="sec0033">
<label>5.1.3</label>
<title>Multi-temporal fusion and data acquisition</title>
<p id="para0111">Multi-temporal fusion is to fuse images taken at different times but of the same modality. Multi-temporal fusion enables easy detection of changes in images by subtracting one or multiple images from another. Data acquisition consists of separate recording and simultaneous recording. The choice between separate or simultaneous acquisition should be cautiously considered. Compared to separate recordings, where an image from each modality is acquired individually, simultaneous acquisitions have relatively lower data quality and more artifacts. For example, EEG-fMRI simultaneously acquires cardio-ballistic artifacts and MRI gradients. Data acquisition consists of separate recording and simultaneous recording <xref ref-type="bibr" rid="bib0226">[226]</xref>. In the MR-PET scenario, components of the MRI scanner would trigger degradation in PET scanning results. Therefore, the costs of simultaneous acquisition are higher than separate acquisition due to subject discomfort and long set-up time. However, there are also many cases in which costs are of less concern, given the benefits of simultaneous acquisitions.</p>
</sec>
<sec id="sec0034">
<label>5.1.4</label>
<title>Multi-view fusion</title>
<p id="para0112">In multi-view fusion, images of the same modality are taken under different conditions at the same time. This fusion technique is applied to increase the amount of information for the fused images, while source images are taken under different conditions.</p>
<p id="para0113">The choice between asymmetric and symmetric data fusion makes a significant difference in the integration and joint analysis of multimodal data. For integration methods falling under the asymmetric category, information from different modalities is assigned with different weights so that information from one modality could be treated as a constraint on the other modality. For instance, fMRI contrast maps limit the source localization of EEG/MEG. Modalities in symmetric data fusion, however, are treated equally in terms of spatial and temporal resolution as well as the uncertainty in the possible indirect relation to neural activity. Hypothesis-driven approaches and data-driven approaches are the two main categories of symmetric fusion approaches. Hypothesis-driven approaches, also called model-driven approaches, are usually in model-based setting, while data-driven approaches belong to blind source separation methods <xref ref-type="bibr" rid="bib0227">[227]</xref>. Examples of different fusion methods are shown in <xref ref-type="table" rid="tbl0003">Table 3</xref>
.<table-wrap id="tbl0003" position="float"><label>Table 3</label><caption><p>Examples of different fusion methods.</p></caption><alt-text id="alt0027">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Methods</th><th valign="top">Source Image 1</th><th valign="top">Source Image 2</th><th valign="top">Fused Image</th></tr></thead><tbody><tr><td valign="top">Multi-focus <xref ref-type="bibr" rid="bib0228">[228]</xref></td><td valign="top"><inline-graphic xlink:href="fx1_lrg.gif"><alt-text id="alt1">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx2_lrg.gif"><alt-text id="alt2">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx3_lrg.gif"><alt-text id="alt3">Image, table 3</alt-text></inline-graphic></td></tr><tr><td valign="top">Multi-temporal</td><td valign="top"><inline-graphic xlink:href="fx4_lrg.gif"><alt-text id="alt4">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx5_lrg.gif"><alt-text id="alt5">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx6_lrg.gif"><alt-text id="alt6">Image, table 3</alt-text></inline-graphic></td></tr><tr><td valign="top">Multi-view</td><td valign="top"><inline-graphic xlink:href="fx7_lrg.gif"><alt-text id="alt7">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx8_lrg.gif"><alt-text id="alt8">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx9_lrg.gif"><alt-text id="alt9">Image, table 3</alt-text></inline-graphic></td></tr><tr><td valign="top">Multi-modal <xref ref-type="bibr" rid="bib0229">[229]</xref></td><td valign="top"><inline-graphic xlink:href="fx10_lrg.gif"><alt-text id="alt10">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx11_lrg.gif"><alt-text id="alt11">Image, table 3</alt-text></inline-graphic></td><td valign="top"><inline-graphic xlink:href="fx12_lrg.gif"><alt-text id="alt12">Image, table 3</alt-text></inline-graphic></td></tr></tbody></table></table-wrap></p>
</sec>
</sec>
<sec id="sec0035">
<label>5.2</label>
<title>Fusion rules</title>
<p id="para0114">Image fusion rules are applied to highlight the features of interest in images while suppressing the unimportant features. Generally, fusion rules are mainly comprised of four components: activity-level measurement, coefficient grouping, coefficient combination, and consistency verification <xref ref-type="bibr" rid="bib0230">[230]</xref>.</p>
<sec id="sec0036">
<label>5.2.1</label>
<title>Components of fusion rules</title>
<p id="para0115">The activity-level measurement rule, as can be subdivided into window-based activity (WBA), coefficient-based activity (CBA) and region-based activity (RBA), characterizes coefficients at different scales. In the coefficient grouping component, there are mainly three typical groupings, including single-scale grouping (SG), multi-scale grouping (MG), and no-grouping (NG). SG indicates that the same strategy is applied to fuse different coefficients between sub-images on the same scale.</p>
<p id="para0116">The coefficient combination component mainly comprises of maximum rules (MR), weighted rules (WAR), and average rules (AR). MR can be given as:<disp-formula id="eqn0001"><label>(1)</label><mml:math altimg="si1.svg" id="M1"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mspace width="0.33em"></mml:mspace><mml:mtext>when</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">&gt;</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mspace width="0.33em"></mml:mspace><mml:mtext>when</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">&gt;</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula> where C<italic><sub>F</sub></italic> indicates the combined coefficient, <inline-formula><mml:math altimg="si2.svg" id="M2"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si3.svg" id="M3"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> are the coefficients of two input images at the level of <italic>j</italic>. For WAR, <inline-formula><mml:math altimg="si2.svg" id="M4"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si3.svg" id="M5"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> are combined by multiplying different weights <inline-formula><mml:math altimg="si4.svg" id="M6"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si5.svg" id="M7"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, that is<disp-formula id="eqn0002"><label>(2)</label><mml:math altimg="si6.svg" id="M8"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">×</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">×</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0118">AR, which has the coefficient of input images averaged, is a special case of WAR.<disp-formula id="eqn0003"><label>(3)</label><mml:math altimg="si7.svg" id="M9"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0119">The consistency verification component ensures that the same rules are applied to fuse the coefficients in the neighborhood.</p>
</sec>
<sec id="sec0037">
<label>5.2.2</label>
<title>Fusion levels</title>
<p id="para0120">There are three different levels of image fusion: pixel level, feature level, and decision level. This categorization can be seen in <xref ref-type="table" rid="tbl0004">Table 4</xref>
and <xref ref-type="fig" rid="fig0010">Fig. 10</xref>
. Pixel level rules directly deal with the information acquired from each pixel of source images and then generates pixel values for the fused image correspondingly. Feature level rules focus on regional information and features such as texture and salient features. The fused image in the decision level is acquired through rules of fuzzy logic and statistics. Before rules in feature level and decision level apply to the source images, segmentation of the source images is needed. Compared to the pixel level fusion, feature and decision level fusion shows more advantages are less affected by noises and misregistration. Feature and decision level fusion also shows better contrast and lower complexity <xref ref-type="bibr" rid="bib0231">[231]</xref>. In the following sections, we will introduce fusion rules for multi-model image fusion, followed by validation metrics.<table-wrap id="tbl0004" position="float"><label>Table 4</label><caption><p>Three fusion levels.</p></caption><alt-text id="alt0028">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Decision level</th><th valign="top">Feature level</th><th valign="top">Pixel level</th></tr></thead><tbody><tr><td valign="top">• Image segmentation</td><td valign="top">• Image segmentation</td><td valign="top">• Information acquisition of each pixel</td></tr><tr><td valign="top">• Fusion based on initial object detection and classification</td><td valign="top">• Fusion based on the properties</td><td valign="top">• Fusion of each pixel based on the information</td></tr></tbody></table></table-wrap><fig id="fig0010"><label>Fig. 10</label><caption><p>pixel-based and window-based fusion.</p></caption><alt-text id="alt0010">Fig. 10</alt-text><graphic xlink:href="gr10_lrg"></graphic></fig></p>
</sec>
<sec id="sec0038">
<label>5.2.3</label>
<title>Fuzzy logic</title>
<p id="para0121">Fuzzy logic-based rules belong to decision level fusion. These rules are usually used to solve challenges in blurry fused images. Mamdani and T-S models are two fuzzy logic models. The difference between them lies in the consequence parts. T-S models linearly mapped input variables into functions to form the consequence parts, while Mamdani models used fuzzy sets. The T-S model is more advantageous than the Mamdani model in regard to number of rules and accuracy.</p>
<p id="para0122">In general, feature extraction through fuzzy logic algorithms is performed prior to fusion, which generates pixel-wise features <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup> from two input images. The fusion procedure can be divided into three steps. In the first step, the fuzzy logic, which is usually comprised of four conditional rules, is used to label the individual pixels as following.<disp-formula id="eqn0004"><label>(4)</label><mml:math altimg="si8.svg" id="M10"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>w</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mtext>both</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>of</mml:mtext><mml:mspace width="0.33em"></mml:mspace></mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mspace width="0.33em"></mml:mspace><mml:mtext>and</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:msup><mml:mrow><mml:mtext>are</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>high</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>Rule</mml:mtext><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mtext>is</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>low</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>but</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:msup><mml:mrow><mml:mtext>is</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>high</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>Rule</mml:mtext><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd><mml:mtd><mml:mrow><mml:mspace width="2.em"></mml:mspace><mml:mtext>OR</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mtext>is</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>high</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>but</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:msup><mml:mrow><mml:mtext>is</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>low</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>Rule</mml:mtext><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mtext>both</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>of</mml:mtext><mml:mspace width="0.33em"></mml:mspace></mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mspace width="0.33em"></mml:mspace><mml:mtext>and</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:msup><mml:mrow><mml:mtext>are</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mtext>low</mml:mtext></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>Rule</mml:mtext><mml:mn>4</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0123">Then the new pixel-wise feature values can be calculated through:<disp-formula id="eqn0005"><label>(5)</label><mml:math altimg="si9.svg" id="M11"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mi>j</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>3</mml:mn></mml:mrow></mml:math></disp-formula> where 1, 2, 3 corresponds to low, medium and high components, respectively. <italic>α</italic> and <italic>β</italic> are the mean and variance of each component. By incorporating the center average defuzzifier to process fuzzy outputs, the weight of fuzzy logic can be obtained.</p>
</sec>
<sec id="sec0039">
<label>5.2.4</label>
<title>Statistics model</title>
<p id="para0125">The essence in statistics-based methods lies in the data-driven technique and high order statistics that can reveal the underlying pattern across multiple modes of data. Principal component analysis (PCA) <xref ref-type="bibr" rid="bib0232">[232]</xref>, <xref ref-type="bibr" rid="bib0233">[233]</xref>, <xref ref-type="bibr" rid="bib0234">[234]</xref>, <xref ref-type="bibr" rid="bib0235">[235]</xref> together with Hidden Markow Tree (HMT) <xref ref-type="bibr" rid="bib0236">[236]</xref>, <xref ref-type="bibr" rid="bib0237">[237]</xref>, <xref ref-type="bibr" rid="bib0238">[238]</xref> are two typical examples of statistic methods in the field of multi-modal medical image fusion.</p>
<p id="para0126">PCA is an orthogonal linear transformation that reveals the most valuable components of the input images. Let <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup> be the two coefficients of the input images that can be denoted as:<disp-formula id="eqn0006"><label>(6)</label><mml:math altimg="si10.svg" id="M12"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msubsup><mml:mi>X</mml:mi><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0007"><label>(7)</label><mml:math altimg="si11.svg" id="M13"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math altimg="si12.svg" id="M14"><mml:msubsup><mml:mi>X</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si13.svg" id="M15"><mml:msubsup><mml:mi>X</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> (1  ≤  <italic>j</italic>  ≤  <italic>M</italic>) are column vectors of two coefficients <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup>. The importance of components is related to the eigenvalues in the covariance matrix between <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup>. The covariance matrix can be computed through:<disp-formula id="eqn0008"><label>(8)</label><mml:math altimg="si14.svg" id="M16"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="bold-italic">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">−</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <bold><italic>E</italic></bold> is the expectation of vectors, <inline-formula><mml:math altimg="si15.svg" id="M17"><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math altimg="si16.svg" id="M18"><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula> corresponds to the average of <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup> respectively, that is<disp-formula id="eqn0009"><label>(9)</label><mml:math altimg="si17.svg" id="M19"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>X</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0010"><label>(10)</label><mml:math altimg="si18.svg" id="M20"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msubsup><mml:mi>X</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0129">Given the eigenvalues obtained from the covariance matrix as <bold><italic>Y</italic></bold>, the normalized weights <italic>w</italic>
<sup>1</sup> and <italic>w</italic>
<sup>2</sup> for <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup> can be denoted as:<disp-formula id="eqn0011"><label>(11)</label><mml:math altimg="si19.svg" id="M21"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0012"><label>(12)</label><mml:math altimg="si20.svg" id="M22"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0130">Therefore, the fused coefficient <italic>C<sup>F</sup></italic> can be the combination of two input images:<disp-formula id="eqn0013"><label>(13)</label><mml:math altimg="si21.svg" id="M23"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0131">The two-state HMT method, unlike PCA methods, can be deployed to model the coefficients. Two mixed Gaussian random distributions, as well as the hidden states, depict Intra-coefficients. The hidden states here refer to the one parent and four children coefficients. Given each coefficient denoted as <bold><italic>C</italic></bold>, the coefficient is obtained by probability density function:<disp-formula id="eqn0014"><label>(14)</label><mml:math altimg="si22.svg" id="M24"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> when the coefficient <italic>C<sup>i</sup></italic> in the state <italic>n</italic> (<italic>n</italic>=0, 1), <inline-formula><mml:math altimg="si23.svg" id="M25"><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:math></inline-formula>is the probability density function correspondingly. The fused coefficient is then given by:<disp-formula id="eqn0015"><label>(15)</label><mml:math altimg="si24.svg" id="M26"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mtext>if</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mo>≥</mml:mo><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mtext>if</mml:mtext><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&lt;</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
</sec>
<sec id="sec0040">
<label>5.2.5</label>
<title>Human vision system</title>
<p id="para0133">Methods based on the Human visual system (HVS) aim at solving the fusion problem in the way of image recognition and comprehension. The system includes components such as visibility, smallest univalve segment assimilating nucleus (SUSAN) <xref ref-type="bibr" rid="bib0239">[239]</xref>, and retina-inspired model (RIM) <xref ref-type="bibr" rid="bib0240">[240,</xref>
<xref ref-type="bibr" rid="bib0241">241]</xref>.</p>
<p id="para0134">The sharpness of an image can be quantified by visibility. Therefore, the images with higher visibility show lower blurriness. Given an image <bold><italic>I</italic></bold> of size <italic>h</italic>  × <italic>w</italic>, then the visibility of the <bold><italic>I</italic></bold> can be mathematically expressed as:<disp-formula id="eqn0016"><label>(16)</label><mml:math altimg="si25.svg" id="M27"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo>×</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>h</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>w</mml:mi></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msup></mml:mfrac><mml:mfrac><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>μ</italic> is acquired by calculating the mean grey value of the image <bold><italic>I</italic></bold> and <italic>α</italic>, the visual constant varying from 0.6 to 0.7.</p>
<p id="para0136">SUSAN, proposed in <xref ref-type="bibr" rid="bib0242">[242]</xref>, is a feature extraction algorithm inspired by HVS. SUSAN computes the feature of a pixel by considering a circular mask around the pixel. In the mask, the area consisting of pixels that have similar brightness to the nucleus, or the central pixel, is selected and is called Univalue Segment Assimilating Nucleus (USAN). Let the input image to be <bold><italic>I</italic></bold> and the circular mask with the radius <inline-formula><mml:math altimg="si26.svg" id="M28"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:math></inline-formula>, then simplest USAN can be given as:<disp-formula id="eqn0017"><label>(17)</label><mml:math altimg="si27.svg" id="M29"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo linebreak="badbreak">&gt;</mml:mo><mml:mi>L</mml:mi><mml:mspace width="0.33em"></mml:mspace></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>r</italic>
<sub>0</sub> is the central pixel, <italic>r</italic> is a nominal depicting surrounding pixels. <bold><italic>I</italic></bold>(<italic>r</italic>)is the pixel value at <italic>r</italic> and <italic>L</italic> is the brightness difference threshold. The value of <italic>L</italic>, which specifies the range of pixel values to be considered, must be carefully chosen because extracted features are sensitive to it. In order to lower the sensitiveness, the distance between pixels is also taken into consideration, and the extended USAN function is:<disp-formula id="eqn0018"><label>(18)</label><mml:math altimg="si28.svg" id="M30"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mi>L</mml:mi></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>ρ</italic> is the distance scaling factor.</p>
<p id="para0139">For components in intensity-hue saturation (IHS) decomposition methods, the RIM model can be used as the fusion rules. There are five layers in RIM. The first cone layer outputs an array of high-resolution cone photoreceptors with high resolution. The second layer is the extractor for spatial feature, while the third layers are horizontal cells. The fourth and fifth layers, which combine features, are bipolar and ganglion cells The RIM based image fusion rule can be demonstrated as:<disp-formula id="eqn0019"><label>(19)</label><mml:math altimg="si29.svg" id="M31"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>C</italic>
<sup>1</sup> and <italic>C</italic>
<sup>2</sup> stand for intensity components of two source images. <italic>h</italic>
<sup>1</sup> and <italic>h</italic>
<sup>2</sup> are the filters of feature extractors. The filter <italic>h</italic>
<sup>1</sup>, a high-scale spatial feature extractor, calculates the spatial difference between high-resolution and low-resolution. Filter <italic>h</italic>
<sup>2</sup> combines the output of horizontal cells.</p>
</sec>
<sec id="sec0041">
<label>5.2.6</label>
<title>Validation metrics</title>
<p id="para0141">Objective evaluation metrics are used to evaluate the efficacy of image fusion rules on improving the quality of the fused image. Widely used metrics includes spatial frequency (SF) <xref ref-type="bibr" rid="bib0243">[243]</xref>, the ratio of spatial frequency error (rSFe) <xref ref-type="bibr" rid="bib0244">[244]</xref>, wavelet entropy (WE) <xref ref-type="bibr" rid="bib0245">[245]</xref>, Signal noise ratio (SNR) <xref ref-type="bibr" rid="bib0246">[246]</xref>, mutual information (MI) <xref ref-type="bibr" rid="bib0247">[247]</xref> and directive contrast (DC) <xref ref-type="bibr" rid="bib0248">[248]</xref>.</p>
<p id="para0142">SF metric, which measures images’ activity, is generally used for PCA integrated HIS methods in multimodal image fusion. SF can be defined as<disp-formula id="eqn0020"><label>(20)</label><mml:math altimg="si30.svg" id="M32"><mml:mrow><mml:mtext>SF</mml:mtext><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula> where <italic>C<sub>F</sub></italic> and <italic>R<sub>F</sub></italic> stand for column frequency and row frequency, respectively.</p>
<p id="para0144">Based on SF, rSFe compares the <inline-formula><mml:math altimg="si31.svg" id="M33"><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:math></inline-formula> of the fused image with<inline-formula><mml:math altimg="si32.svg" id="M34"><mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mi>S</mml:mi><mml:msubsup><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si33.svg" id="M35"><mml:mrow><mml:mi>S</mml:mi><mml:msubsup><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> of the input images where SF’ can be extended as:<disp-formula id="eqn0021"><label>(21)</label><mml:math altimg="si34.svg" id="M36"><mml:mrow><mml:msup><mml:mtext>SF</mml:mtext><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula> where <italic>MD<sub>F</sub></italic> and <italic>SD<sub>F</sub></italic> are the main diagonal SF and the secondary diagonal SF. rSFe can then be formulated as:<disp-formula id="eqn0022"><label>(22)</label><mml:math altimg="si35.svg" id="M37"><mml:mrow><mml:mi>r</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mn>1</mml:mn><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mrow><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mn>1</mml:mn><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mn>2</mml:mn><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mrow><mml:msubsup><mml:mtext>SF</mml:mtext><mml:mn>2</mml:mn><mml:msup><mml:mrow></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0146">Calculated by multi-scale entropy, WE can be given by:<disp-formula id="eqn0023"><label>(23)</label><mml:math altimg="si36.svg" id="M38"><mml:mrow><mml:mi>W</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>i</italic> is the resolution level, <italic>p<sub>i</sub></italic> is the density distribution derived from the energy of the detail signal <italic>E<sub>i</sub></italic> and the total energy <italic>E<sub>T</sub></italic>
<disp-formula id="eqn0024"><label>(24)</label><mml:math altimg="si37.svg" id="M39"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0148">In <xref ref-type="bibr" rid="bib0246">[246]</xref>, the authors proposed SNR-based image fusion rules. The proposed fusion method can be formulated in the following form.<disp-formula id="eqn0025"><label>(25)</label><mml:math altimg="si38.svg" id="M40"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>l</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> for a specific region <italic>k. A</italic>(<italic>t</italic>) is the activity level of that, while <italic>M<sub>t</sub></italic> is the total number of pixels. The probability of pixel activity <italic>p<sub>l</sub></italic> can be calculated through:<disp-formula id="eqn0026"><label>(26)</label><mml:math altimg="si39.svg" id="M41"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>3.2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:munder><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>w</italic> is the weight of SNR from the image, <italic>N</italic> is the number of decomposition levels and <italic>d<sub>i</sub></italic>(<italic>j</italic>
<sub>1</sub>, <italic>j</italic>
<sub>2</sub>) is the detailed wavelet coefficient.</p>
<p id="para0150">MI is deployed as the fusion rule for WT-based multi-modal medical image fusion. <italic>MI</italic> between the input images <bold><italic>I</italic></bold>
<sub>1</sub> , <bold><italic>I</italic></bold>
<sub>2</sub> and the fused image <bold><italic>I</italic></bold>
<sub><bold><italic>F</italic></bold></sub>, is maximized to give <italic>C<sup>F</sup></italic>, which is acquired by:<disp-formula id="eqn0027"><label>(27)</label><mml:math altimg="si40.svg" id="M42"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>F</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0151">DC measures the difference between the pixel and its neighbors. The ratio between the high-frequency intensity <bold><italic>I</italic></bold>
<sub><italic>H</italic></sub> and the low-frequency intensity <bold><italic>I</italic></bold>
<sub><italic>L</italic></sub> is the intensity contrast <italic>DC</italic>.<disp-formula id="eqn0028"><label>(28)</label><mml:math altimg="si41.svg" id="M43"><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
</sec>
</sec>
<sec id="sec0042">
<label>5.3</label>
<title>Decomposition and reconstruction</title>
<p id="para0152">For the multi-modal image fusion of neuroimaging, the selection of various decomposition and reconstruction methods influence the fusion procedure and outcomes. In this survey, we shall discuss seven popular methods: (i) RGB-IHS; (ii) Pyramid representation; (iii) wavelet-based approach and its variants; (iv) multi-resolution analysis; (v) sparse representation; and (vi) salient features.</p>
<sec id="sec0043">
<label>5.3.1</label>
<title>RGB-IHS</title>
<p id="para0153">The intensity-hue-saturation (IHS) model <xref ref-type="bibr" rid="bib0249">[249]</xref> helps transform the original image in RGB color space to hue, saturation, and intensity channels. This RGB to IHS procedure is calculated by simple equations. Besides, the reconstruction is carried out by inversed transformation (IHS to RGB).</p>
<p id="para0154">For the RGB-&gt;IHS procedure, the intensity of each input image was estimated by the following equations:<disp-formula id="eqn0029"><label>(29)</label><mml:math altimg="si42.svg" id="M44"><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0155">Then, we consider three conditions: <italic>C</italic>
<sub>1</sub>: <italic>B</italic>&lt;<italic>R,G; C</italic>
<sub>2</sub>: <italic>R</italic>&lt;<italic>B,G; C</italic>
<sub>3</sub>: <italic>G</italic>&lt;<italic>R,B</italic>.</p>
<p id="para0156">The hue value is obtained by<disp-formula id="eqn0030"><label>(30)</label><mml:math altimg="si43.svg" id="M45"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>G</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0157">The saturation value was yielded from the equation<disp-formula id="eqn0031"><label>(31)</label><mml:math altimg="si44.svg" id="M46"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0158">On the other hand, after the revision was performed on the <italic>I, H</italic>, and <italic>S</italic> components, we can convert from IHS color space to original RGB space based on three various conditions:</p>
<p id="para0159">For <italic>C</italic>
<sub>1</sub>, we have<disp-formula id="eqn0032"><label>(32)</label><mml:math altimg="si45.svg" id="M47"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>2</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0160">For <italic>C</italic>
<sub>2</sub>, we have<disp-formula id="eqn0033"><label>(33)</label><mml:math altimg="si46.svg" id="M48"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>5</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>4</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0161">For <italic>C</italic>
<sub>3</sub>, we have<disp-formula id="eqn0034"><label>(34)</label><mml:math altimg="si47.svg" id="M49"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>7</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo linebreak="badbreak">=</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>8</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>3</mml:mn><mml:mi>S</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p>
</sec>
<sec id="sec0044">
<label>5.3.2</label>
<title>Pyramid representation</title>
<p id="para0162">Pyramid representation (PR) is commonly employed in image fusion. PR is a typical multi-scale signal representation approach that can be used for 1D signals, 2D images, etc. <xref ref-type="fig" rid="fig0011">Fig. 11</xref>
shows a toy example using the cameraman picture. There are two pyramids commonly seen in practice: lowpass pyramid and bandpass pyramid. The former smooths the image and then under-samples the smoothed image. The latter generates the difference between images at adjacent levels and carry out image interpolation between neighboring levels of resolution.<fig id="fig0011"><label>Fig. 11</label><caption><p>A toy example showing pyramid representation.</p></caption><alt-text id="alt0011">Fig. 11</alt-text><graphic xlink:href="gr11_lrg"></graphic></fig></p>
<p id="para0163">Let us assume we have input image <italic>I</italic>. We have a series of pyramid representations of this input image <italic>I</italic> as<disp-formula id="eqn0035"><label>(35)</label><mml:math altimg="si48.svg" id="M50"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p>
<p id="para0164">Suppose <italic>F<sub>k</sub></italic> is the filter and down-sampling, PR can be obtained by<disp-formula id="eqn0036"><label>(36)</label><mml:math altimg="si49.svg" id="M51"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0037"><label>(37)</label><mml:math altimg="si50.svg" id="M52"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>r<sub>k</sub></italic> is the residual image of <italic>I</italic> at <italic>i</italic>-th level, and <italic>k</italic> is in the range of<disp-formula id="eqn0038"><label>(38)</label><mml:math altimg="si51.svg" id="M53"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>K</italic> is the maximum iteration number.</p>
<p id="para0167">After modification on the PR of input images, the fused image <italic>I<sub>F</sub></italic> is obtained by inverse pyramid transformation (IPT) as<disp-formula id="eqn0039"><label>(39)</label><mml:math altimg="si52.svg" id="M54"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0168">The process of PR based fusion is shown in <xref ref-type="fig" rid="fig0012">Fig. 12</xref>
.<fig id="fig0012"><label>Fig. 12</label><caption><p>Diagram of two-level pyramid representation based fusion.</p></caption><alt-text id="alt0012">Fig. 12</alt-text><graphic xlink:href="gr12_lrg"></graphic></fig></p>
</sec>
<sec id="sec0045">
<label>5.3.3</label>
<title>Wavelet-based method</title>
<p id="para0169">Wavelet transform (WT) based fusion is one of the multi-scale analysis methods. The idea is simple, as shown in <xref ref-type="fig" rid="fig0013">Fig. 13</xref>
. WT will decompose the input images into low-frequency (LF) and high-frequency (HF) subbands. The corresponding image fusion rules will be applied to fuse LF and HF subbands. Finally, the fused image is yielded by the WT reconstruction technique <xref ref-type="bibr" rid="bib0250">[250]</xref>.<fig id="fig0013"><label>Fig. 13</label><caption><p>Diagram of wavelet-based fusion methods.</p></caption><alt-text id="alt0013">Fig. 13</alt-text><graphic xlink:href="gr13_lrg"></graphic></fig></p>
<p id="para0170">Continuous wavelet transform (CWT) decomposes a square-integrable function <italic>S</italic>(<italic>t</italic>) as follows<disp-formula id="eqn0040"><label>(40)</label><mml:math altimg="si53.svg" id="M55"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mstyle mathvariant="normal"><mml:mi>Φ</mml:mi></mml:mstyle></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mstyle mathvariant="normal"><mml:mi>Φ</mml:mi></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula> where<disp-formula id="eqn0041"><label>(41)</label><mml:math altimg="si54.svg" id="M56"><mml:mrow><mml:msub><mml:mstyle mathvariant="normal"><mml:mi>Φ</mml:mi></mml:mstyle><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo linebreak="goodbreak">×</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Φ</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:mfrac></mml:mstyle><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0172">Here, the wavelet Φ<sub><italic>c, b</italic></sub>(<italic>t</italic>) is calculated from the mother wavelet Φ(<italic>t</italic>) by translation and dilation. The dilation factor <italic>c</italic> and translation factor <italic>b</italic> are all positive numbers.</p>
<p id="para0173">On the other hand, the discrete wavelet transform (DWT) captures both spatial information along <italic>x</italic> and <italic>y</italic> axes, by discretizing above two equations<disp-formula id="eqn0042"><label>(42)</label><mml:math altimg="si55.svg" id="M57"><mml:mrow><mml:mi>c</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0043"><label>(43)</label><mml:math altimg="si56.svg" id="M58"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>q</mml:mi><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0174">Then, suppose a low-pass filter (LPF) <inline-formula><mml:math altimg="si57.svg" id="M59"><mml:mi mathvariant="double-struck">g</mml:mi></mml:math></inline-formula> and a high-pass filter (HPF) <inline-formula><mml:math altimg="si58.svg" id="M60"><mml:mi mathvariant="double-struck">h</mml:mi></mml:math></inline-formula> are created, we have<disp-formula id="eqn0044"><label>(44)</label><mml:math altimg="si59.svg" id="M61"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">g</mml:mi><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:msup><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>↓</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0045"><label>(45)</label><mml:math altimg="si60.svg" id="M62"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:msubsup><mml:mi mathvariant="double-struck">h</mml:mi><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:msup><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>↓</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>n</italic> is the discrete version of time <italic>t, e</italic> represents the coefficients, <italic>a</italic> and <italic>d</italic> correspond to approximation and detail, respectively. The symbol ↓ represents the downsampling operation.<disp-formula id="eqn0046"><label>(46)</label><mml:math altimg="si61.svg" id="M63"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>↓</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0176">The above decomposition process can be iterated with successive approximations being decomposed in turn, so that one signal is broken down into various levels of resolution.</p>
<p id="para0177">When <italic>I</italic>(<italic>n</italic>) is extended to be a 2D brain image <italic>I</italic>(<italic>m, n</italic>), the 1D-DWT is applied to row and column directions separately. The approximation (<italic>a</italic>) and detail (<italic>d</italic>) subbands now expand to four subbands: approximation subband (<italic>a</italic>), horizontal subband (<italic>h</italic>), vertical subband (<italic>v</italic>), and diagonal subband (<italic>d</italic>), as shown in <xref ref-type="fig" rid="fig0014">Fig. 14</xref>
.<disp-formula id="eqn0047"><label>(47)</label><mml:math altimg="si62.svg" id="M64"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0048"><label>(48)</label><mml:math altimg="si63.svg" id="M65"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0049"><label>(49)</label><mml:math altimg="si64.svg" id="M66"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0050"><label>(50)</label><mml:math altimg="si65.svg" id="M67"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mi mathvariant="double-struck">h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<fig id="fig0014"><label>Fig. 14</label><caption><p>Diagram of 2D-DWT.</p></caption><alt-text id="alt0014">Fig. 14</alt-text><graphic xlink:href="gr14_lrg"></graphic></fig></p>
<p id="para0178">2D-DWT can even be generalized to 3D-DWT. A straightforward implementation is to apply 1D-DWT to row, column, and slice directions, respectively. <xref ref-type="fig" rid="fig0015">Fig. 15</xref>
shows an example of carrying out 3D-DWT to a cubic image. Instead of using <italic>a, h, v</italic>, and <italic>d</italic>, we now use LLL, LLH, LHL, LHH, HLL, HLH, HHL, and HHH to represent the eight subbands in 3D-DWT <xref ref-type="bibr" rid="bib0251">[251]</xref>. Here L and H represent the result after LPF and HPF, respectively.<fig id="fig0015"><label>Fig. 15</label><caption><p>Illustration of performing 3D-DWT on a cubic image.</p></caption><alt-text id="alt0015">Fig. 15</alt-text><graphic xlink:href="gr15_lrg"></graphic></fig></p>
</sec>
<sec id="sec0046">
<label>5.3.4</label>
<title>Variants of wavelet-based analysis</title>
<p id="para0179">The DWT can provide better performances than traditional signal processing techniques, but it lacks translation-invariance and directional selectivity. Hence, new variants of wavelet-based techniques have been applied to multi-modality image fusion.</p>
<p id="para0180">The stationary wavelet transform (SWT) can solve the shift-variance problem by getting rid of the downsampling operation from ordinary DWT. SWT can provide more details and texture information than DWT. Prakash and Khare <xref ref-type="bibr" rid="bib0252">[252]</xref> fused CT and MR images based on SWT by Modulus Maxima. Pawar and Kadam <xref ref-type="bibr" rid="bib0253">[253]</xref> used SWT and convolutional sparse representation for multi-modal image fusion.</p>
<p id="para0181">The DWT calculates each decomposition level by passing only the previous approximation coefficients to quadrature mirror filters (QMF). Nevertheless, the discrete wavelet packet transform (DWPT) <xref ref-type="bibr" rid="bib0254">[254,</xref>
<xref ref-type="bibr" rid="bib0255">255]</xref> passes all coefficients (both approximation and detail) through QMF to create a full binary tree. Sreekala and Kuncheria <xref ref-type="bibr" rid="bib0256">[256]</xref> used WPT to implement misaligned image fusion. Shah, Merchant <xref ref-type="bibr" rid="bib0257">[257]</xref> combined curvelet wavelet and the WPT method to carry out image fusion.</p>
<p id="para0182">To increase directional selectivity, the dual-tree complex wavelet transform (DTCWT) used two separate two-channel filter banks. The scaling and wavelet filters in the dual-tree cannot be selected arbitrarily. In one tree, the wavelet and scaling filters produce a wavelet and scaling function, which are approximate Hilbert transforms of those generated by another tree. At each level of 2D DTCWT, it produces a total of six directionally selective subbands (±15<bold>°</bold>, ±45<bold>°</bold>, ±75<bold>°</bold>). <xref ref-type="fig" rid="fig0016">Fig. 16</xref>
presented a comparison of DWT, SWT, DWPT, and DTCWT.<fig id="fig0016"><label>Fig. 16</label><caption><p>Comparison of different wavelet-based decomposition techniques.</p></caption><alt-text id="alt0016">Fig. 16</alt-text><graphic xlink:href="gr16_lrg"></graphic></fig></p>
<p id="para0183">Lift scheme (LS) is a solution to reduce computation time and accelerate the design and application of DWT <xref ref-type="bibr" rid="bib0258">[258]</xref>. The idea of LS is to factorize DWT with finite filters, into a sequence of ordinary convolution operators, which are called “lifting steps” <xref ref-type="bibr" rid="bib0259">[259]</xref>. This procedure can reduce the arithmetic operations by almost two. Mathematically, the analysis filters <inline-formula><mml:math altimg="si66.svg" id="M68"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="double-struck">h</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> can be formulated in the form of polyphase matrix<disp-formula id="eqn0051"><label>(51)</label><mml:math altimg="si67.svg" id="M69"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">h</mml:mi><mml:mtext>even</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">h</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">g</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0184">This polyphase matrix P(z) is a 2 × 2 matrix, which contains the analysis LPF and HPF, each split up into their even and odd polynomial coefficients and normalized.<disp-formula id="eqn0052"><label>(52)</label><mml:math altimg="si68.svg" id="M70"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="double-struck">b</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0185">P(z) is then factored into a series of 2 × 2 upper triangular matrices (UTM) and lower triangular matrices (LTM), each with diagonal entries equal to 1. UTM contains the coefficients <inline-formula><mml:math altimg="si69.svg" id="M71"><mml:mi mathvariant="double-struck">a</mml:mi></mml:math></inline-formula> for prediction, while LTM contains the coefficients <inline-formula><mml:math altimg="si70.svg" id="M72"><mml:mi mathvariant="double-struck">b</mml:mi></mml:math></inline-formula> for updates. Prakash, Park <xref ref-type="bibr" rid="bib0260">[260]</xref> used LS based biorthogonal wavelet transform to realize the multi-scale fusion of multimodal medical images. Haouam, Beladgham <xref ref-type="bibr" rid="bib0261">[261]</xref> used the level-set method and LS-based CDF wavelet to compress magnetic resonance images.</p>
</sec>
<sec id="sec0047">
<label>5.3.5</label>
<title>Other multi-resolution analysis</title>
<p id="para0186">Apart from the wavelet analysis, scholars have also proposed other multi-resolution analysis (MRA) methods for multimodal image fusion. The wavelet does not work well in detecting smoothness along the edges, and it lacks directional resolution because it only has three high-frequency subbands. The contourlet transform (CT) utilizes the contour segments to capture the geometrical structures of the input images. The procedure is two-stage: First, the Laplacian pyramid (LP) performs multi-scale decomposition, capturing point discontinuities. Second, directional filter bank (DFB) yields directional information and forms those point discontinuities into a linear structure. The flowchart of CT is shown in <xref ref-type="fig" rid="fig0017">Fig. 17</xref>
.<fig id="fig0017"><label>Fig. 17</label><caption><p>Diagram of contour transform.</p></caption><alt-text id="alt0017">Fig. 17</alt-text><graphic xlink:href="gr17_lrg"></graphic></fig></p>
<p id="para0187">Similar to stationary wavelet transform developed from discrete wavelet transform, nonsubsampled contourlet transform (NSCT) <xref ref-type="bibr" rid="bib0262">[262]</xref> was also developed due to the shift-variance of CT. Ramlal, Sachdeva <xref ref-type="bibr" rid="bib0263">[263]</xref> proposed an improved multimodal medical image fusion scheme, via a hybrid combination technique of NSCT and SWT. Li, Wang <xref ref-type="bibr" rid="bib0264">[264]</xref> presented a new practical medical image enhancement based on NSCT. Wang, Zhao <xref ref-type="bibr" rid="bib0265">[265]</xref> used NSCT and simplified-spatial frequency-pulse coupled neural network to develop a multi-modal functional/anatomical medical image fusion framework.</p>
<p id="para0188">Wavelets also fail to capture the geometric regularity along the singularities of surfaces, because of their isotropic support. Shearlet transform (ST) is one of the best sparse directional image representation methods. <xref ref-type="fig" rid="fig0018">Fig. 18</xref>
showed the ST coefficients, where the input image is <xref ref-type="fig" rid="fig0016">Fig. 16</xref>(a). Li, Wang <xref ref-type="bibr" rid="bib0266">[266]</xref> proposed a novel medical image fusion approach based on nonsubsampled ST (NSST). Vishwakarma and Bhuyan <xref ref-type="bibr" rid="bib0267">[267]</xref> offered a new image fusion framework via adjustable NSST. Akbarpour, Shamsi <xref ref-type="bibr" rid="bib0268">[268]</xref> suggested a novel combination of NSST and principal component averaging.<fig id="fig0018"><label>Fig. 18</label><caption><p>Example of coefficients of shearlet transform.</p></caption><alt-text id="alt0018">Fig. 18</alt-text><graphic xlink:href="gr18_lrg"></graphic></fig></p>
</sec>
<sec id="sec0048">
<label>5.3.6</label>
<title>Sparse representation</title>
<p id="para0189">Different from standard multi-scale analytic methods, the sparse representation (SR) assumes that both high-frequency and low-frequency components share the same set of sparse coefficients <xref ref-type="bibr" rid="bib0269">[269]</xref>. SR based fusion method root from compressed sensing. Nowadays, there are some famous variants of SR, such as group sparse representation and joint sparse representation (JSP) <xref ref-type="bibr" rid="bib0270">[270]</xref>, the diagram of which is shown in <xref ref-type="fig" rid="fig0019">Fig. 19</xref>
.<fig id="fig0019"><label>Fig. 19</label><caption><p>Diagram of JSP based fusion method.</p></caption><alt-text id="alt0019">Fig. 19</alt-text><graphic xlink:href="gr19_lrg"></graphic></fig></p>
<p id="para0190">From <xref ref-type="fig" rid="fig0019">Fig. 19</xref> we can observe the detailed steps of JSP <xref ref-type="bibr" rid="bib0271">[271]</xref>. First, both input images <italic>I</italic>
<sub>1</sub> and <italic>I</italic>
<sub>2</sub> are transformed into vectors <inline-formula><mml:math altimg="si71.svg" id="M73"><mml:mi mathvariant="bold-script">B</mml:mi></mml:math></inline-formula> via sliding window<disp-formula id="eqn0053"><label>(53)</label><mml:math altimg="si72.svg" id="M74"><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mtext>SW</mml:mtext><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> where SW represents the sliding window method. An over-complete dictionary sparsely represents the vectors of both images<disp-formula id="eqn0054"><label>(54)</label><mml:math altimg="si73.svg" id="M75"><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math altimg="si74.svg" id="M76"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup></mml:math></inline-formula> represents the intersection of vectors of images <italic>I</italic>
<sub>1</sub> and <italic>I</italic>
<sub>2</sub>, and <inline-formula><mml:math altimg="si75.svg" id="M77"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:msup></mml:math></inline-formula> represents their differences. The two variances <inline-formula><mml:math altimg="si74.svg" id="M78"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si75.svg" id="M79"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:msup></mml:math></inline-formula> can be obtained by<disp-formula id="eqn0055"><label>(55)</label><mml:math altimg="si76.svg" id="M80"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0056"><label>(56)</label><mml:math altimg="si77.svg" id="M81"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">×</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mi>U</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>E</italic> stands for the over-complete dictionary, and <italic>T<sup>C</sup></italic> and <italic>T<sup>U</sup></italic> denote the sparse coefficients (SC) of <inline-formula><mml:math altimg="si74.svg" id="M82"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math altimg="si75.svg" id="M83"><mml:msup><mml:mrow><mml:mi mathvariant="bold-script">B</mml:mi></mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:msup></mml:math></inline-formula>.</p>
<p id="para0194">Afterward, SCs from both input images are combined using fusion rules <inline-formula><mml:math altimg="si78.svg" id="M84"><mml:mi mathvariant="bold-script">G</mml:mi></mml:math></inline-formula> as<disp-formula id="eqn0057"><label>(57)</label><mml:math altimg="si79.svg" id="M85"><mml:mrow><mml:msub><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="bold-script">G</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:msup><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mi mathvariant="normal">U</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>T<sub>F</sub></italic> stands for the fused SC. The over-complete dictionary can generate the fused vectors<disp-formula id="eqn0058"><label>(58)</label><mml:math altimg="si80.svg" id="M86"><mml:mrow><mml:msub><mml:mi mathvariant="bold-script">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math altimg="si81.svg" id="M87"><mml:msub><mml:mi mathvariant="bold-script">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:msub></mml:math></inline-formula> stands for the fused vector.</p>
<p id="para0197">Finally, we can transform <inline-formula><mml:math altimg="si81.svg" id="M88"><mml:msub><mml:mi mathvariant="bold-script">B</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:msub></mml:math></inline-formula> to image space and get the fused image <italic>I<sub>F</sub></italic>.</p>
</sec>
<sec id="sec0049">
<label>5.3.7</label>
<title>Salient feature</title>
<p id="para0198">Salient feature based fusion approaches are a type of novel methods, with the benefits of shift-invariance, low-cost computation, and saliency-feature preservation. The edge-preserving filter (EPF) is an important research method among all salient feature fusion approaches. Scholars have proposed many EPF approaches, such as local extrema scheme <xref ref-type="bibr" rid="bib0272">[272]</xref>, multi-scale edge-preserving decomposition <xref ref-type="bibr" rid="bib0273">[273]</xref>, edge-preserving smoothing pyramid <xref ref-type="bibr" rid="bib0274">[274]</xref>.</p>
<p id="para0199">
<xref ref-type="fig" rid="fig0020">Fig. 20</xref>
shows the diagram of the EPF-based image fusion approach, where BL and DL represent the base layer and detailed layer, respectively. The procedures of EPF-based image fusion approach are listed below:<fig id="fig0020"><label>Fig. 20</label><caption><p>Diagram of EPF based fusion approach.</p></caption><alt-text id="alt0020">Fig. 20</alt-text><graphic xlink:href="gr20_lrg"></graphic></fig></p>
<p id="para0200">First, both input images are decomposed into base layer (BL) and detailed layer (DL) by edge-preserving filters. The BL of each input image at different scales is obtained by<disp-formula id="eqn0059"><label>(59)</label><mml:math altimg="si82.svg" id="M89"><mml:mrow><mml:msubsup><mml:mi>B</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>k</italic> ∈ [1, 2] means the index of two input images, and <italic>j</italic> is the level of decomposition, and <italic>F<sub>j</sub></italic> represents the EPF at <italic>j</italic>-th level. Similarly, the DLs are obtained by<disp-formula id="eqn0060"><label>(60)</label><mml:math altimg="si83.svg" id="M90"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0202">Afterward, the BL and DL fusion rules <inline-formula><mml:math altimg="si84.svg" id="M91"><mml:msub><mml:mi mathvariant="double-struck">G</mml:mi><mml:mi mathvariant="normal">B</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math altimg="si85.svg" id="M92"><mml:msub><mml:mi mathvariant="double-struck">G</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:msub></mml:math></inline-formula> are applied to corresponding BLs and DLs, respectively;<disp-formula id="eqn0061"><label>(61)</label><mml:math altimg="si86.svg" id="M93"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">G</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0062"><label>(62)</label><mml:math altimg="si87.svg" id="M94"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">G</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>B<sub>f</sub></italic> and <italic>D<sub>f</sub></italic> are the fused BL and DL, and <italic>J</italic> is the maximum decomposition level. Finally, the fused image <italic>I<sub>F</sub></italic> is recovered by the summation below or other weighted equations.<disp-formula id="eqn0063"><label>(63)</label><mml:math altimg="si88.svg" id="M95"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
</p>
</sec>
</sec>
</sec>
<sec id="sec0050">
<label>6</label>
<title>Multimodal imaging data fusion: assessment</title>
<p id="para0204">Assessment is an essential component of multimodal data fusion that provides perspective into the quality of fusion results. There are two main forms of qualitative assessment: subjective quality assessment and objective quality assessment. In this part, we will review the conventional techniques and metrics applied in both forms.</p>
<sec id="sec0051">
<label>6.1</label>
<title>Subjective quality assessment</title>
<p id="para0205">Subjective assessments are established as a reliable form of quality evaluation in image fusion <xref ref-type="bibr" rid="bib0275">[275]</xref>. Professional subjective assessments based on medical and radiology expertise are widely applied in neuroimaging studies. Conventional subjective quality assessments are in the form of surveys with a set predesignated of questions and solutions, which represents a non-linear mapping to quantitative quality metrics <xref ref-type="bibr" rid="bib0276">[276]</xref>. Typical questions in neuroimaging fusion are the number of artifacts or distortion, while typical solutions can be in the form of basic descriptors (e.g. none, minimal, some, substantial) associated with continuous scores. Subjects are required to answer questions with respect to a set of images. It is ideal for image sets between subjects to overlap in order to ensure multiple surveys for the same image. With a set of high-quality fused images as references, the survey scores are then converted to difference scores with respect to scores of reference images. The difference scores are then converted to Z-scores and rescaled to the required range for analysis <xref ref-type="bibr" rid="bib0276">[276]</xref>. The double stimulus method is commonly applied to account for the potential misalignment in quality scales between different image sets or different assessment sessions. In the double stimulus method, both surveys and references are randomly included in the test set <xref ref-type="bibr" rid="bib0277">[277]</xref>. Conventional quality metrics obtained using these methods are the mean opinion score (MOS) and difference mean opinion score (DMOS). Data noise elimination and outlier removal for images &amp; subjects are then performed after obtaining the scores. Subjective quality assessments are vulnerable to both intra-expert and inter-expert variability, while the requirement for expertise significantly increases the cost of assessments. For extensive studies, considerable efforts are required to standardize and maintain assessment protocols for optimal consistency <xref ref-type="bibr" rid="bib0278">[278]</xref>.</p>
</sec>
<sec id="sec0052">
<label>6.2</label>
<title>Objective quality assessment</title>
<p id="para0206">In contrast to subjective Quality Assessment (QA), which involves a complex organization of observers and strict tests, objective QA only requires the computation of a single numerical score. Objective QA should be consistent with subjective QA, and often relies on statistical properties of the images, or even stochastic modeling of the Human Visual System (HVS) <xref ref-type="bibr" rid="bib0279">[279]</xref>.</p>
<p id="para0207">Objective QA can be classified according to the reference (distortion-free) image <italic>I<sub>I</sub></italic> used in the score computation. The best-case scenario is the <bold>full-reference</bold> QA <xref ref-type="bibr" rid="bib0280">[280]</xref>, in which <italic>I<sub>I</sub></italic>, with the same resolution as the fused image <italic>I<sub>F</sub></italic>, is known. This approach is, by far, the most widely extended. However, this full-reference is rarely available in practical applications. For example, in multi-spectral satellite imaging or medical imaging, some image modalities are acquired at a lower resolution and then fused with higher-resolution gray-level images. This is known as the <bold>reduced-reference</bold> case, where <italic>I<sub>I</sub></italic> is sometimes available at a coarser resolution and sometimes just a set of extracted features, and the QA is performed in this feature space. Finally, a third case is when <bold>no reference</bold> is available at all <xref ref-type="bibr" rid="bib0281">[281]</xref>. In this case, <italic>I<sub>F</sub></italic> is considered by itself, and measures like entropy and contrast are computed using only the information contained within.</p>
<p id="para0208">Depending on the type of quantification used, the metrics can be classified into “signal distortion” and “salient feature” categories. The former uses strict mathematical theory to assess quality, e.g. with entropy, standard deviation, etc. The later is grounded on a modeling of the HVS to assess salient feature transferred from <italic>I<sub>I</sub></italic> to <italic>I<sub>F</sub></italic>.</p>
<sec id="sec0053">
<label>6.2.1</label>
<title>Signal distortion based metrics</title>
<p id="para0209">The first signal distortion metric is a commonly used statistic, the standard deviation (STD) between the input image <italic>I<sub>I</sub></italic> and <italic>I<sub>F</sub></italic>. Let us consider the images as a function <italic>I<sub>I</sub></italic>(<italic>x, y</italic>) and <italic>I<sub>F</sub></italic>(<italic>x, y</italic>) defined in the range <inline-formula><mml:math altimg="si89.svg" id="M96"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si90.svg" id="M97"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>, with <italic>M</italic> the number of rows and <italic>N</italic> the number of columns in the image. The STD is computed as:<disp-formula id="eqn0064"><label>(64)</label><mml:math altimg="si91.svg" id="M98"><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math altimg="si92.svg" id="M99"><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> being the mean intensity of the fused image.</p>
<p id="para0210">Similar is the Root Mean Squared Error (RMSE), also widely used in many applications, included objective QA:<disp-formula id="eqn0065"><label>(65)</label><mml:math altimg="si93.svg" id="M100"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0211">An additional measure based only on pixel intensities is the Sharpness (SP), that reflects the level of detail transferred to <italic>I<sub>F</sub></italic>:<disp-formula id="eqn0066"><label>(66)</label><mml:math altimg="si94.svg" id="M101"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd></mml:mtd><mml:mtd></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd><mml:mtd></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="0.16em"></mml:mspace><mml:mo linebreak="goodbreak">×</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd><mml:mtd></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="0.16em"></mml:mspace></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p>
<p id="para0212">And finally, the Peak Signal-to-Noise Ratio (PSNR), which is perhaps the most widely used objective quality assessment:<disp-formula id="eqn0067"><label>(67)</label><mml:math altimg="si95.svg" id="M102"><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>10</mml:mn><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>L</italic> is the number of intensity levels, typically 256 for 8-bit images.</p>
<p id="para0214">Now there are many other measures based on information theory and entropy <xref ref-type="bibr" rid="bib0282">[282]</xref>. To perform this computation, let us note <italic>P</italic>(<italic>i</italic>∨<italic>I</italic>) as the ratio of pixels with gray value equal to <italic>i</italic> over the total number of pixels <italic>N</italic> × <italic>M</italic> of image <italic>I</italic>. We define the Entropy (EN) as:<disp-formula id="eqn0068"><label>(68)</label><mml:math altimg="si96.svg" id="M103"><mml:mrow><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0215">The difference of entropy (DEN) <xref ref-type="bibr" rid="bib0282">[282]</xref> quantifies the differences between <italic>I<sub>F</sub></italic> and <italic>I<sub>I</sub></italic>. The smaller, the better:<disp-formula id="eqn0069"><label>(69)</label><mml:math altimg="si97.svg" id="M104"><mml:mrow><mml:mi>D</mml:mi><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0216">Another possibility when a reference image is available is to compute the Cross-Entropy (CE) between the reference <italic>I<sub>I</sub></italic> and the fused image <italic>I<sub>F</sub></italic>:<disp-formula id="eqn0070"><label>(70)</label><mml:math altimg="si98.svg" id="M105"><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">/</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0217">From CE we can derive the Overall Cross-Entropy (OCE), which measures the entropy between several input images and the fused image. The formula below refers to <italic>K</italic> images, but the most common case is the one that uses just two images, <italic>I</italic>
<sub>1</sub> and <italic>I</italic>
<sub>2</sub>:<disp-formula id="eqn0071"><label>(71)</label><mml:math altimg="si99.svg" id="M106"><mml:mrow><mml:mi>O</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0218">In Sheikh and Bovik <xref ref-type="bibr" rid="bib0276">[276]</xref>, the authors propose the Visual Information Fidelity (VIF) measure. This measure quantifies how much of the information at <italic>I<sub>I</sub></italic> can be extracted from <italic>I<sub>F</sub></italic>. It is based on a Gaussian Scale Mixture (GSM) random field source model and a modelling of the HVS as a distortion channel. It uses the information <italic>MI</italic>(<italic>X, Y</italic>) between two given images, and sums over all subbands as in:<disp-formula id="eqn0072"><label>(72)</label><mml:math altimg="si100.svg" id="M107"><mml:mrow><mml:mi>V</mml:mi><mml:mi>I</mml:mi><mml:mi>F</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>I</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math altimg="si101.svg" id="M108"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>I</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si102.svg" id="M109"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> represent the information that could ideally be extracted by the brain in the reference and fusion images. <inline-formula><mml:math altimg="si103.svg" id="M110"><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math altimg="si104.svg" id="M111"><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>F</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math altimg="si105.svg" id="M112"><mml:msubsup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mi>I</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> represent <italic>N</italic> elements of the GSM model for the visual model (<italic>C</italic>), reference image (<italic>I<sub>I</sub></italic>) and fused image (<italic>I<sub>F</sub></italic>) in subband <italic>j</italic>. A more detailed explanation of the computation of these coefficients is found at Sheikh and Bovik <xref ref-type="bibr" rid="bib0276">[276]</xref>.</p>
<p id="para0220">However, Hossny, Nahavandi <xref ref-type="bibr" rid="bib0283">[283]</xref> propose a new formulation of mutual information (MI), based on the joint statistical distribution of two random variables. To do so, we note the mutual entropy as:<disp-formula id="eqn0073"><label>(73)</label><mml:math altimg="si106.svg" id="M113"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∨</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∨</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∨</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∨</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula> where <italic>P</italic>(<italic>i</italic>∨<italic>X</italic>) and <italic>P</italic>(<italic>j</italic>∨<italic>Y</italic>) are the probability of intensity <italic>i</italic> and <italic>j</italic> on the images <italic>X</italic> and <italic>Y</italic> respectively, and <italic>P</italic>(<italic>i, j</italic>∨<italic>X, Y</italic>) is their joint probability. Note that <inline-formula><mml:math altimg="si107.svg" id="M114"><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, identical to <xref ref-type="disp-formula" rid="eqn0068">Eq. (68)</xref>.</p>
<p id="para0222">The Mutual Information (MI) is then obtained in <xref ref-type="bibr" rid="bib0284">[284]</xref> as:<disp-formula id="eqn0074"><label>(74)</label><mml:math altimg="si108.svg" id="M115"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> with <inline-formula><mml:math altimg="si109.svg" id="M116"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to obtain them with no reference images. However, the <italic>H</italic>(<italic>I<sub>F</sub>, I</italic>
<sub>1</sub>) and <italic>H</italic>(<italic>I<sub>F</sub>, I</italic>
<sub>2</sub>) are not guaranteed to be on the same scale. The solution is the Normalized Mutual Information, devised in <xref ref-type="bibr" rid="bib0284">[284]</xref> and re-formulated in <xref ref-type="bibr" rid="bib0283">[283]</xref> for the fusion of two source images <italic>I</italic>
<sub>1</sub>, <italic>I</italic>
<sub>2</sub> as:<disp-formula id="eqn0075"><label>(75)</label><mml:math altimg="si110.svg" id="M117"><mml:mrow><mml:mi>N</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0224">The Spatial Frequency (SF) <xref ref-type="bibr" rid="bib0285">[285]</xref> is often used to measure the overall clarity of the fused images. It is obtained from the row frequency (<italic>F<sub>R</sub></italic>) and column frequency (<italic>F<sub>C</sub></italic>) of the image <italic>I</italic> as follows:<disp-formula id="eqn0076"><label>(76)</label><mml:math altimg="si111.svg" id="M118"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>
<disp-formula id="eqn0077"><label>(77)</label><mml:math altimg="si112.svg" id="M119"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula> and the SP is obtained by the harmonic mean of these two measures.<disp-formula id="eqn0078"><label>(78)</label><mml:math altimg="si113.svg" id="M120"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>
</p>
</sec>
<sec id="sec0054">
<label>6.2.2</label>
<title>Salient feature based metrics</title>
<p id="para0226">Salient feature metrics assess whether the salient features of the source images have passed to the fused image, following the hypothesis that the HVS is highly adapted for the perception of structural information. The most widely used measure in this category is the Structural Similarity (SSIM) index <xref ref-type="bibr" rid="bib0281">[281]</xref>, which is based on the degradation of structural information.</p>
<p id="para0227">The SSIM compares local patterns of pixel intensities, based on luminance <italic>l</italic>, contrast <italic>c</italic> and structure <italic>s</italic>:<disp-formula id="eqn0079"><label>(79)</label><mml:math altimg="si114.svg" id="M121"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>α</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>β</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mi>γ</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> weighted by some exponential variables that were set to <inline-formula><mml:math altimg="si115.svg" id="M122"><mml:mrow><mml:mi>α</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>β</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>γ</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in the original paper <xref ref-type="bibr" rid="bib0281">[281]</xref>. Several expressions are provided for <italic>s</italic>(<italic>X, Y</italic>), <italic>l</italic>(<italic>X, Y</italic>) and <italic>c</italic>(<italic>X, Y</italic>), with certain constraints, and after substituting on the previous equation, it becomes:<disp-formula id="eqn0080"><label>(80)</label><mml:math altimg="si116.svg" id="M123"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>C</italic>
<sub>1</sub> and <italic>C</italic>
<sub>2</sub> are two constraints of the model, expressed by <inline-formula><mml:math altimg="si117.svg" id="M124"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math altimg="si118.svg" id="M125"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <italic>K</italic>
<sub>1</sub> ≪ 1, <italic>K</italic>
<sub>2</sub> ≪ 1. The notation <italic>μ<sub>I</sub></italic> and <italic>σ<sub>I</sub></italic> corresponds to the mean intensity and unbiased standard deviation of the intensities of <italic>I</italic>, as in previous equations, and <inline-formula><mml:math altimg="si119.svg" id="M126"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is the covariance matrix of the intensities of <italic>I<sub>I</sub></italic> and <italic>I<sub>F</sub></italic>.</p>
<p id="para0230">Finally, Mittal et al. proposed the Natural Image Quality Evaluator (NIQE) <xref ref-type="bibr" rid="bib0286">[286]</xref>, a QA model based on the construction of a Multivariate Gaussian Model (MGM) from a corpus of undistorted images with mean <italic>μ<sub>M</sub></italic> and covariance matrix <italic>σ<sub>M</sub>, N</italic>(<italic>μ<sub>M</sub>, σ<sub>M</sub></italic>). These features are “quality aware”, and the quality of <italic>I<sub>F</sub></italic> is estimated as a distance between the statistics of the model and the fused image.<disp-formula id="eqn0081"><label>(81)</label><mml:math altimg="si120.svg" id="M127"><mml:mrow><mml:mi>N</mml:mi><mml:mi>I</mml:mi><mml:mi>Q</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula> with <inline-formula><mml:math altimg="si92.svg" id="M128"><mml:msub><mml:mi>μ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math altimg="si121.svg" id="M129"><mml:msub><mml:mi>σ</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> the mean vector and covariance matrix of the MGM of <italic>I<sub>F</sub></italic>.</p>
</sec>
</sec>
</sec>
<sec id="sec0055">
<label>7</label>
<title>Multimodal imaging data fusion: Benefits</title>
<p id="para0232">Multimodal neuroimaging and the fusion of multimodal data tackle the challenges of neuroimaging and the fundamental limitations of individual modalities, and therefore provide significant benefits to the overarching aim to achieve higher image quality and reveal brain physiology. This part will review some of the main benefits with specific fusion examples.</p>
<sec id="sec0056">
<label>7.1</label>
<title>Combination of physiological aspects of brain structures and processes</title>
<sec id="sec0057">
<label>7.1.1</label>
<title>MR-PET</title>
<p id="para0233">MR-PET refers to a functional metabolic and molecular multimodal imaging method integrated through a combination of MRI and PET. It has the potential to achieve the maximum complementary advantages with the examination function of both PET and MRI <xref ref-type="bibr" rid="bib0287">[287]</xref>.</p>
<p id="para0234">MRI can not only display structural details through multi-parameter sequences but also perform a variety of functional imaging. In other words, it can be seen as a means of anatomical imaging. However, compared with PET, MRI still has certain limitations in metabolite imaging. PET imaging can show trace amounts of radiolabeled molecules, but its image resolution is poor and the anatomical structure is not clear. It is the complementary characteristics of MRI and PET that led to the birth of MR-PET imaging, which not only has high soft-tissue contrast and resolution but also can provide valuable functional information <xref ref-type="bibr" rid="bib0288">[288]</xref>.</p>
<p id="para0235">At the radiological society of North America meeting in November 2006, Siemens presented the first MR-PET images of the brain from their diagnostic machine. During the imaging process of MR-PET, PET and MR imaging can be performed simultaneously with minimal interference; the PET scan detects the accumulation of fluorodeoxyglucose, while multiple sequences of MR images were obtained <xref ref-type="bibr" rid="bib0289">[289]</xref>.</p>
</sec>
<sec id="sec0058">
<label>7.1.2</label>
<title>Combination of different contrast images</title>
<p id="para0236">In neuroimaging studies of MRI, the common approach tends to acquire T1- and T2-weighted anatomical, T2*-weighted functional data within the same session and then make a combination of these different contrasts for further exploration and research. Here, T1 refers to the recovery time of longitudinal magnetization, and T2 refers to the decay time of transverse magnetization. Both of them are special values associated with the spin of the nucleus in the tissues.</p>
<p id="para0237">In humans, T1 and T2 values of diseased tissue and normal tissue are different, so diseases can be diagnosed by nuclear magnetic resonance imaging <xref ref-type="bibr" rid="bib0290">[290]</xref>.</p>
<p id="para0238">Magnetic resonance images are presented in different shades of gray, reflecting differences in the intensity of magnetic resonance signals, or in the length of relaxation times T1 and T2.</p>
<p id="para0239">Pure T2 dephasing is intrinsic to the sample. While T2* dephasing is relevant with true T2, field inhomogeneity (T2M) and tissue susceptibility (T2MS) as shown in <xref ref-type="disp-formula" rid="eqn0082">Eq. (82)</xref>.<disp-formula id="eqn0082"><label>(82)</label><mml:math altimg="si122.svg" id="M130"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mn>2</mml:mn><mml:mtext>MS</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0240">According to the common neuroimaging protocols, T1-weighted scans have good resolution and gray-white matter contrast, so it is better for observation of anatomical structure. T2-weighted scans perform well in showing histologic lesions, so it is often used for checking permanent brain injury. T2* is mostly used in scanning brain activity. An increase in T2* weighted signal between baseline and an active condition is associated with brain activation in studies using blood oxygenation-level dependent (BOLD) fMRI. For instance, as displayed in <xref ref-type="fig" rid="fig0021">Fig. 21</xref>
, brain regions become oxygen-rich after activity, which leads to a decrease of the Hbr/HbrO2 ratio and the increase of fMRI signal. Consequently, for T1, T2 and T2*, each method provides a physiologically and physically filtered view on one or more brain processes of interest <xref ref-type="bibr" rid="bib0291">[291]</xref>.<fig id="fig0021"><label>Fig. 21</label><caption><p>Comparison of Hbr/HbrO2 ratio in brain capillaries before and after activity.</p></caption><alt-text id="alt0021">Fig. 21</alt-text><graphic xlink:href="gr21_lrg"></graphic></fig></p>
<p id="para0241">Thus, combining different contrasts has the general merit of getting a more comprehensive physiological view on brain processes than utilizing just one imaging method alone.</p>
</sec>
</sec>
<sec id="sec0059">
<label>7.2</label>
<title>Improving temporal/spatial resolution</title>
<p id="para0242">In <xref ref-type="fig" rid="fig0004">Fig. 4</xref>, the spatial and temporal ranges for the most widely used non-invasive functional imaging methods are presented. Functional MRI, as the non-invasive functional imaging method, has the highest spatial resolution though the temporal resolution is relatively low compared to neuronal population dynamics. MEG and EEG can measure magnetic and electrical changes on millisecond time scales while the spatial resolution is beyond seven millimeters <xref ref-type="bibr" rid="bib0015">[15]</xref>. Ultra high-field MRI has been widely used for the mesoscopic level of neuroscience in humans <xref ref-type="bibr" rid="bib0292">[292]</xref>, <xref ref-type="bibr" rid="bib0293">[293]</xref>, <xref ref-type="bibr" rid="bib0294">[294]</xref>. As a consequence, the Spatio-temporal resolution can be improved by combining different imaging methods, especially through the combination of one modality that has a higher temporal resolution with another modality of superior spatial resolution. The fused images are called validation when Spatio-temporal resolutions are similar.</p>
<sec id="sec0060">
<label>7.2.1</label>
<title>Nominal resolution and effective resolution</title>
<p id="para0243">When considering improving spatio-temporal resolution, the resolution of each modality is not of the biggest concern. Each modality's effective resolution and the additional effect after the combination of data from different modalities should be considered instead. While the nominal resolution is generated according to fixed physical parameters of instruments, the effective resolution depends on the information of the data instead. In fMRI, field-of-view and the k-space acquisition matrix determine the nominal spatial resolution, while the effective resolution is largely affected by fMRI sequences and spread of the hemodynamic response <xref ref-type="bibr" rid="bib0295">[295]</xref>. Because of the slow development of hemodynamic response, the effective resolution is lower than nominal temporal resolution in fMRI, while the latter resolution is determined by the repetition timeTR.</p>
<p id="para0244">The nominal temporal resolution of electrophysiological methods is usually in the millisecond range. The effective resolution, however, can vary from tens of milliseconds to hundreds of milliseconds due to the slow evolution of neuronal field potentials and statistical detectability. Also, many repetitions have to be implemented before a detectable signal comes up for the evoked potentials in EEG, while a detectable hemodynamic response can be acquired through a single stimulus in fMRI.</p>
</sec>
<sec id="sec0061">
<label>7.2.2</label>
<title>EEG-fMRI</title>
<p id="para0245">EEG-fMRI, as the combination of EEG and fMRI, turns out to be the typical example of improving spatio-temporal resolution through multimodal neuroimaging. Epilepsy research facilitated the development of simultaneous EEG-fMRI recording <xref ref-type="bibr" rid="bib0296">[296]</xref>. In the case of localizing epileptic foci, simultaneously recorded fMRI help by connecting epileptic foci with interictal epileptiform discharges (IED) acquired by EEG. Simultaneous EEG-fMRI can also be utilized as neuronal oscillations in neuroscience <xref ref-type="bibr" rid="bib0209">[209,</xref>
<xref ref-type="bibr" rid="bib0297">297]</xref>. In these studies, the process of analyzing the recorded fMRI data takes power in the frequency domain of the EEG signal as a variable.</p>
</sec>
</sec>
<sec id="sec0062">
<label>7.3</label>
<title>Distortion correction</title>
<p id="para0246">An MRI machine uses a static, homogenous, external magnetic field B0, a radio-frequency magnetic field B1, and three orthogonal gradient fields. However, magnetic field variation may result from imperfections of the main magnetic field coil, the exciting gradient coils, and susceptibility of the tissues <xref ref-type="bibr" rid="bib0298">[298]</xref>. Field inhomogeneity is one of the causes of blurring or spatial shifts in MRI images. A typical region of distortion is the borderline between tissue and air, where a difference in magnetic susceptibilities exists. A standard process in neuroimage processing and fusion is the registration of multimodal images to standardized templates. Hellier and Barillot <xref ref-type="bibr" rid="bib0299">[299]</xref> introduced 3D non-rigid registration of multimodal images by mapping the deformation through cost optimization. The registration process provides the foundation for higher quality fusion with less distortion.</p>
<p id="para0247">The fusion of the main imaging sequence with supplementary sequences is also widely employed for distortion correction. As mentioned previously, a well-studied example is EPI, a high temporal resolution MRI technique that can collect multiple images per second. The high imaging frequency of EPI acquisition results in geometric distortions to its images. Acquisition of the static Bo fieldmap before or after EPI can provide additional information on local static field inhomogeneity and magnetic susceptibility. Holland, Kuperman <xref ref-type="bibr" rid="bib0300">[300]</xref> developed an efficient non-linear registration method by acquiring EPI of opposite phase encoding polarities, while Oh, Chung <xref ref-type="bibr" rid="bib0213">[213]</xref> applied PSF modeling to map the distortion field.</p>
<p id="para0248">Another example is the fusion of MRI and PET images in MR-PET systems. In these systems, apart from the combination of structural and metabolic information, the fusion of MRI and PET also allows enhanced distortion correction. In simultaneous MP-PET systems, motion distortions of PET can be corrected with the volumetric information provided by continuous EPI acquisitions or navigator sequences <xref ref-type="bibr" rid="bib0301">[301,</xref>
<xref ref-type="bibr" rid="bib0302">302]</xref>. Joint reconstruction of MRI and PET utilizes dependence between the two modalities for image reconstruction of one mode based on the other. Reconstructed images show sharper edges and less distortion <xref ref-type="bibr" rid="bib0303">[303]</xref>. This entire process can also be formulated as a single optimization problem <xref ref-type="bibr" rid="bib0301">[301]</xref>.</p>
</sec>
</sec>
<sec id="sec0063">
<label>8</label>
<title>Multimodal imaging data fusion: atlas-based segmentation</title>
<p id="para0249">Segmentation has been one of the main challenges faced by neuroimaging experts during the last decades. It refers to the process of tagging image pixels or voxels with biologically meaningful labels, such as anatomical structures and tissue types <xref ref-type="bibr" rid="bib0304">[304]</xref>. The segmentation task and data analysis are very often guided in most of the neuroimaging experiments by atlases that provide standardized, or stereotaxic, 3D coordinate systems for statistical data analysis and report of findings <xref ref-type="bibr" rid="bib0305">[305]</xref>. One of the earliest works in anatomical brain standardization was by Talairach, Tournoux <xref ref-type="bibr" rid="bib0306">[306]</xref>, who developed a 3D coordinate space for the brain to assist surgical techniques. This work was later updated by a full printed three-dimensional atlas of the human brain, which was especially beneficial for clinical studies, electroencephalographic investigations, and statistical computations <xref ref-type="bibr" rid="bib0307">[307]</xref>. Tzourio-Mazoyer, Landeau <xref ref-type="bibr" rid="bib0308">[308]</xref> developed an anatomical parcellation approach with the spatially normalized single-subject high-resolution T1 volume provided by the Montreal Neurological Institute (MNI) <xref ref-type="bibr" rid="bib0309">[309]</xref>. These developments have contributed to better define the relationships between brain structures and their functions in modern human neuroscience research, as well as to reduce the variability found between different subjects.</p>
<p id="para0250">The traditional approach for brain image segmentation is the manual annotation or delimitation of the regions of interest (ROIs) by a trained expert <xref ref-type="bibr" rid="bib0304">[304]</xref>. This process is subjective and strongly influenced by expert performance. It is then of limited applicability since it is time-consuming, prone to error and difficult to reproduce.</p>
<p id="para0251">Automated segmentation methods have been developed during the past decades. These methods can be classified into basic tissue classification and anatomical segmentation procedures <xref ref-type="bibr" rid="bib0310">[310]</xref>. Tissue classification methods segment a 3D image of the brain into different tissue types (Grey Matter, White Matter, CSF, etc.), while also correcting for spatial intensity variations (also known as bias field or RF inhomogeneities). These methods normally incorporate probabilistic tissue atlases to improve their performance and have been successfully automated for fMRI studies as well as integrated into several neuroimaging toolkits such as FSL FMRIB's Automated Segmentation Tool (FAST) <xref ref-type="bibr" rid="bib0311">[311]</xref> and Statistical Parametric Mapping (SPM) <xref ref-type="bibr" rid="bib0312">[312]</xref>. However, anatomical segmentation procedures are much more difficult to automate, since different anatomical structures that consist of different tissue types may exhibit similar signal properties. This observation is the reason why automated anatomical segmentation of the brain into non-homogeneous regions needs to be guided by atlases.</p>
<sec id="sec0064">
<label>8.1</label>
<title>Single-subject atlas-based segmentation</title>
<p id="para0252">For atlas-based segmentation, an atlas is defined as the combination of a brain volume (atlas template) and its corresponding coregistered segmented volume (atlas labels). The atlas-based segmentation method registers the atlas template to the target image, and then the atlas labels are propagated to the target image using an effective image warping method <xref ref-type="bibr" rid="bib0313">[313]</xref>.</p>
<p id="para0253">Let <italic>I</italic>(<italic>x</italic>) be the volume to be segmented, with <inline-formula><mml:math altimg="si123.svg" id="M131"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> representing a 3D voxel coordinate vector. For the purpose of single-subject atlas-based segmentation, a grey level atlas volume <italic>A</italic>(<italic>x</italic>), a labeled volume<disp-formula id="eqn0083"><label>(83)</label><mml:math altimg="si124.svg" id="M132"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mo>∈</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>L</italic> is the number of anatomical regions or labels defined in the atlas and, optionally, a probabilistic atlas <italic>P</italic>(x), all within the same spatial coordinates (the atlas space), are required. The segmentation process consists of a registration step and a label propagation step.</p>
<p id="para0255">In the registration step, the atlas volume <italic>A</italic>(<italic>x</italic>) is registered to the input image <italic>I</italic>(<italic>x</italic>) by means of a spatial transformation <italic>T</italic>(<italic>x</italic>), which optimizes a given cost function. In the label propagation step, labels defined in the tagged volume <italic>L</italic>(<italic>x</italic>) are assigned to each of the voxels of the input image <italic>I</italic>(<italic>x</italic>) by applying the normalization transformation <italic>T</italic>(<italic>x</italic>) to <italic>L</italic>(<italic>x</italic>), thus spatially aligning the atlas to the input image <italic>I</italic>(<italic>x</italic>). The atlas probability maps <italic>P</italic>(<italic>x</italic>) are normally used for MRI since they enable to improve the segmentation accuracy of T1-weighted MRI into grey matter, white matter and cerebrospinal fluid tissues with higher accuracy when the regional tissue densities need to be quantified and compared for between different groups of subjects <xref ref-type="bibr" rid="bib0314">[314]</xref>.</p>
<p id="para0256">Medical image registration is routinely used in neuroimaging. It aims at determining the spatial alignment between images of the same or different subjects by means of optimizing a cost function, which measures the similarity between the transformed input image and the reference image (template) <xref ref-type="bibr" rid="bib0315">[315]</xref>. The registration process involves a spatial transformation being global rigid and affine transformations usually enough for intra-subject image registration <xref ref-type="bibr" rid="bib0313">[313]</xref>. However, the atlas-based segmentation task requires intersubject matching or registration of an input image to an atlas image and, as a consequence of the variability of the anatomical structure across different subjects, non-rigid registration algorithms are widely used.</p>
</sec>
<sec id="sec0065">
<label>8.2</label>
<title>Multi-atlas segmentation</title>
<p id="para0257">Single-subject atlas-based segmentation approaches suffer from a reduced ability to capture the variability of the spatial distribution of anatomical structures across different subjects. Multi-atlas segmentation was introduced in some pioneering works <xref ref-type="bibr" rid="bib0310">[310,</xref>
<xref ref-type="bibr" rid="bib0316">316,</xref>
<xref ref-type="bibr" rid="bib0317">317]</xref> to address this problem and to offer superior segmentation accuracy.</p>
<p id="para0258">
<xref ref-type="fig" rid="fig0022">Fig. 22</xref>
shows a block diagram of a multi-atlas segmentation method. Instead of a model-based average atlas representation, a number of expert-annotated image volumes of different subjects are required. The input image is coregistered to each one of the different atlases available, and label propagation is performed. The final label is obtained for each voxel of the input image through a label fusion technique. In a multi-atlas segmentation approach, each atlas is used in the parcellation of the input image separately, since they are not summarized in a single probabilistic model.<fig id="fig0022"><label>Fig. 22</label><caption><p>Block diagram of multi-atlas segmentation based on image registration, label propagation, and label fusion.</p></caption><alt-text id="alt0022">Fig. 22</alt-text><graphic xlink:href="gr22_lrg"></graphic></fig></p>
<sec id="sec0066">
<label>8.2.1</label>
<title>Atlas propagation</title>
<p id="para0259">For the purposes of reviewing the existing methods for atlas propagation and atlas fusion in multi-atlas based segmentation, a similar notation to the adopted ones in <xref ref-type="bibr" rid="bib0318">[318,</xref>
<xref ref-type="bibr" rid="bib0319">319]</xref> will be used. These works have addressed the atlas-based segmentation problem as a classification task where the atlas is the training set, and the training is associated with the process of computing the registration between the image and the atlas.</p>
<p id="para0260">Let <italic>I</italic>(<italic>x</italic>) be a 3D image to be segmented into <italic>L</italic> different classes belonging to the atlas label set <inline-formula><mml:math altimg="si125.svg" id="M133"><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. Multi-atlas segmentation methods use <italic>K</italic> 3D atlas images <italic>A<sub>k</sub></italic>(<italic>x</italic>) and their corresponding atlas labels<disp-formula id="eqn0084"><label>(84)</label><mml:math altimg="si126.svg" id="M134"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mo>∈</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mi>k</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0261">The coordinate transformation <italic>T<sub>k</sub></italic> : R<sup>3</sup> → R<sup>3</sup> defines a mapping from the coordinates of the <italic>k</italic> atlas <italic>A<sub>k</sub></italic>(x) to the target image <italic>I</italic>(x). Once the registration transformation is obtained, the input image <italic>I</italic>(<italic>x</italic>) is automatically segmented for each one of the <italic>K</italic> atlases available by applying a label propagation technique, thus obtaining <italic>K</italic> different segmentations<disp-formula id="eqn0085"><label>(85)</label><mml:math altimg="si127.svg" id="M135"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mi>k</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0262">These candidate segmentations must be combined later into a final estimated segmentation <italic>I<sup>s</sup></italic>(x) ∈  Λ by applying a label fusion technique, as shown in <xref ref-type="fig" rid="fig0022">Fig. 22</xref>.</p>
<p id="para0263">Once each of the <italic>K</italic> atlas images has been coregistered to the input image, the labels are then propagated to the space of the input image in a process called label propagation. Label propagation applies the registration transformations to map each of the available atlas volumes <italic>L<sub>k</sub></italic>(<italic>x</italic>) to the input image, and an image warping technique preserves the discrete nature of the labels. Note that the transformations <italic>T<sub>k</sub></italic>: applied to the atlas label volumes <italic>L<sub>k</sub></italic>(x) are 3D continuous transformations that do not match grid points in the atlas space to grid points in the target input image. The purpose of label propagation is to perform the interpolation of the transformed atlas label volume for grid points in the target image space <xref ref-type="bibr" rid="bib0320">[320]</xref>.</p>
<p id="para0264">Among the different label propagation techniques, the most widely used are nearest-neighbor interpolation <xref ref-type="bibr" rid="bib0310">[310]</xref> and linear interpolation <xref ref-type="bibr" rid="bib0316">[316,</xref>
<xref ref-type="bibr" rid="bib0320">320]</xref>. Nearest-neighbor provides the unique label of the nearest atlas grid point. Partial volume interpolation, a technique that was first introduced in <xref ref-type="bibr" rid="bib0321">[321]</xref> for image registration, is a more sophisticated method for label propagation that takes into account the contribution of adjacent grip points to the final propagated label. During the last decade, several improvements have been proposed and reviewed for label propagation in multi-atlas segmentation <xref ref-type="bibr" rid="bib0304">[304]</xref>. Most of them incorporate additional information to improve the performance by augmenting the information with a tissue consistency map in nearest-neighbor interpolation <xref ref-type="bibr" rid="bib0322">[322]</xref> or introducing signed distance maps of the original atlas labels <xref ref-type="bibr" rid="bib0323">[323,</xref>
<xref ref-type="bibr" rid="bib0324">324]</xref>.</p>
</sec>
<sec id="sec0067">
<label>8.2.2</label>
<title>Atlas fusion</title>
<p id="para0265">The final step in atlas-based segmentation is atlas fusion. After registration and label propagation for each of the <italic>K</italic> available atlases, a final unique segmentation <italic>I<sup>s</sup></italic>(x) ∈  Λ is obtained by merging the information provided by each of the individual atlas-based segmentations into a single segmented image. This approach has been shown to be more accurate than an individual atlas segmentation <xref ref-type="bibr" rid="bib0316">[316]</xref> in the same manner as a combination of classifiers is generally more accurate than a single classifier in many pattern recognition scenarios <xref ref-type="bibr" rid="bib0325">[325]</xref>, <xref ref-type="bibr" rid="bib0326">[326]</xref>, <xref ref-type="bibr" rid="bib0327">[327]</xref>, <xref ref-type="bibr" rid="bib0328">[328]</xref>, <xref ref-type="bibr" rid="bib0329">[329]</xref>, <xref ref-type="bibr" rid="bib0330">[330]</xref>.</p>
<p id="para0266">The conventional method for combining individual segmentations is an equally weighted majority voting framework <xref ref-type="bibr" rid="bib0319">[319]</xref>. A more sophisticated approach estimates the performances of the individual atlas segmentations and combines them by weighting them according to their estimated performance. Both approaches are described and discussed in this subsection.</p>
<p id="para0267">
<italic>Majority voting atlas fusion.</italic>
</p>
<p id="para0268">The combined multi-atlas segmentation output <italic>I<sup>s</sup></italic>(x) ∈  Λ for a voxel sample <italic>x</italic> given the set of <italic>K</italic> single atlas segmentations <inline-formula><mml:math altimg="si128.svg" id="M136"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math altimg="si129.svg" id="M137"><mml:mrow><mml:mi>k</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>, is obtained through the vote rule decision function<disp-formula id="eqn0086"><label>(86)</label><mml:math altimg="si130.svg" id="M138"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where the <italic>Q</italic> function is defined to be<disp-formula id="eqn0087"><label>(87)</label><mml:math altimg="si131.svg" id="M139"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0270">Atlas label fusion based on majority voting considers all the segmentations equally accurate, and no prior knowledge of segmentation performance is required.</p>
<p id="para0271">
<italic>Performance weighting atlas fusion.</italic>
</p>
<p id="para0272">Majority voting equally weighs the individual atlas segmentation without using a segmentation performance model. In order to improve the performance of the multi-atlas segmentation, the combined segmentation output <italic>I<sup>s</sup></italic>(x) ∈  Λ should be the class maximizing the probability, given all the individual segmentations <inline-formula><mml:math altimg="si128.svg" id="M140"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mspace width="0.33em"></mml:mspace><mml:mstyle mathvariant="normal"><mml:mi>Λ</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math altimg="si129.svg" id="M141"><mml:mrow><mml:mi>k</mml:mi><mml:mspace width="0.33em"></mml:mspace><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.33em"></mml:mspace><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> and some available segmentation performance model <bold>P</bold>
<xref ref-type="bibr" rid="bib0319">[319]</xref>:<disp-formula id="eqn0088"><label>(88)</label><mml:math altimg="si132.svg" id="M142"><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mtext>argmax</mml:mtext><mml:mi>i</mml:mi></mml:munder><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>K</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
<p id="para0273">Using the Bayes rule, and assuming independence of the individual classifiers, simplifies the problem and enables us to find an optimal segmentation based on a model of segmentation performance. Rohlfing, Russakoff <xref ref-type="bibr" rid="bib0319">[319]</xref> proposed different classifier performance models and used an expectation-maximization (EM) algorithm that simultaneously estimated the performance parameters of the segmentations and provided an estimation of the unknown ground truth. This novel strategy enabled us to learn classifier performance parameters and to adopt weighted atlas combination for atlas label fusion in multi-atlas segmentation following a novel supervised training stage <xref ref-type="bibr" rid="bib0331">[331,</xref>
<xref ref-type="bibr" rid="bib0332">332]</xref>.</p>
</sec>
</sec>
</sec>
<sec id="sec0068">
<label>9</label>
<title>Multimodal imaging data fusion: quantification</title>
<p id="para0274">The application and improvement of the quantification in medical image allows to increase characteristics such as sensitivity and specificity, among others, obtaining more accurate patient's diagnoses. To perform this analysis, the emission computed tomography (ECT) is used, since it is the most important medical imaging modality in nuclear medicine. More specifically, the two image modalities analyzed are positron emission tomography (PET) and single-photon emission computed tomography (SPECT), which differ in the radiotracer used and the nature of the emission measurement. It should be noted that the factors associated with quantification that affect PET and SPECT are also extrapolated to multimodal images. Furthermore, multimodal images can be of great interest to improve quantification. Both, PET and SPECT, have proven to be effective imaging techniques for the diagnosis and monitoring of treatments in different medical applications, offering information about biological processes, which is very important for the study of brain activity <xref ref-type="bibr" rid="bib0333">[333]</xref>. The level of intensity in these types of images is bound to physical parameters, such as cerebral blood flow, glucose or receptor binding among others <xref ref-type="bibr" rid="bib0334">[334,</xref>
<xref ref-type="bibr" rid="bib0335">335]</xref>.</p>
<p id="para0275">Since these modalities were conceived in the 50’s and 60’s Jones and Townsend <xref ref-type="bibr" rid="bib0336">[336]</xref>, several improvements have been developed in terms of quantification. One of the most relevant improvements in this field is due to the development of dual-modality imaging in the 90′s <xref ref-type="bibr" rid="bib0337">[337]</xref>. This type of imaging allows the combination of complementing functional information (PET and SPECT) with structural information (computed tomography (CT) and magnetic resonance imaging (MRI)), leading to increases in sensitivity and specificity. The combination of different characteristics that can be observed from each type of image allows us to obtain a better understanding of the structure and function of the human body. For example, PET/MRI combination offers remarkable advantages compared to the use of PET/CT since CT radiation dose is avoided and soft tissue images from MR could be acquired at the same time as the PET ones <xref ref-type="bibr" rid="bib0338">[338]</xref>.</p>
<p id="para0276">PET offers higher resolution and better quality than SPECT, particularly in PET/CT. Furthermore, it allows for easier quantitative measurements <xref ref-type="bibr" rid="bib0335">[335]</xref>. For example, sensitivity could be up to two orders of magnitude greater with comparable axial fields of view <xref ref-type="bibr" rid="bib0339">[339]</xref>. Nevertheless, the spatial resolution that can be obtained in both techniques tends to be low, due to its dependence on the radiation dose administrated to the patients. Also, ECT images are usually degraded by several factors, such as photon attenuation, partial volume effect or scatter radiation <xref ref-type="bibr" rid="bib0337">[337]</xref>. Moreover, spatial resolutions associated with structural images continue to be higher (≈ 1 mm) than in functional images (≈ 4–6 mm) <xref ref-type="bibr" rid="bib0340">[340]</xref>.</p>
<p id="para0277">Regarding quantification, an increasing number of relevant research aims to propose new quantification techniques or improve previously existing ones is observed in recent years. The possibility of quantification in nuclear medicine imaging is one of its successes. Due to the increasing use of nuclear images in therapies, the way of measuring functional images is changing. While historically, the use of relative and semiquantitative measures was typical, the recent application of absolute quantification is gaining support. One of the first steps was considered activity concentration and normalized uptake using the standard uptake value (SUV) <xref ref-type="bibr" rid="bib0341">[341]</xref>. Nevertheless, to reach quantitative imaging as a useful and potential tool, several factors must be addressed. As Zaidi and Hasegawa <xref ref-type="bibr" rid="bib0339">[339]</xref> mentioned among these factors are the system sensitivity and spatial resolution, dead-time and pulse pile-up effects, the linear and angular sampling intervals of the projections, the size of the object, photon attenuation, scatter, partial volume effects, patient motion, kinetic effects, and filling of excretory routes (e.g., the bladder) by the radiopharmaceutical.</p>
<p id="para0278">In neuroimaging, PET and SPECT are a popular option to detect biomarkers associated with degenerative diseases. For Alzheimer's disease (AD), the accumulation of Amyloid-<italic>β</italic> plaques and tau aggregates can be detected <xref ref-type="bibr" rid="bib0342">[342,</xref>
<xref ref-type="bibr" rid="bib0343">343]</xref>, while an increased diffusivity in the striatum and thalamus can be observed in Parkinson's disease (PD) patients. A well-defined quantification of these biomarkers would allow more precise and reliable diagnoses.</p>
<p id="para0279">This section is organized as follows. Firstly, quantification both in PET and SPECT is analyzed considering the associated effects and their correction, especially those related to photon attenuation, scatter, and partial volume effects. Then, novel developments focused on cerebral medical imaging are highlighted.</p>
<sec id="sec0069">
<label>9.1</label>
<title>PET and SPECT quantification</title>
<p id="para0280">Even though PET scans could be evaluated qualitatively through the visual examination of the tracer uptake in cortical regions by a trained radiologist <xref ref-type="bibr" rid="bib0344">[344]</xref>, the best way to analyze these images is quantitative. For that, automated or semi-automated localization methods can be used to evaluate regional levels of tracer uptake <xref ref-type="bibr" rid="bib0345">[345,</xref>
<xref ref-type="bibr" rid="bib0346">346]</xref>. Within automated studies, an essential step is to use spatial normalization to register subjects' brain to a standardized template space, so that all subjects in the study can be compared <xref ref-type="bibr" rid="bib0347">[347]</xref>. Another procedure of great importance is intensity normalization, which is the object of study in this analysis.</p>
<p id="para0281">As Foster, Bagci <xref ref-type="bibr" rid="bib0348">[348]</xref> mentioned, several semi-quantitative and quantitative parameters that can be considered for intensity normalization exist, such as standardized uptake value (SUV), fractional uptake rate (FUR), tumor-to-background ratio (TBR), nonlinear regression techniques, total lesion evaluation (TLE) or the Patlak-derived methods. Nevertheless, the most widely used is SUV, which can also be used in SPECT <xref ref-type="bibr" rid="bib0348">[348]</xref>. The use of body weight (BW) in SUV has been discussed in the literature, and the general advice is to use more reliable measures such as body surface area (BSA) or lean body mass (LBM) <xref ref-type="bibr" rid="bib0349">[349,</xref>
<xref ref-type="bibr" rid="bib0350">350]</xref>. Moreover, a decay factor that depends on the particular radiotracer used may also be considered <xref ref-type="bibr" rid="bib0351">[351]</xref>.</p>
<p id="para0282">Several physiological and physical factors can influence the standardized uptake value obtained <xref ref-type="bibr" rid="bib0348">[348,</xref>
<xref ref-type="bibr" rid="bib0352">352]</xref>. Regarding physiological factors, some of them are weight and fat of the subject or blood glucose concentration, etc. Physical factors include partial volume effect (PVE), image manipulation (reconstruction, smoothing), and artifacts related to involuntary movements of the patient. The relevance of these factors lies in the variability of SUV. Studies carried out in the literature suggest that these factors may alter SUV in the range of 10%–30% <xref ref-type="bibr" rid="bib0353">[353,</xref>
<xref ref-type="bibr" rid="bib0354">354]</xref>. Nevertheless, the importance of correcting these effects depends on the clinical trial performed, since it can be a complex task and is not always worthwhile <xref ref-type="bibr" rid="bib0355">[355]</xref>.</p>
<p id="para0283">Moreover, these factors affect not only semi-quantitative measurements but also affect absolute quantification. There are several methods to consider within the absolute quantification, such as image reconstruction, effects correction or calibration to obtain the measured activity distribution <xref ref-type="bibr" rid="bib0356">[356,</xref>
<xref ref-type="bibr" rid="bib0357">357]</xref>.</p>
<p id="para0284">Among the mentioned factors associated with imaging quantification, we analyzed three effects and their respective corrections, since they are popular bias sources for PET and SPECT quantification. These effects are PVE, scatter, and photon attenuation effects.</p>
<sec id="sec0070">
<label>9.1.1</label>
<title>Partial volume effect</title>
<p id="para0285">Regarding the quantitative accuracy of PET image, partial volume effect (PVE) is the most relevant effect <xref ref-type="bibr" rid="bib0358">[358]</xref>. PVE is related to the finite spatial resolution of PET scanners and its discrete nature. That discrete nature causes a voxel to could be composed of more than one tissue, which leads to a final signal that is an averaged mix of signals, which is also called tissue fraction effect. Moreover, the larger the voxel size, the more tissues can coexist in it. This effect, along with the point-spread effect (or spillover), is the cause of image blur <xref ref-type="bibr" rid="bib0354">[354,</xref>
<xref ref-type="bibr" rid="bib0359">359]</xref>. Also, it is an effect that often causes confusion among clinicians and researchers when analyzing the image, since it is necessary to distinguish between loss of radioactivity due to PVE and the true loss of tissue.</p>
<p id="para0286">Strategies to address this effect are called partial volume correction (PVC) methods. Distribution of both signal and noise are the main factors to consider for the selection of algorithms for PVC to apply <xref ref-type="bibr" rid="bib0359">[359]</xref>. Several PVC techniques have been proposed in the literature for improving image quality and quantitative accuracy in PET <xref ref-type="bibr" rid="bib0360">[360]</xref>, <xref ref-type="bibr" rid="bib0361">[361]</xref>, <xref ref-type="bibr" rid="bib0362">[362]</xref>, <xref ref-type="bibr" rid="bib0363">[363]</xref>. Historically, these techniques have been classified in several forms. In this work, these methods are grouped as region-based (RB) and voxel-based (VB) techniques, based the most commonly used classifications <xref ref-type="bibr" rid="bib0340">[340,</xref>
<xref ref-type="bibr" rid="bib0355">355]</xref>. The most relevant differences between them are that VB methods produce PVE-recovered images while RB methods do not, and RB methods are associated with regions of interest (ROIs) while VB methods are applied at a voxel level considering the recovery of the spatial resolution of the system. Usually, RB techniques tend to be used more frequently than VB ones since they include regional homogeneity assumptions, simplifying the problem <xref ref-type="bibr" rid="bib0359">[359]</xref>.</p>
<p id="para0287">
<xref ref-type="table" rid="tbl0005">Table 5</xref>
shows a summary of the different existing techniques. Among the several PVC methods, the one associated with recovery coefficients (RC) is the simplest and most popular in clinical trials. Nevertheless, the most common technique used in neuroimaging is PVE correction based on anatomical images, which was developed for neuroimaging. The first technique proposed by Videen, Perlmutter <xref ref-type="bibr" rid="bib0364">[364]</xref> in 2D and later extended to 3D by Meltzer, Leal <xref ref-type="bibr" rid="bib0365">[365]</xref> was a segmentation in anatomy into two classes: brain and non-brain. Mullergartner, Links <xref ref-type="bibr" rid="bib0366">[366]</xref> proposed an improved version of this technique (known as GM algorithm) to use three instead of two classes: grey matter (GM), white matter (WM) and cerebrospinal fluid (CSF). Meltzer, Zubieta <xref ref-type="bibr" rid="bib0367">[367]</xref> added a fourth class to the previous algorithms in order to compensate for the real heterogeneity in GM tissue. The importance of this method is that PVE can be corrected between high and low intensity brain structures. Regarding the iterative reconstruction algorithms associated to VB methods, as V. Bettinardi, Castiglioni <xref ref-type="bibr" rid="bib0340">[340]</xref> noted, some of the best known and used is the Van Cittert (VC) algorithm, the re-blurred Van Cittert (R-VC) and the Richardson–Lucy (RL) algorithm. For broader coverage, readers are referred to reviews related to PVE <xref ref-type="bibr" rid="bib0340">[340]</xref> and iterative reconstruction methods <xref ref-type="bibr" rid="bib0368">[368]</xref>.<table-wrap id="tbl0005" position="float"><label>Table 5</label><caption><p>Classification of PVC methods (RB = region-based; VB = voxel-based).</p></caption><alt-text id="alt0029">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Category</th><th valign="top">Method</th><th valign="top">Definition</th><th valign="top">Benefits</th><th valign="top">Limitations</th><th valign="top">Examples</th></tr></thead><tbody><tr><td valign="top">RB</td><td valign="top">Recovery coefficients (RC)</td><td valign="top">They are numerical calculated in the image domain as a ratio of measured radioactivity concentration and actual concentration using spheres filled with a known radioactivity concentration</td><td valign="top">Fairly simple Practical Popular within clinical trials</td><td valign="top">More related to oncology</td><td valign="top"><xref ref-type="bibr" rid="bib0369">[369,</xref><xref ref-type="bibr" rid="bib0370">370]</xref></td></tr><tr><td valign="top"></td><td valign="top">PET raw data</td><td valign="top">Instead of using the image domain, the sinogram is used</td><td valign="top">Low computational cost</td><td valign="top">Some methods require segmentation</td><td valign="top"><xref ref-type="bibr" rid="bib0371">[371]</xref>, <xref ref-type="bibr" rid="bib0372">[372]</xref>, <xref ref-type="bibr" rid="bib0373">[373]</xref></td></tr><tr><td valign="top"></td><td valign="top">Geometric transfer matrix (GTM) method</td><td valign="top">Average uptake is estimated considering multiple ROIs using co-registered anatomical images</td><td valign="top">Commonly used Good accuracy</td><td valign="top">Requires segmentation Bias</td><td valign="top"><xref ref-type="bibr" rid="bib0374">[374]</xref>, <xref ref-type="bibr" rid="bib0375">[375]</xref>, <xref ref-type="bibr" rid="bib0376">[376]</xref>, <xref ref-type="bibr" rid="bib0377">[377]</xref></td></tr><tr><td valign="top">VB</td><td valign="top">Image reconstruction</td><td valign="top">Spatial resolution is recovered within the image reconstruction process. It is highly recommended to use iterative reconstruction algorithms. The most relevant drawback of this is the high number of iterations needed</td><td valign="top">Increase the quality of reconstruction</td><td valign="top">Computational cost</td><td valign="top"><xref ref-type="bibr" rid="bib0378">[378]</xref>, <xref ref-type="bibr" rid="bib0379">[379]</xref>, <xref ref-type="bibr" rid="bib0380">[380]</xref>, <xref ref-type="bibr" rid="bib0381">[381]</xref></td></tr><tr><td valign="top"></td><td valign="top">Image deconvolution</td><td valign="top">Spatial resolution is recovered by using a post-reconstruction restoration technique (deconvolution) that applies the point spread function (PSF). Iterative algorithms are commonly used</td><td valign="top">Compensation of spill-over effects</td><td valign="top">Computational cost</td><td valign="top"><xref ref-type="bibr" rid="bib0382">[382]</xref></td></tr><tr><td valign="top"></td><td valign="top">Multi-resolution approach</td><td valign="top">Spatial resolution is recovered by using information from spatially coregistered high-resolution anatomical images, which allows transfer high spatial frequencies from anatomical to functional images</td><td valign="top">Adjust of radioactivity concentration Used in brain imaging</td><td valign="top">In poorly correlated areas the amount of artifacts increases</td><td valign="top"><xref ref-type="bibr" rid="bib0383">[383]</xref></td></tr><tr><td valign="top"></td><td valign="top">Anatomical images</td><td valign="top">First, a segmentation of a coregistered anatomical image is done. Then, PVE correction is applied to the set of voxels associated with each region/class (tissue)</td><td valign="top">Highly used in brain imaging</td><td valign="top">Need segmentation</td><td valign="top">Two classes: <xref ref-type="bibr" rid="bib0364">[364,</xref><xref ref-type="bibr" rid="bib0365">365]</xref> Three classes: <xref ref-type="bibr" rid="bib0366">[366]</xref> Four classes: <xref ref-type="bibr" rid="bib0367">[367]</xref></td></tr></tbody></table></table-wrap></p>
<p id="para0288">Finally, data extracted from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) is used in order to show some PVE correction methods. This data consists of a patient diagnosed with Alzheimer's disease from which FDG-PET and T1 weighted MR images were taken, as shown in <xref ref-type="fig" rid="fig0023">Fig. 23</xref>
. Both modalities are registered to respective templates of T1 and PET, and PET image was not normalized in intensity due to use only one subject in this manuscript. PVC techniques are applied by using PVElab <xref ref-type="bibr" rid="bib0384">[384,</xref>
<xref ref-type="bibr" rid="bib0385">385]</xref>, a platform developed by the EU sponsored PVEOut project. PVElab can facilitate PVE correction through a graphical interface with several steps, including registration, segmentation, reslicing, optional application of an atlas (none in our case), and PVE correction. Three methods discussed above are those analyzed: 3D Meltzer technique <xref ref-type="bibr" rid="bib0365">[365]</xref> and GM algorithm <xref ref-type="bibr" rid="bib0366">[366]</xref> regarding VB methods, and the RB method proposed by Rousset, Ma <xref ref-type="bibr" rid="bib0374">[374]</xref>.<fig id="fig0023"><label>Fig. 23</label><caption><p>Original PET (left) and MR (right) images.</p></caption><alt-text id="alt0023">Fig. 23</alt-text><graphic xlink:href="gr23_lrg"></graphic></fig></p>
<p id="para0289">The large difference between methods is shown in <xref ref-type="fig" rid="fig0024">Fig. 24</xref>
a. The difference in the result between considering two (<xref ref-type="fig" rid="fig0024">Fig. 24</xref>a) and three tissues (<xref ref-type="fig" rid="fig0024">Fig. 24</xref>b) is highlighted. The method related to GTM (<xref ref-type="fig" rid="fig0024">Fig. 24</xref>c) reflects intermediate results between the two images already commented.<fig id="fig0024"><label>Fig. 24</label><caption><p>Evolution of PVC techniques.</p></caption><alt-text id="alt0024">Fig. 24</alt-text><graphic xlink:href="gr24_lrg"></graphic></fig></p>
</sec>
<sec id="sec0071">
<label>9.1.2</label>
<title>Attenuation effect</title>
<p id="para0290">Another relevant effect to analyze is photon attenuation. This phenomenon occurs due to the interaction between the photon associated to the radiotracer and elements in the body such as tissue. Normally, this interaction leads to a scatter in the photon radiation. The probability that a photon experiences an interaction is represented by the linear attenuation coefficient <xref ref-type="bibr" rid="bib0386">[386]</xref>. The situation of this phenomenon differs between PET and SPECT imaging. In PET, two antiparallel photons are detected in the collimator, so the total tissue thickness crossed by them is the same body thickness that intersected the straight line between the two detections (line of response). Nevertheless, in SPECT imaging, the attenuation depends on the total tissue thickness crossed and its type (e.g., soft tissue, bone), which varies depending on where the points of emission and detection are <xref ref-type="bibr" rid="bib0386">[386]</xref>. Traditionally, this correction was much more commonly performed in PET imaging than in SPECT. Nowadays, it is done in both to be able to achieve much more accurate quantitative results, although it is more difficult to apply in SPECT.</p>
<p id="para0291">Attenuation correction is necessary in order to obtain accurate quantitative results and is widely implemented. For this, an attenuation map is needed, which Zaidi and Hasegawa <xref ref-type="bibr" rid="bib0386">[386]</xref> defined as a representation of the spatial distribution of linear attenuation coefficients and delineates the body structures located in the image. Once the attenuation map is done, the reconstruction algorithm can be implemented with more information. Attenuation correction techniques can be classified into two major groups: transmission-less approaches and the ones based on transmission scanning. While the first group considers the measured emission data to develop the attenuation map or assumes a uniform distribution for the coefficients, the second one uses transmission data of external sources such as CT and MRI, i.e., the use of anatomical data. The second group offers a more accurate solution. Nevertheless, there are situations in which the correction made by the first group is sufficient, without the need to complicate the applied technique. A brief introduction to the various existing methods for performing the attenuation map is shown in <xref ref-type="table" rid="tbl0006">Table 6</xref>
. For a more extensive reading of this topic, readers are referred to reviews about attenuation correction <xref ref-type="bibr" rid="bib0386">[386]</xref>, <xref ref-type="bibr" rid="bib0387">[387]</xref>, <xref ref-type="bibr" rid="bib0388">[388]</xref>.<table-wrap id="tbl0006" position="float"><label>Table 6</label><caption><p>Classification of attenuation correction methods.</p></caption><alt-text id="alt0030">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Category</th><th valign="top">Method</th><th valign="top">Definition</th><th valign="top">Benefit</th><th valign="top">Limitations</th><th valign="top">Examples</th></tr></thead><tbody><tr><td valign="top">Transmission-less based</td><td valign="top">Uniform fit-ellipse method (UFEM)</td><td valign="top">Object outline is approximated as an ellipse around its edges. In order to generate the attenuation map, uniform attenuation is assigned Then, uniform attenuation is assigned inside the figure.</td><td valign="top">Quick and easy Functional for brain studies</td><td valign="top">Low precision Limited to homogeneous areas</td><td valign="top"><xref ref-type="bibr" rid="bib0389">[389]</xref></td></tr><tr><td valign="top"></td><td valign="top">Automated contour detection method (ACDM)</td><td valign="top">Edge-detection algorithms are used to generate the shape of the object. This allows convex shapes</td><td valign="top">It is independent of the specialist Functional for brain studies</td><td valign="top">Low precision but higher than UFEM Only for homogeneous areas</td><td valign="top"><xref ref-type="bibr" rid="bib0390">[390,</xref><xref ref-type="bibr" rid="bib0391">391]</xref></td></tr><tr><td valign="top"></td><td valign="top">Other methods</td><td valign="top">Several techniques fit here, such as algebraic reconstruction–based techniques (MLAA or MLACF) or machine-learning techniques</td><td valign="top">No anatomical information is needed</td><td valign="top">Possibility of cross-talk between emission data and attenuation map Virtually not used in clinical trials</td><td valign="top"><xref ref-type="bibr" rid="bib0392">[392]</xref>, <xref ref-type="bibr" rid="bib0393">[393]</xref>, <xref ref-type="bibr" rid="bib0394">[394]</xref>, <xref ref-type="bibr" rid="bib0395">[395]</xref>, <xref ref-type="bibr" rid="bib0396">[396]</xref></td></tr><tr><td valign="top">Transmission-based</td><td valign="top">Radionuclide transmission</td><td valign="top">Apply an external source (PET, SPECT or SPECT/PET) interleaving transmission and emission scanning</td><td valign="top">Available in most systems. Existence of complementary methods to reduce noise</td><td valign="top">Need to modify the obtained attenuation coefficients due to be energy-dependent Possibility of errors due to cross-talk of data</td><td valign="top"><xref ref-type="bibr" rid="bib0397">[397]</xref>, <xref ref-type="bibr" rid="bib0398">[398]</xref>, <xref ref-type="bibr" rid="bib0399">[399]</xref>, <xref ref-type="bibr" rid="bib0400">[400]</xref></td></tr><tr><td valign="top"></td><td valign="top">CT transmission</td><td valign="top">It can be addressed by segmenting the CT regions and assigning linear attenuation coefficients to each tissue or transforming the CT image to the attenuation map associated with the radiotracer photon's energy (or a combination of both methods using different scale factors for bone and tissues)</td><td valign="top">Quick Low noise Good spatial resolution Functional for brain studies</td><td valign="top">Misregistration due to respiratory movements Erroneus uptake due to patient's possession of unnatural materials (e.g. prostheses) CT photonic energy usually differs from that of the radionuclide used for emission scanning</td><td valign="top"><xref ref-type="bibr" rid="bib0401">[401]</xref>, <xref ref-type="bibr" rid="bib0402">[402]</xref>, <xref ref-type="bibr" rid="bib0403">[403]</xref>, <xref ref-type="bibr" rid="bib0404">[404]</xref>, <xref ref-type="bibr" rid="bib0405">[405]</xref></td></tr><tr><td valign="top"></td><td valign="top">MRI transmission</td><td valign="top">Segmentation-based techniques, where first the PET and MRI images are co-registered and then a segmentation technique is applied. Usually fuzzy is used to divide the image from two to five tissues, assigning to each tissue some attenuation coefficients Atlas-based techniques, where an MR template is used instead of multistep segmentation procedures</td><td valign="top">High precision Popular for brain studies</td><td valign="top">Total dependence on co-registration success in the patient's image</td><td valign="top"><xref ref-type="bibr" rid="bib0370">[370,</xref><xref ref-type="bibr" rid="bib0405">[405]</xref>, <xref ref-type="bibr" rid="bib0406">[406]</xref>, <xref ref-type="bibr" rid="bib0407">[407]</xref>, <xref ref-type="bibr" rid="bib0408">[408]</xref>, <xref ref-type="bibr" rid="bib0409">[409]</xref></td></tr></tbody></table></table-wrap></p>
<p id="para0292">Transmission-less based methods are easily applied in brain imaging because the brain is considered a practically homogeneous region composed mainly of soft-tissue. Moreover, these methods allow results regarding the skull <xref ref-type="bibr" rid="bib0390">[390]</xref> and optical tracking systems to be used to obtain head contours <xref ref-type="bibr" rid="bib0391">[391]</xref>. Despite being a typology that has not been used frequently, research on these methods has increased over the last years due to shift away from anatomical information. One of the first methods was proposed by Nuyts, Dupont <xref ref-type="bibr" rid="bib0392">[392]</xref>, which consists of a maximum likelihood approach known as MLAA (maximum likelihood reconstruction of attenuation and activity). It is commonly used in non-time-of-flight (TOF) PET images, and several studies have tried to improve or test it <xref ref-type="bibr" rid="bib0393">[393,</xref>
<xref ref-type="bibr" rid="bib0395">395,</xref>
<xref ref-type="bibr" rid="bib0396">396]</xref>. Another popular algorithm is MLACF (maximum likelihood attenuation correction factors) <xref ref-type="bibr" rid="bib0394">[394]</xref>, mostly used in TOF PET, which jointly estimates the attenuation sinogram and the activity image. Less popular than the previous one, this method has also been tested in other studies <xref ref-type="bibr" rid="bib0410">[410]</xref>. Moreover, machine learning techniques for attenuation correction are emerging, such as deep convolutional encoder-decoders <xref ref-type="bibr" rid="bib0334">[334]</xref> or deep convolutional neural networks <xref ref-type="bibr" rid="bib0411">[411]</xref> have also been applied in attenuation correction. For this last type of technique, there is considerable interest in this field of research, and new algorithms are constantly presented.</p>
<p id="para0293">Regarding transmission-based techniques, the added difficulty for MRI systems to obtain attenuation maps compared to CT systems should be highlighted. The reason for this is the connection between the attenuation coefficients and the electron density of the tissue. In CT, the data is associated with electron density and photon attenuation properties of the tissues. In contrast, MRI data is correlated to proton density and magnetic relaxation properties of the tissues <xref ref-type="bibr" rid="bib0387">[387]</xref>. As a challenge related to PET/MRI (or SPECT/MRI) systems, several methods have been proposed in literature regarding attenuation estimation. The MRI-based methods can be classified into two groups: segmentation-based and atlas-based methods.</p>
<p id="para0294">Of the two groups, the segmentation-based one tends to be used more on the literature. One of the first methods published in this area was that of Le Goff-Rougetet, Frouin <xref ref-type="bibr" rid="bib0412">[412]</xref>. This method aims to reduce the patient dose associated with PET imaging without affecting the accurate quantification. This technique is based on a surface matching technique for coregistration of PET and MR images. Another relevant proposal was based on registered T1-weighted MRI. Zaidi, Montandon <xref ref-type="bibr" rid="bib0407">[407]</xref> used a supervised fuzzy C-means clustering segmentation technique, which depends on the density and composition of five tissues: air, skull, brain tissue, and nasal sinuses. Similar to the latter case, Wagenknecht, Kops <xref ref-type="bibr" rid="bib0408">[408]</xref> presented a method where tissue classification is done with neural networks. First, a voxel classification with five tissues (GM, WM, CFS, adipose tissue (AT) and background (BG)) is made, then a second classification is done depending on the previous classes to detect extracerebral tissue and finally, segmentation is obtained. The main drawback of this method is the possibility of mis-segmentation or over-segmentation of bones, especially in the presence of abnormal anatomy or pathology.</p>
<p id="para0295">In order to improve bone detection, ultrashort echo time (UTE) MRI began to be investigated. Firstly, dual-echo ultra-short echo time and three tissue classification (bone, soft tissue and air) were used <xref ref-type="bibr" rid="bib0370">[370,</xref>
<xref ref-type="bibr" rid="bib0413">413]</xref>. In parallel, methods related to the Dixon technique were raised. Dixon-Water–Fat-segmentation (DWFS) allows the separation of soft and adipose tissues. The method was applied DWFS for the whole body in <xref ref-type="bibr" rid="bib0414">[414]</xref>. Considering both techniques (UTE and DWFS), Berker, Franke <xref ref-type="bibr" rid="bib0415">[415]</xref> proposed a method trying to reduce the drawbacks of these two techniques: time-consuming and complex image registration. This new technique applies UTE sampling for bone detection and uses gradient echoes for water-fat separation, obtaining a four-class PET attenuation map. An example of DWFS application in neuroimaging is the study conducted by Andersen, Ladefoged <xref ref-type="bibr" rid="bib0416">[416]</xref>, where this technique is used to generate the attenuation maps. Taking into account the time-consuming problem with UTE, short echo-time (STE) has been tested as a method to obtain attenuation maps of three (cortical bone, air and soft tissue) <xref ref-type="bibr" rid="bib0417">[417]</xref> and four (cortical bone, air, soft tissue and fat tissue) tissues <xref ref-type="bibr" rid="bib0418">[418]</xref>, both in combination with fuzzy C-means (FCM) clustering method. Besides, the second study also used two-point Dixon sequences in image acquisition. The latest research in this field has been the use of zero echo time (ZTE), initially proposed by Wiesinger, Sacolick <xref ref-type="bibr" rid="bib0419">[419]</xref> as part of a segmentation method for cranial bone structures. All studies found in the literature, specially the ones related to ZTE, indicate an improvement in results compared to the use of atlas-based methods <xref ref-type="bibr" rid="bib0420">[420]</xref>, <xref ref-type="bibr" rid="bib0421">[421]</xref>, <xref ref-type="bibr" rid="bib0422">[422]</xref>, <xref ref-type="bibr" rid="bib0423">[423]</xref>.</p>
<p id="para0296">The basis of atlas-based approaches is the use of an MR template instead of multistep segmentation procedures, so atlas-based methods use a general map while segmentation-based methods generate a map for each individual. This template can be obtained from two sources: an image labeled as if it were a segmentation of the different tissues or a coregistered attenuation map from a PET or CT scan with continuous attenuation values <xref ref-type="bibr" rid="bib0354">[354]</xref>. As usual, it is obtained from an average of co-registered normal patients, and it has to be warped to the target patient image volume. In any case, most of the studies related to this approach combine atlas-based and segmentation-based methods since the use of an atlas can be an important source of information and can reduce computational cost <xref ref-type="bibr" rid="bib0424">[424]</xref>, <xref ref-type="bibr" rid="bib0425">[425]</xref>, <xref ref-type="bibr" rid="bib0426">[426]</xref>. Despite this, other studies have proposed methods purely based on atlas images <xref ref-type="bibr" rid="bib0405">[405,</xref>
<xref ref-type="bibr" rid="bib0409">409]</xref>, such as the one developed by Johanson, Garpebring <xref ref-type="bibr" rid="bib0427">[427]</xref> where linear attenuation coefficients are predicted from a Gaussian mixture regression algorithm.</p>
<p id="para0297">Once the attenuation map is generated, attenuation correction is applied in the process. For that, two techniques are mainly used <xref ref-type="bibr" rid="bib0386">[386]</xref>. The first one multiplies the attenuation coefficients obtained by PET image data in the sinogram (or projection) space <xref ref-type="bibr" rid="bib0428">[428]</xref>. The other procedure is performed when PET image reconstruction is performed with an iterative algorithm, using attenuation coefficients as data weighting. While PET images can apply both methods, only iterative methods are used for SPECT <xref ref-type="bibr" rid="bib0429">[429]</xref>, <xref ref-type="bibr" rid="bib0430">[430]</xref>, <xref ref-type="bibr" rid="bib0431">[431]</xref>.</p>
</sec>
<sec id="sec0072">
<label>9.1.3</label>
<title>Scatter effect</title>
<p id="para0298">Scatter in PET and SPECT is another relevant effect for absolute quantification, especially in SPECT, and which is usually associated with Compton scattering. It consists of an energy loss and a change of direction of a photon after an interaction with surrounding atoms. Nevertheless, this effect is not highly relevant in the clinical environment, according to the literature. The reason for this is that the techniques implemented have little impact on the final result, and since the photon change of direction is practically zero, the energy loss is minimal. However, several researchers are convinced that in order to obtain a high accuracy quantitative image, this artifact should be removed <xref ref-type="bibr" rid="bib0432">[432,</xref>
<xref ref-type="bibr" rid="bib0433">433]</xref>.</p>
<p id="para0299">Several methods have been proposed for scatter correction, but most of them were developed many years ago and are inefficient. However, a few recent methods are getting remarkable results. Regarding the categorization of these techniques, a similar classification could be made for both PET and SPECT, considering that PET techniques began to be developed much earlier. Following the review of Zaidi and Montandon <xref ref-type="bibr" rid="bib0432">[432]</xref>, the classification would consist of five groups: hardware approaches using coarse septa or beam stoppers, multiple-energy-window approaches, convolution/deconvolution-based approaches, approaches based on direct estimation of scatter distribution and approaches based on statistical reconstruction. Due to the low clinical implementation and the diversity of existing methods, this work will only highlight the most innovative and relevant methods. Therefore, readers interested in this effect are advised to read the following reviews <xref ref-type="bibr" rid="bib0389">[389,</xref>
<xref ref-type="bibr" rid="bib0433">433,</xref>
<xref ref-type="bibr" rid="bib0434">434]</xref>. <xref ref-type="table" rid="tbl0007">Table 7</xref>
summarizes the different existing categories indicating some examples.<table-wrap id="tbl0007" position="float"><label>Table 7</label><caption><p>Classification of scatter correction methods.</p></caption><alt-text id="alt0031">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Category</th><th valign="top">Methods Description</th><th valign="top">Benefit</th><th valign="top">Limitations</th><th valign="top">Examples</th></tr></thead><tbody><tr><td valign="top">Hardware-based techniques</td><td valign="top">If coarse septa or beam stoppers are used. lines of response intercepted by the septa can be used to determine the scatter component</td><td valign="top">No noise increase</td><td valign="top">Unused</td><td valign="top"><xref ref-type="bibr" rid="bib0442">[442,</xref><xref ref-type="bibr" rid="bib0443">443]</xref></td></tr><tr><td valign="top">Multiple-energy window techniques</td><td valign="top">The energy spectrum is estimated by using windows below and above the photopeak window</td><td valign="top">Highly used Simple</td><td valign="top">Noise</td><td valign="top"><xref ref-type="bibr" rid="bib0444">[444,</xref><xref ref-type="bibr" rid="bib0445">445]</xref></td></tr><tr><td valign="top">Convolution and deconvolution-based techniques</td><td valign="top">In this case, the standard energy acquisition window is used. Data collected in it helps to estimate the distribution of scatter</td><td valign="top">Good image contrast Good accuracy</td><td valign="top">Not commonly used</td><td valign="top"><xref ref-type="bibr" rid="bib0446">[446]</xref>, <xref ref-type="bibr" rid="bib0447">[447]</xref>, <xref ref-type="bibr" rid="bib0448">[448]</xref></td></tr><tr><td valign="top">Direct calculation techniques</td><td valign="top">Extract information from emission data, or a combination of emission and transmission data for estimating scatter distribution. Monte Carlo technique and ToF information can achieve great progress</td><td valign="top">The most popular High accuracy</td><td valign="top">Computational cost</td><td valign="top"><xref ref-type="bibr" rid="bib0449">[449]</xref>, <xref ref-type="bibr" rid="bib0450">[450]</xref>, <xref ref-type="bibr" rid="bib0451">[451]</xref>, <xref ref-type="bibr" rid="bib0452">[452]</xref></td></tr><tr><td valign="top">Iterative reconstruction-based scatter-correction techniques</td><td valign="top">Scatter distribution is obtained and used during image reconstruction</td><td valign="top">Parallel processing High contrast Low noise</td><td valign="top">Computational cost</td><td valign="top"><xref ref-type="bibr" rid="bib0368">[368,</xref><xref ref-type="bibr" rid="bib0453">[453]</xref>, <xref ref-type="bibr" rid="bib0454">[454]</xref>, <xref ref-type="bibr" rid="bib0455">[455]</xref></td></tr></tbody></table></table-wrap></p>
<p id="para0300">The first method proposed to narrow the photopeak energy window to avoid the acceptance of scattered photons. However, this technique has significant drawbacks, such as the elimination of unscattered photons in the process, and therefore, a loss of intensity appears in the image <xref ref-type="bibr" rid="bib0434">[434,</xref>
<xref ref-type="bibr" rid="bib0435">435]</xref>. Thus, multiple-energy-window became much more popular, and it is one of the simplest and most used approaches. The techniques have been developed from two <xref ref-type="bibr" rid="bib0436">[436,</xref>
<xref ref-type="bibr" rid="bib0437">437]</xref> three <xref ref-type="bibr" rid="bib0438">[438,</xref>
<xref ref-type="bibr" rid="bib0439">439]</xref> or even multiple energy windows <xref ref-type="bibr" rid="bib0440">[440,</xref>
<xref ref-type="bibr" rid="bib0441">441]</xref>. Decades have passed since it is known that Monte Carlo techniques are ideal for scatter correction <xref ref-type="bibr" rid="bib0433">[433]</xref>, <xref ref-type="bibr" rid="bib0434">[434]</xref>, <xref ref-type="bibr" rid="bib0435">[435]</xref>. Nevertheless, it has recently become a viable solution in the clinical environment, as the computational cost is reducing and techniques are faster. Moreover, the Monte Carlo technique can be applied to several methods, either iterative reconstruction based or direct calculation methods.</p>
<p id="para0301">Finally, another factor to consider in the quantification of functional images is the voluntary and involuntary movement of the patient (e.g. breathing). However, since it does not have high relevance in neuroimaging, its analysis has been dismissed in this study.</p>
</sec>
</sec>
<sec id="sec0073">
<label>9.2</label>
<title>PET and SPECT in neurology</title>
<p id="para0302">Functional imaging is very useful for the diagnosis of neurodegenerative diseases, such as Alzheimer's disease or Parkinson's disease due to differences in brain activity which can be observed in temporal and parietal lobes (AD) or the striatum (PD) with respect to healthy subjects. Therefore, the systems must allow a correct quantification in order to offer an accurate diagnosis of the patient's condition.</p>
<p id="para0303">As already mentioned, the correction of the aforementioned effects improves the quantification of the images. For example, some studies associated with attenuation correction demonstrate this improvement. Delso, Kemp <xref ref-type="bibr" rid="bib0421">[421]</xref> achieved a bias reduction of −0<italic>.</italic>5% using a CT-based correction instead of a regular ZTE attenuation correction in a PET/MR image. Also associated with PET/MR images, Berker, Franke <xref ref-type="bibr" rid="bib0415">[415]</xref> compared 4-class tissue segmentation and 3-class tissue segmentation, concluding that better results are obtained with a 4-class tissue segmentation, and these results are also very similar to those that would be obtained with a PET/CT system. In the study developed by Sousa, Appel <xref ref-type="bibr" rid="bib0422">[422]</xref>, a comparison between using an attenuation correction method based on ZTE and atlas-based correction shows that the former produces less variability than the latter. However, bias correction is similar in analyzed brain regions. For example, the correlation coefficient associated with anterior cortical regions is 0.99 for ZTE-based correction and 0.92 for atlas-based correction. Similar results are obtained in the study by Sgard, Khalife <xref ref-type="bibr" rid="bib0423">[423]</xref>. Other interesting results are produced by those that apply machine learning methods. For example, the deep convolutional neural network used by Yang, Park <xref ref-type="bibr" rid="bib0411">[411]</xref> for both attenuation and scatter correction designed for situations where it is difficult to use a combined CT or transmission source. The results were similar to the ones obtain with an CT-based scatter and attenuation correction.</p>
<p id="para0304">PVC methods are also used in brain images. For example, regarding PD studies, Du, Tsui <xref ref-type="bibr" rid="bib0376">[376]</xref> proved than using a modified GTM method in brain SPECT images, the underestimation of striatal activities could be reduced to 1<italic>.</italic>2% from an initial value of 30%.</p>
<p id="para0305">Finally, a correct normalization of the images can also highly increase the accuracy of the study. Salas-Gonzalez, Gorriz <xref ref-type="bibr" rid="bib0456">[456]</xref> proposed a method for intensity normalization of FP-CIT SPECT brain image based on <italic>α</italic>-stable distribution. This method was tested by Castillo-Barnes, Arenas <xref ref-type="bibr" rid="bib0457">[457]</xref>, showing significant differences between the images before and after normalizing. For more information on intensity normalization, especially associated with PD, we recommend reading <xref ref-type="bibr" rid="bib0458">[458]</xref>.</p>
<p id="para0306">In conclusion, the possibility of improving the techniques exposed in this manuscript to reduce unwanted effects on images is highlighted. Despite being applied in clinical systems, they are not considered of high relevance due to the limited improvement they provide or, sometimes, the increase in noise they produce. Therefore, the area of absolute quantification must continue to be investigated for more accurate and faster solutions.</p>
</sec>
</sec>
<sec id="sec0074">
<label>10</label>
<title>Conclusion</title>
<p id="para0307">This review presents an overview of multimodal data fusion in the field of neuroimaging, including current developments and challenges. We first outlined the fundamental limitations of individual modalities, which can include distortion, non-quantitative nature, and limited temporal/spatial resolutions. These limitations are the general motivators for the development of multimodal neuroimaging and fusion. Multimodal neuroimaging provides more comprehensive information on pathology.</p>
<p id="para0308">We have summarised the individual benefits and limitations of the current imaging technologies and modalities, including CT, PET, SPECT, MRI, fMRI, DWI, PWI, and MRF. Building upon the available individual techniques, we summarised current development and application of multimodal neuroimaging and fusion in terms of neurological disorders and brain diseases, with a focus in three areas: developing brains, degenerative brains, and psychiatric disorders. The utilization of multiple modalities helps in clinical diagnosis, prevention of misdiagnosis, progression analysis, and research-oriented studies that allow us to gain a more in-depth understanding of human brain pathologies. Nowadays the effects of COVID in the human body are not well-known and maybe we could find in the future some people affected in the brain structure (micro ictus or ischemia). Nevertheless, the fusion techniques and strategies discussed in this survey may be transferred to COVID-19 multimodality image analysis <xref ref-type="bibr" rid="bib0459">[459]</xref>. AI can contribute to the information fusion <xref ref-type="bibr" rid="bib0460">[460]</xref>.</p>
<p id="para0309">The forms of multimodality fusion include multi-modal, multi-focus, multi-temporal, and multi-view. These forms combine images from different instruments/acquisitions, acquisition focal lengths, time of acquisitions and conditions, respectively. Fusion rules were specified with respect to its components and three levels of fusion and theoretical foundation, i.e. rules derived from fuzzy logic, statistic models, and the human vision system. Then we summarised conventional and novel image decomposition and reconstruction methods for the fusion process, including methods based on RGB-IHS, pyramid representations, multi-resolution analysis, sparse representation, and salient features. In addition, we summarised both subjective and objective methods for fusion quality assessment.</p>
<p id="para0310">The major benefits of multimodal data fusion in neuroimaging include distortion correction, higher temporal/spatial resolution, the combination of structural and functional information. We summarised these benefits with the current applications of multimodal fusion, e.g. MR-PET, EEG-fMRI, and EPI correction. This review also adds a particular focus on the application of multimodal image fusion in standardization, via atlas-based anatomical brain segmentation and the use of multi-atlas fusion. In addition, we summarised the effect of multimodal data fusion on the shift of neuroimaging diagnosis from qualitative analysis to quantitative evaluation, with an example of the effect of multimodal data in photon attenuation, scatter, and partial volume effects of PET and SPECT quantification.</p>
<p id="para0311">Modern neuroimaging has seen significant improvements in acquisition quality and a constant increase in the abundance of imaging modalities. The fusion of modalities combines complementary information, expands resolution limits, provides standardization, and improves data quality.</p>
<p id="para0312">It is expected that the effect of multimodal imaging fusion could also effectively scale the amount and quality of information accessible to radiologists for more both precise diagnosis and higher quality research in multiple aspects.</p>
</sec>
<sec id="sec0074a">
<title>CRediT authorship contribution statement</title>
<p id="para0312a">
<bold>Yu-Dong Zhang:</bold> Conceptualization, Project administration, Resources, Supervision, Validation, Writing - original draft, Writing - review &amp; editing. <bold>Zhengchao Dong:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Shui-Hua Wang:</bold> Methodology, Validation, Writing - original draft, Writing - review &amp; editing. <bold>Xiang Yu:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Xujing Yao:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Qinghua Zhou:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Hua Hu:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Min Li:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Carmen Jiménez-Mesa:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Javier Ramirez:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Francisco J. Martinez:</bold> Validation, Writing - original draft, Writing - review &amp; editing. <bold>Juan Manuel Gorriz:</bold> Conceptualization, Methodology, Project administration, Resources, Supervision, Validation, Writing - original draft, Writing - review &amp; editing.</p>
</sec>
<sec sec-type="COI-statement">
<title>Declaration of Competing Interest</title>
<p id="para0313">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
<p id="para0314">The authors declare the following financial interests/personal relationships which may be considered as potential competing interests.</p>
</sec>
</body>
<back>
<ref-list id="cebibl1">
<title>References</title>
<ref id="bib0001">
<label>1</label>
<element-citation id="sbref0001" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Trip</surname>
<given-names>S.A.</given-names>
</name>
</person-group>
<article-title>Imaging in multiple sclerosis</article-title>
<source/>J. Neurol. Neurosurg. Psychiatry
          <volume>76</volume>
<issue>Suppl 3</issue>
<year>2005</year>
<fpage>iii11</fpage>
<lpage>iii18</lpage>
<pub-id pub-id-type="pmid">16107385</pub-id>
</element-citation>
</ref>
<ref id="bib0002">
<label>2</label>
<element-citation id="sbref0002" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Levenson</surname>
<given-names>R.W.</given-names>
</name>
</person-group>
<article-title>Emotional and behavioral symptoms in neurodegenerative disease: a model for studying the neural bases of psychopathology</article-title>
<source/>Annu. Rev. Clin. Psychol.
          <volume>10</volume>
<year>2014</year>
<fpage>581</fpage>
<lpage>606</lpage>
<pub-id pub-id-type="pmid">24437433</pub-id>
</element-citation>
</ref>
<ref id="bib0003">
<label>3</label>
<element-citation id="sbref0003" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bertram</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>The genetic epidemiology of neurodegenerative disease</article-title>
<source/>J. Clin. Invest.
          <volume>115</volume>
<issue>6</issue>
<year>2005</year>
<fpage>1449</fpage>
<lpage>1457</lpage>
<pub-id pub-id-type="pmid">15931380</pub-id>
</element-citation>
</ref>
<ref id="bib0004">
<label>4</label>
<element-citation id="sbref0004" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>MR image features predicting hemorrhagic transformation in acute cerebral infarction: a multimodal study</article-title>
<source/>Neuroradiology
          <volume>57</volume>
<issue>11</issue>
<year>2015</year>
<fpage>1145</fpage>
<lpage>1152</lpage>
<pub-id pub-id-type="pmid">26280516</pub-id>
</element-citation>
</ref>
<ref id="bib0005">
<label>5</label>
<element-citation id="sbref0005" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Macintosh</surname>
<given-names>B.J.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance imaging to visualize stroke and characterize stroke recovery: a review</article-title>
<source/>Front. Neurol.
          <volume>4</volume>
<year>2013</year>
<fpage>60</fpage>
<pub-id pub-id-type="pmid">23750149</pub-id>
</element-citation>
</ref>
<ref id="bib0006">
<label>6</label>
<element-citation id="sbref0006" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ercan</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>A multimodal MRI approach to identify and characterize microstructural brain changes in neuropsychiatric systemic lupus erythematosus</article-title>
<source/>Neuroimage Clin.
          <volume>8</volume>
<year>2015</year>
<fpage>337</fpage>
<lpage>344</lpage>
<pub-id pub-id-type="pmid">26106559</pub-id>
</element-citation>
</ref>
<ref id="bib0007">
<label>7</label>
<element-citation id="sbref0007" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Astley</surname>
<given-names>S.J.</given-names>
</name>
</person-group>
<article-title>Functional magnetic resonance imaging outcomes from a comprehensive magnetic resonance study of children with fetal alcohol spectrum disorders</article-title>
<source/>J. Neurodev. Disord.
          <volume>1</volume>
<issue>1</issue>
<year>2009</year>
<fpage>61</fpage>
<lpage>80</lpage>
<pub-id pub-id-type="pmid">21547622</pub-id>
</element-citation>
</ref>
<ref id="bib0008">
<label>8</label>
<element-citation id="sbref0008" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Calhoun</surname>
<given-names>V.D.</given-names>
</name>
</person-group>
<article-title>Multimodal fusion of brain imaging data: a key to finding the missing link(s) in complex mental illness</article-title>
<source/>Biol. Psychiatry Cognit. Neurosci, Neuroimaging
          <volume>1</volume>
<issue>3</issue>
<year>2016</year>
<fpage>230</fpage>
<lpage>244</lpage>
<pub-id pub-id-type="pmid">27347565</pub-id>
</element-citation>
</ref>
<ref id="bib0009">
<label>9</label>
<element-citation id="sbref0009" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tulay</surname>
<given-names>E.E.</given-names>
</name>
</person-group>
<article-title>Multimodal neuroimaging: basic concepts and classification of neuropsychiatric diseases</article-title>
<source/>Clin. EEG Neurosci.
          <volume>50</volume>
<issue>1</issue>
<year>2019</year>
<fpage>20</fpage>
<lpage>33</lpage>
<pub-id pub-id-type="pmid">29925268</pub-id>
</element-citation>
</ref>
<ref id="bib0010">
<label>10</label>
<element-citation id="sbref0010" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Uludağ</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>General overview on the merits of multimodal neuroimaging data fusion</article-title>
<source/>Neuroimage
          <volume>102</volume>
<year>2014</year>
<fpage>3</fpage>
<lpage>10</lpage>
<pub-id pub-id-type="pmid">24845622</pub-id>
</element-citation>
</ref>
<ref id="bib0011">
<label>11</label>
<element-citation id="sbref0011" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Tang</surname>
<given-names>T.</given-names>
</name>
</person-group>
<chapter-title>PET/SPECT/MRI multimodal nanoparticles</chapter-title>
<source/>Design and Applications of Nanoparticles in Biomedical Imaging
          <year>2017</year>
<publisher-name>Springer</publisher-name>
<fpage>205</fpage>
<lpage>228</lpage>
</element-citation>
</ref>
<ref id="bib0012">
<label>12</label>
<element-citation id="sbref0012" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hu</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<article-title>From PET/CT to PET/MRI: advances in instrumentation and clinical applications</article-title>
<source/>Mol. Pharm.
          <volume>11</volume>
<issue>11</issue>
<year>2014</year>
<fpage>3798</fpage>
<lpage>3809</lpage>
<pub-id pub-id-type="pmid">25058336</pub-id>
</element-citation>
</ref>
<ref id="bib0013">
<label>13</label>
<element-citation id="sbref0013" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Luker</surname>
<given-names>G.D.</given-names>
</name>
</person-group>
<article-title>Optical imaging: current applications and future directions</article-title>
<source/>J. Nucl. Med.
          <volume>49</volume>
<issue>1</issue>
<year>2008</year>
<fpage>1</fpage>
<lpage>4</lpage>
<pub-id pub-id-type="pmid">18077528</pub-id>
</element-citation>
</ref>
<ref id="bib0014">
<label>14</label>
<element-citation id="sbref0014" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weiskopf</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Principles of a brain-computer interface (BCI) based on real-time functional magnetic resonance imaging (fMRI)</article-title>
<source/>IEEE Trans. Biomed. Eng.
          <volume>51</volume>
<issue>6</issue>
<year>2004</year>
<fpage>966</fpage>
<lpage>970</lpage>
<pub-id pub-id-type="pmid">15188865</pub-id>
</element-citation>
</ref>
<ref id="bib0015">
<label>15</label>
<element-citation id="sbref0015" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hari</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Magnetoencephalography: from SQUIDs to neuroscience: neuroimage 20th anniversary special edition</article-title>
<source/>Neuroimage
          <volume>61</volume>
<issue>2</issue>
<year>2012</year>
<fpage>386</fpage>
<lpage>396</lpage>
<pub-id pub-id-type="pmid">22166794</pub-id>
</element-citation>
</ref>
<ref id="bib0016">
<label>16</label>
<element-citation id="sbref0016" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Michel</surname>
<given-names>C.M.</given-names>
</name>
</person-group>
<article-title>Towards the utilization of EEG as a brain imaging tool</article-title>
<source/>Neuroimage
          <volume>61</volume>
<issue>2</issue>
<year>2012</year>
<fpage>371</fpage>
<lpage>385</lpage>
<pub-id pub-id-type="pmid">22227136</pub-id>
</element-citation>
</ref>
<ref id="bib0017">
<label>17</label>
<element-citation id="sbref0017" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ances</surname>
<given-names>B.M.</given-names>
</name>
</person-group>
<article-title>Regional differences in the coupling of cerebral blood flow and oxygen metabolism changes in response to activation: implications for BOLD-fMRI</article-title>
<source/>Neuroimage
          <volume>39</volume>
<issue>4</issue>
<year>2008</year>
<fpage>1510</fpage>
<lpage>1521</lpage>
<pub-id pub-id-type="pmid">18164629</pub-id>
</element-citation>
</ref>
<ref id="bib0018">
<label>18</label>
<element-citation id="sbref0018" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blockley</surname>
<given-names>N.P.</given-names>
</name>
</person-group>
<article-title>A review of calibrated blood oxygenation level‐dependent (BOLD) methods for the measurement of task‐induced changes in brain oxygen metabolism</article-title>
<source/>NMR Biomed.
          <volume>26</volume>
<issue>8</issue>
<year>2013</year>
<fpage>987</fpage>
<lpage>1003</lpage>
<pub-id pub-id-type="pmid">22945365</pub-id>
</element-citation>
</ref>
<ref id="bib0019">
<label>19</label>
<element-citation id="sbref0019" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chiarelli</surname>
<given-names>P.A.</given-names>
</name>
</person-group>
<article-title>A calibration method for quantitative BOLD fMRI based on hyperoxia</article-title>
<source/>Neuroimage
          <volume>37</volume>
<issue>3</issue>
<year>2007</year>
<fpage>808</fpage>
<lpage>820</lpage>
<pub-id pub-id-type="pmid">17632016</pub-id>
</element-citation>
</ref>
<ref id="bib0020">
<label>20</label>
<mixed-citation id="sbref0020" publication-type="other">Hoge, R.D., Calibrated fMRI. NeuroImage, 2012. 62(2): p. 930–937.</mixed-citation>
</ref>
<ref id="bib0021">
<label>21</label>
<element-citation id="sbref0021" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Borogovac</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Arterial spin labeling (ASL) fMRI: advantages, theoretical constrains and experimental challenges in neurosciences</article-title>
<source/>Int. J. Biomed. Imaging
          <year>2012</year>
<fpage>2012</fpage>
</element-citation>
</ref>
<ref id="bib0022">
<label>22</label>
<element-citation id="sbref0022" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zeng</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Image distortion correction in EPI: comparison of field mapping with point spread function mapping</article-title>
<source/>Mag. Reson. Med.
          <volume>48</volume>
<issue>1</issue>
<year>2002</year>
<fpage>137</fpage>
<lpage>146</lpage>
</element-citation>
</ref>
<ref id="bib0023">
<label>23</label>
<element-citation id="sbref0023" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Musalar</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Conventional vs invert-grayscale X-ray for diagnosis of pneumothorax in the emergency setting</article-title>
<source/>Am. J. Emerg. Med.
          <volume>35</volume>
<issue>9</issue>
<year>2017</year>
<fpage>1217</fpage>
<lpage>1221</lpage>
<pub-id pub-id-type="pmid">28343817</pub-id>
</element-citation>
</ref>
<ref id="bib0024">
<label>24</label>
<element-citation id="sbref0024" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Application of high-resolution CT images information in complicated infection of lung tumors</article-title>
<source/>J. Infect. Public Health
          <year>2019</year>
</element-citation>
</ref>
<ref id="bib0025">
<label>25</label>
<element-citation id="sbref0025" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhao</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Application of CT combined with electrocardiographic gating in hypertensive patients with brain and nerve diseases</article-title>
<source/>World Neurosurg.
          <year>2020</year>
</element-citation>
</ref>
<ref id="bib0026">
<label>26</label>
<element-citation id="sbref0026" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stepniak</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Novel 3D printing technology for CT phantom coronary arteries with high geometrical accuracy for biomedical imaging applications</article-title>
<source/>Bioprinting
          <year>2020</year>
<fpage>e00074</fpage>
</element-citation>
</ref>
<ref id="bib0027">
<label>27</label>
<element-citation id="sbref0027" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>S.-H.</given-names>
</name>
</person-group>
<chapter-title>Pathological Brain Detection</chapter-title>
<year>2018</year>
<publisher-name>Springer</publisher-name>
<publisher-loc>Germany</publisher-loc>
<fpage>222</fpage>
</element-citation>
</ref>
<ref id="bib0028">
<label>28</label>
<element-citation id="sbref0028" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Warman Chardon</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>MYO-MRI diagnostic protocols in genetic myopathies</article-title>
<source/>Neuromuscul. Disord.
          <volume>29</volume>
<issue>11</issue>
<year>2019</year>
<fpage>827</fpage>
<lpage>841</lpage>
<pub-id pub-id-type="pmid">31727541</pub-id>
</element-citation>
</ref>
<ref id="bib0029">
<label>29</label>
<element-citation id="sbref0029" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Comparison of the clinical application value of mo-targeted X-ray, color doppler ultrasound and MRI in preoperative comprehensive evaluation of breast cancer</article-title>
<source/>Saudi J. Biol. Sci.
          <volume>26</volume>
<issue>8</issue>
<year>2019</year>
<fpage>1973</fpage>
<lpage>1977</lpage>
<pub-id pub-id-type="pmid">31889780</pub-id>
</element-citation>
</ref>
<ref id="bib0030">
<label>30</label>
<element-citation id="sbref0030" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Magnetic Fe3O4@PVP nanotubes with high heating efficiency for MRI-guided magnetic hyperthermia applications</article-title>
<source/>Mater. Lett.
          <volume>262</volume>
<year>2020</year>
<object-id pub-id-type="publisher-id">127187</object-id>
</element-citation>
</ref>
<ref id="bib0031">
<label>31</label>
<element-citation id="sbref0031" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kazemivalipour</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Reconfigurable MRI technology for low-SAR imaging of deep brain stimulation at 3T: application in bilateral leads, fully-implanted systems, and surgically modified lead trajectories</article-title>
<source/>Neuroimage
          <volume>199</volume>
<year>2019</year>
<fpage>18</fpage>
<lpage>29</lpage>
<pub-id pub-id-type="pmid">31096058</pub-id>
</element-citation>
</ref>
<ref id="bib0032">
<label>32</label>
<element-citation id="sbref0032" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van As</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>MRI of plants and foods</article-title>
<source/>J. Magn. Reson.
          <volume>229</volume>
<year>2013</year>
<fpage>25</fpage>
<lpage>34</lpage>
<pub-id pub-id-type="pmid">23369439</pub-id>
</element-citation>
</ref>
<ref id="bib0033">
<label>33</label>
<mixed-citation id="sbref0033" publication-type="other">Wang, S.-H., et al., Unilateral sensorineural hearing loss identification based on double-density dual-tree complex wavelet transform and multinomial logistic regression. 2019. 26(4): p. 411–426.</mixed-citation>
</ref>
<ref id="bib0034">
<label>34</label>
<element-citation id="sbref6032" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lee</surname>
<given-names>D.H.</given-names>
</name>
</person-group>
<article-title>Mechanisms of contrast enhancement in magnetic resonance imaging</article-title>
<source/>Can. Assoc. Radiol. J.
          <volume>42</volume>
<issue>1</issue>
<year>1991</year>
<fpage>6</fpage>
<lpage>12</lpage>
<pub-id pub-id-type="pmid">2001531</pub-id>
</element-citation>
</ref>
<ref id="bib0035">
<label>35</label>
<element-citation id="sbref0035" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Magnetic properties and magnetization reversal process in (Pt/CoFe/MgO)10 multilayers at low temperature</article-title>
<source/>J. Magn. Magn. Mater.
          <volume>499</volume>
<year>2019</year>
<object-id pub-id-type="publisher-id">166318</object-id>
</element-citation>
</ref>
<ref id="bib0036">
<label>36</label>
<element-citation id="sbref0036" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Parsons</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Single-subject manual independent component analysis and resting state fMRI connectivity outcomes in patients with juvenile absence epilepsy</article-title>
<source/>Magn. Reson. Imaging
          <volume>66</volume>
<year>2020</year>
<fpage>42</fpage>
<lpage>49</lpage>
<pub-id pub-id-type="pmid">31734272</pub-id>
</element-citation>
</ref>
<ref id="bib0037">
<label>37</label>
<element-citation id="sbref0037" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Angenstein</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>The role of ongoing neuronal activity for baseline and stimulus-induced BOLD signals in the rat hippocampus</article-title>
<source/>Neuroimage
          <volume>202</volume>
<year>2019</year>
<object-id pub-id-type="publisher-id">116082</object-id>
</element-citation>
</ref>
<ref id="bib0038">
<label>38</label>
<element-citation id="sbref0038" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Magdziarz</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Lamperti transformation of scaled Brownian motion and related Langevin equations</article-title>
<source/>Commun. Nonlinear Sci. Numer. Simul.
          <volume>83</volume>
<year>2020</year>
<object-id pub-id-type="publisher-id">105077</object-id>
</element-citation>
</ref>
<ref id="bib0039">
<label>39</label>
<element-citation id="sbref0039" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xie</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Denoising arterial spin labeling perfusion MRI with deep machine learning</article-title>
<source/>Magn. Reson. Imaging
          <year>2020</year>
</element-citation>
</ref>
<ref id="bib0040">
<label>40</label>
<element-citation id="sbref0040" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ma</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance fingerprinting</article-title>
<source/>Nature
          <volume>495</volume>
<issue>7440</issue>
<year>2013</year>
<fpage>187</fpage>
<lpage>192</lpage>
<pub-id pub-id-type="pmid">23486058</pub-id>
</element-citation>
</ref>
<ref id="bib0041">
<label>41</label>
<element-citation id="sbref0041" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Johnson</surname>
<given-names>M.H.</given-names>
</name>
</person-group>
<article-title>Functional brain development in humans</article-title>
<source/>Nat. Rev. Neurosci.
          <volume>2</volume>
<issue>7</issue>
<year>2001</year>
<fpage>475</fpage>
<lpage>483</lpage>
<pub-id pub-id-type="pmid">11433372</pub-id>
</element-citation>
</ref>
<ref id="bib0042">
<label>42</label>
<element-citation id="sbref0042" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gogtay</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Dynamic mapping of human cortical development during childhood through early adulthood</article-title>
<source/>Proc. Natl. Acad. Sci. U. S. A.
          <volume>101</volume>
<issue>21</issue>
<year>2004</year>
<fpage>8174</fpage>
<lpage>8179</lpage>
<pub-id pub-id-type="pmid">15148381</pub-id>
</element-citation>
</ref>
<ref id="bib0043">
<label>43</label>
<element-citation id="sbref0043" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Siegel</surname>
<given-names>D.J.</given-names>
</name>
</person-group>
<chapter-title>The Developing Mind: How Relationships and the Brain Interact to Shape Who We Are</chapter-title>
<edition>third ed.</edition>
<year>2020</year>
<publisher-name>Guilford Press</publisher-name>
<publisher-loc>New York</publisher-loc>
</element-citation>
</ref>
<ref id="bib0044">
<label>44</label>
<element-citation id="sbref0044" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>O'Connor</surname>
<given-names>T.G.</given-names>
</name>
</person-group>
<article-title>Maternal antenatal anxiety and behavioural/emotional problems in children: a test of a programming hypothesis</article-title>
<source/>J. Child Psychol. Psychiatry
          <volume>44</volume>
<issue>7</issue>
<year>2003</year>
<fpage>1025</fpage>
<lpage>1036</lpage>
<pub-id pub-id-type="pmid">14531585</pub-id>
</element-citation>
</ref>
<ref id="bib0045">
<label>45</label>
<element-citation id="sbref0045" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nyaradi</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Diet in the early years of life influences cognitive outcomes at 10 years: a prospective cohort study</article-title>
<source/>Acta Paediatr.
          <volume>102</volume>
<issue>12</issue>
<year>2013</year>
<fpage>1165</fpage>
<lpage>1173</lpage>
<pub-id pub-id-type="pmid">23879236</pub-id>
</element-citation>
</ref>
<ref id="bib0046">
<label>46</label>
<element-citation id="sbref0046" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>O'Muircheartaigh</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>White matter development and early cognition in babies and toddlers</article-title>
<source/>Hum. Brain Mapp.
          <volume>35</volume>
<issue>9</issue>
<year>2014</year>
<fpage>4475</fpage>
<lpage>4487</lpage>
<pub-id pub-id-type="pmid">24578096</pub-id>
</element-citation>
</ref>
<ref id="bib0047">
<label>47</label>
<element-citation id="sbref0047" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dean</surname>
<given-names>D.C.</given-names>
<suffix>3rd</suffix>
</name>
</person-group>
<article-title>Modeling healthy male white matter and myelin development: 3 through 60months of age</article-title>
<source/>Neuroimage
          <volume>84</volume>
<year>2014</year>
<fpage>742</fpage>
<lpage>752</lpage>
<pub-id pub-id-type="pmid">24095814</pub-id>
</element-citation>
</ref>
<ref id="bib0048">
<label>48</label>
<element-citation id="sbref0048" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kulikova</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Multi-parametric evaluation of the white matter maturation</article-title>
<source/>Brain Struct. Funct.
          <volume>220</volume>
<issue>6</issue>
<year>2015</year>
<fpage>3657</fpage>
<lpage>3672</lpage>
<pub-id pub-id-type="pmid">25183543</pub-id>
</element-citation>
</ref>
<ref id="bib0049">
<label>49</label>
<element-citation id="sbref0049" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Levine</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Central nervous system abnormalities assessed with prenatal magnetic resonance imaging</article-title>
<source/>Obstet. Gynecol.
          <volume>94</volume>
<issue>6</issue>
<year>1999</year>
<fpage>1011</fpage>
<lpage>1019</lpage>
<pub-id pub-id-type="pmid">10576192</pub-id>
</element-citation>
</ref>
<ref id="bib0050">
<label>50</label>
<element-citation id="sbref0050" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barkovich</surname>
<given-names>A.J.</given-names>
</name>
</person-group>
<article-title>Techniques and methods in pediatric magnetic resonance imaging</article-title>
<source/>Semin. Ultrasound CT MR
          <volume>9</volume>
<issue>3</issue>
<year>1988</year>
<fpage>186</fpage>
<lpage>191</lpage>
<pub-id pub-id-type="pmid">3078667</pub-id>
</element-citation>
</ref>
<ref id="bib0051">
<label>51</label>
<element-citation id="sbref0051" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Holland</surname>
<given-names>B.A.</given-names>
</name>
</person-group>
<article-title>MRI of normal brain maturation</article-title>
<source/>AJNR Am. J. Neuroradiol.
          <volume>7</volume>
<issue>2</issue>
<year>1986</year>
<fpage>201</fpage>
<lpage>208</lpage>
<pub-id pub-id-type="pmid">3082150</pub-id>
</element-citation>
</ref>
<ref id="bib0052">
<label>52</label>
<element-citation id="sbref0052" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Reiss</surname>
<given-names>A.L.</given-names>
</name>
</person-group>
<article-title>Brain development, gender and IQ in children. A volumetric imaging study</article-title>
<source/>Brain
          <volume>119</volume>
<issue>Pt 5</issue>
<year>1996</year>
<fpage>1763</fpage>
<lpage>1774</lpage>
<pub-id pub-id-type="pmid">8931596</pub-id>
</element-citation>
</ref>
<ref id="bib0053">
<label>53</label>
<element-citation id="sbref0053" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jernigan</surname>
<given-names>T.L.</given-names>
</name>
</person-group>
<article-title>Late childhood changes in brain morphology observable with MRI</article-title>
<source/>Dev. Med. Child Neurol.
          <volume>32</volume>
<issue>5</issue>
<year>1990</year>
<fpage>379</fpage>
<lpage>385</lpage>
<pub-id pub-id-type="pmid">2354751</pub-id>
</element-citation>
</ref>
<ref id="bib0054">
<label>54</label>
<element-citation id="sbref0054" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Phan</surname>
<given-names>T.V.</given-names>
</name>
</person-group>
<article-title>Processing of structural neuroimaging data in young children: bridging the gap between current practice and state-of-the-art methods</article-title>
<source/>Dev. Cognit. Neurosci.
          <volume>33</volume>
<year>2018</year>
<fpage>206</fpage>
<lpage>223</lpage>
<pub-id pub-id-type="pmid">29033222</pub-id>
</element-citation>
</ref>
<ref id="bib0055">
<label>55</label>
<element-citation id="sbref0055" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thieba</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Factors associated with successful MRI scanning in unsedated young children</article-title>
<source/>Front. Pediat.r
          <volume>6</volume>
<year>2018</year>
<fpage>146</fpage>
</element-citation>
</ref>
<ref id="bib0056">
<label>56</label>
<element-citation id="sbref0056" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>MR fingerprinting enables quantitative measures of brain tissue relaxation times and myelin water fraction in the first five years of life</article-title>
<source/>Neuroimage
          <volume>186</volume>
<year>2019</year>
<fpage>782</fpage>
<lpage>793</lpage>
<pub-id pub-id-type="pmid">30472371</pub-id>
</element-citation>
</ref>
<ref id="bib0057">
<label>57</label>
<element-citation id="sbref0057" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>de Blank</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance fingerprinting to characterize childhood and young adult brain tumors</article-title>
<source/>Pediatr. Neurosurg.
          <volume>54</volume>
<issue>5</issue>
<year>2019</year>
<fpage>310</fpage>
<lpage>318</lpage>
<pub-id pub-id-type="pmid">31416081</pub-id>
</element-citation>
</ref>
<ref id="bib0058">
<label>58</label>
<element-citation id="sbref0058" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Langa</surname>
<given-names>K.M.</given-names>
</name>
</person-group>
<article-title>The diagnosis and management of mild cognitive impairment: a clinical review</article-title>
<source/>JAMA
          <volume>312</volume>
<issue>23</issue>
<year>2014</year>
<fpage>2551</fpage>
<lpage>2561</lpage>
<pub-id pub-id-type="pmid">25514304</pub-id>
</element-citation>
</ref>
<ref id="bib0059">
<label>59</label>
<element-citation id="sbref0059" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Petersen</surname>
<given-names>R.C.</given-names>
</name>
</person-group>
<article-title>Prevalence of mild cognitive impairment is higher in men. The Mayo Clinic Study of Aging</article-title>
<source/>Neurology
          <volume>75</volume>
<issue>10</issue>
<year>2010</year>
<fpage>889</fpage>
<lpage>897</lpage>
<pub-id pub-id-type="pmid">20820000</pub-id>
</element-citation>
</ref>
<ref id="bib0060">
<label>60</label>
<element-citation id="sbref0060" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jack</surname>
<given-names>C.R.</given-names>
<suffix>Jr.</suffix>
</name>
</person-group>
<article-title>Alzheimer disease: new concepts on its neurobiology and the clinical role imaging will play</article-title>
<source/>Radiology
          <volume>263</volume>
<issue>2</issue>
<year>2012</year>
<fpage>344</fpage>
<lpage>361</lpage>
<pub-id pub-id-type="pmid">22517954</pub-id>
</element-citation>
</ref>
<ref id="bib0061">
<label>61</label>
<element-citation id="sbref0061" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu-Ambrose</surname>
<given-names>T.Y.</given-names>
</name>
</person-group>
<article-title>Increased risk of falling in older community-dwelling women with mild cognitive impairment</article-title>
<source/>Phys. Ther.
          <volume>88</volume>
<issue>12</issue>
<year>2008</year>
<fpage>1482</fpage>
<lpage>1491</lpage>
<pub-id pub-id-type="pmid">18820094</pub-id>
</element-citation>
</ref>
<ref id="bib0062">
<label>62</label>
<element-citation id="sbref0062" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ackl</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Hippocampal metabolic abnormalities in mild cognitive impairment and Alzheimer's disease</article-title>
<source/>Neurosci. Lett.
          <volume>384</volume>
<issue>1-2</issue>
<year>2005</year>
<fpage>23</fpage>
<lpage>28</lpage>
<pub-id pub-id-type="pmid">15905028</pub-id>
</element-citation>
</ref>
<ref id="bib0063">
<label>63</label>
<element-citation id="sbref0063" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Petersen</surname>
<given-names>R.C.</given-names>
</name>
</person-group>
<article-title>Current concepts in mild cognitive impairment</article-title>
<source/>Arch. Neurol.
          <volume>58</volume>
<issue>12</issue>
<year>2001</year>
<fpage>1985</fpage>
<lpage>1992</lpage>
<pub-id pub-id-type="pmid">11735772</pub-id>
</element-citation>
</ref>
<ref id="bib0064">
<label>64</label>
<element-citation id="sbref0064" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Apostolova</surname>
<given-names>L.G.</given-names>
</name>
</person-group>
<article-title>Use of magnetic resonance imaging to identify mild cognitive impairment: who should be imaged?</article-title>
<source/>CNS Spectr.
          <volume>13</volume>
<issue>10 Suppl 16</issue>
<year>2008</year>
<fpage>18</fpage>
<lpage>20</lpage>
</element-citation>
</ref>
<ref id="bib0065">
<label>65</label>
<element-citation id="sbref0065" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bartos</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Brain volumes and their ratios in Alzheimer s disease on magnetic resonance imaging segmented using Freesurfer 6.0</article-title>
<source/>Psychiatry Res. Neuroimaging
          <volume>287</volume>
<year>2019</year>
<fpage>70</fpage>
<lpage>74</lpage>
<pub-id pub-id-type="pmid">31003044</pub-id>
</element-citation>
</ref>
<ref id="bib0066">
<label>66</label>
<element-citation id="sbref0066" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Basiratnia</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Hippocampal volume and hippocampal angle (a more practical marker) in mild cognitive impairment: a case-control magnetic resonance imaging study</article-title>
<source/>Adv. Biomed. Res.
          <volume>4</volume>
<year>2015</year>
<fpage>192</fpage>
<pub-id pub-id-type="pmid">26605231</pub-id>
</element-citation>
</ref>
<ref id="bib0067">
<label>67</label>
<element-citation id="sbref0067" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Atmaca</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Volumetric MRI study of orbito-frontal cortex and thalamus in obsessive-compulsive personality disorder</article-title>
<source/>J. Clin. Neurosci.
          <volume>64</volume>
<year>2019</year>
<fpage>89</fpage>
<lpage>93</lpage>
<pub-id pub-id-type="pmid">30962057</pub-id>
</element-citation>
</ref>
<ref id="bib0068">
<label>68</label>
<element-citation id="sbref0068" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bilello</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Correlating cognitive decline with white matter lesion and brain atrophy magnetic resonance imaging measurements in alzheimer's disease</article-title>
<source/>J. Alzheimers Dis.
          <volume>48</volume>
<issue>4</issue>
<year>2015</year>
<fpage>987</fpage>
<lpage>994</lpage>
<pub-id pub-id-type="pmid">26402108</pub-id>
</element-citation>
</ref>
<ref id="bib0069">
<label>69</label>
<element-citation id="sbref0069" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huang</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Inhibition of eukaryotic initiation factor 3B suppresses proliferation and promotes apoptosis of chronic myeloid leukemia cells</article-title>
<source/>Adv. Clin. Exp. Med.
          <year>2019</year>
</element-citation>
</ref>
<ref id="bib0070">
<label>70</label>
<element-citation id="sbref0070" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Saka</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Linear measures of temporal lobe atrophy on brain magnetic resonance imaging (MRI) but not visual rating of white matter changes can help discrimination of mild cognitive impairment (MCI) and Alzheimer's disease (AD)</article-title>
<source/>Arch. Gerontol. Geriatr.
          <volume>44</volume>
<issue>2</issue>
<year>2007</year>
<fpage>141</fpage>
<lpage>151</lpage>
<pub-id pub-id-type="pmid">16723158</pub-id>
</element-citation>
</ref>
<ref id="bib0071">
<label>71</label>
<element-citation id="sbref0071" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shen</surname>
<given-names>Q.</given-names>
</name>
</person-group>
<article-title>Volumetric and visual rating of magnetic resonance imaging scans in the diagnosis of amnestic mild cognitive impairment and Alzheimer's disease</article-title>
<source/>Alzheimers Dement.
          <volume>7</volume>
<issue>4</issue>
<year>2011</year>
<fpage>e101</fpage>
<lpage>e108</lpage>
<pub-id pub-id-type="pmid">21784342</pub-id>
</element-citation>
</ref>
<ref id="bib0072">
<label>72</label>
<element-citation id="sbref0072" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chandra</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance imaging in Alzheimer's disease and mild cognitive impairment</article-title>
<source/>J. Neurol.
          <volume>266</volume>
<issue>6</issue>
<year>2019</year>
<fpage>1293</fpage>
<lpage>1302</lpage>
<pub-id pub-id-type="pmid">30120563</pub-id>
</element-citation>
</ref>
<ref id="bib0073">
<label>73</label>
<element-citation id="sbref0073" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xu</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Prediction of progressive mild cognitive impairment by multi-modal neuroimaging biomarkers</article-title>
<source/>J. Alzheimers Dis.
          <volume>51</volume>
<issue>4</issue>
<year>2016</year>
<fpage>1045</fpage>
<lpage>1056</lpage>
<pub-id pub-id-type="pmid">26923024</pub-id>
</element-citation>
</ref>
<ref id="bib0074">
<label>74</label>
<element-citation id="sbref0074" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chiti</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Functional magnetic resonance imaging with encoding task in patients with mild cognitive impairment and different severity of leukoaraiosis</article-title>
<source/>Psychiatry Res. Neuroimaging
          <volume>282</volume>
<year>2018</year>
<fpage>126</fpage>
<lpage>131</lpage>
<pub-id pub-id-type="pmid">30539733</pub-id>
</element-citation>
</ref>
<ref id="bib0075">
<label>75</label>
<element-citation id="sbref0075" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Forouzannezhad</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>A survey on applications and analysis methods of functional magnetic resonance imaging for Alzheimer's disease</article-title>
<source/>J. Neurosci. Methods
          <volume>317</volume>
<year>2019</year>
<fpage>121</fpage>
<lpage>140</lpage>
<pub-id pub-id-type="pmid">30593787</pub-id>
</element-citation>
</ref>
<ref id="bib0076">
<label>76</label>
<element-citation id="sbref0076" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Frederick</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Brain proton magnetic resonance spectroscopy in Alzheimer disease: changes after treatment with xanomeline</article-title>
<source/>Am. J. Geriatr. Psychiatry
          <volume>10</volume>
<issue>1</issue>
<year>2002</year>
<fpage>81</fpage>
<lpage>88</lpage>
<pub-id pub-id-type="pmid">11790638</pub-id>
</element-citation>
</ref>
<ref id="bib0077">
<label>77</label>
<element-citation id="sbref0077" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Modrego</surname>
<given-names>P.J.</given-names>
</name>
</person-group>
<article-title>Conversion from mild cognitive impairment to probable Alzheimer's disease predicted by brain magnetic resonance spectroscopy</article-title>
<source/>Am. J. Psychiatry
          <volume>162</volume>
<issue>4</issue>
<year>2005</year>
<fpage>667</fpage>
<lpage>675</lpage>
<pub-id pub-id-type="pmid">15800137</pub-id>
</element-citation>
</ref>
<ref id="bib0078">
<label>78</label>
<element-citation id="sbref0078" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Garcia Santos</surname>
<given-names>J.M.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance spectroscopy performance for detection of dementia, Alzheimer's disease and mild cognitive impairment in a community-based survey</article-title>
<source/>Dement. Geriatr. Cognit. Disord.
          <volume>26</volume>
<issue>1</issue>
<year>2008</year>
<fpage>15</fpage>
<lpage>25</lpage>
<pub-id pub-id-type="pmid">18566544</pub-id>
</element-citation>
</ref>
<ref id="bib0079">
<label>79</label>
<element-citation id="sbref0079" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jahng</surname>
<given-names>G.H.</given-names>
</name>
</person-group>
<article-title>Glutamine and glutamate complex, as measured by functional magnetic resonance spectroscopy, alters during face-name association task in patients with mild cognitive impairment and alzheimer's disease</article-title>
<source/>J. Alzheimers Dis.
          <volume>52</volume>
<issue>1</issue>
<year>2016</year>
<fpage>145</fpage>
<lpage>159</lpage>
<pub-id pub-id-type="pmid">27060946</pub-id>
</element-citation>
</ref>
<ref id="bib0080">
<label>80</label>
<element-citation id="sbref0080" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vijayakumari</surname>
<given-names>A.A.</given-names>
</name>
</person-group>
<article-title>Glutamatergic response to a low load working memory paradigm in the left dorsolateral prefrontal cortex in patients with mild cognitive impairment: a functional magnetic resonance spectroscopy study</article-title>
<source/>Brain Imaging Behav
          <year>2019</year>
</element-citation>
</ref>
<ref id="bib0081">
<label>81</label>
<element-citation id="sbref0081" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wong</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Reduced hippocampal glutamate and posterior cingulate N-acetyl aspartate in mild cognitive impairment and alzheimer's disease is associated with episodic memory performance and white matter integrity in the cingulum: a pilot study</article-title>
<source/>J. Alzheimers Dis.
          <year>2020</year>
</element-citation>
</ref>
<ref id="bib0082">
<label>82</label>
<element-citation id="sbref0082" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oeltzschner</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Neurometabolites and associations with cognitive deficits in mild cognitive impairment: a magnetic resonance spectroscopy study at 7 Tesla</article-title>
<source/>Neurobiol. Aging
          <volume>73</volume>
<year>2019</year>
<fpage>211</fpage>
<lpage>218</lpage>
<pub-id pub-id-type="pmid">30390554</pub-id>
</element-citation>
</ref>
<ref id="bib0083">
<label>83</label>
<element-citation id="sbref0083" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kantarci</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Proton MRS in mild cognitive impairment</article-title>
<source/>J. Magn. Reson. Imaging
          <volume>37</volume>
<issue>4</issue>
<year>2013</year>
<fpage>770</fpage>
<lpage>777</lpage>
<pub-id pub-id-type="pmid">23526756</pub-id>
</element-citation>
</ref>
<ref id="bib0084">
<label>84</label>
<element-citation id="sbref0084" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Coutinho</surname>
<given-names>A.M.N.</given-names>
</name>
</person-group>
<article-title>Analysis of the posterior cingulate cortex with [18F]FDG-PET and Naa/mI in mild cognitive impairment and Alzheimer's disease: correlations and differences between the two methods</article-title>
<source/>Dement. Neuropsychol.
          <volume>9</volume>
<issue>4</issue>
<year>2015</year>
<fpage>385</fpage>
<lpage>393</lpage>
<pub-id pub-id-type="pmid">29213988</pub-id>
</element-citation>
</ref>
<ref id="bib0085">
<label>85</label>
<element-citation id="sbref0085" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vannini</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Anosognosia for memory deficits in mild cognitive impairment: insight into the neural mechanism using functional and molecular imaging</article-title>
<source/>Neuroimage Clin.
          <volume>15</volume>
<year>2017</year>
<fpage>408</fpage>
<lpage>414</lpage>
<pub-id pub-id-type="pmid">28616381</pub-id>
</element-citation>
</ref>
<ref id="bib0086">
<label>86</label>
<element-citation id="sbref0086" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bailly</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Precuneus and cingulate cortex atrophy and hypometabolism in patients with alzheimer's disease and mild cognitive impairment: MRI and (18)F-FDG PET quantitative analysis using freesurfer</article-title>
<source/>Biomed. Res. Int.
          <volume>2015</volume>
<year>2015</year>
<object-id pub-id-type="publisher-id">583931</object-id>
</element-citation>
</ref>
<ref id="bib0087">
<label>87</label>
<element-citation id="sbref0087" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Marcus</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Brain PET in the diagnosis of Alzheimer's disease</article-title>
<source/>Clin. Nucl. Med.
          <volume>39</volume>
<issue>10</issue>
<year>2014</year>
<fpage>e413</fpage>
<lpage>e422</lpage>
<comment>quiz e423-6</comment>
<pub-id pub-id-type="pmid">25199063</pub-id>
</element-citation>
</ref>
<ref id="bib0088">
<label>88</label>
<element-citation id="sbref0088" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cohen</surname>
<given-names>A.D.</given-names>
</name>
</person-group>
<article-title>Early detection of Alzheimer's disease using PiB and FDG PET</article-title>
<source/>Neurobiol. Dis.
          <volume>72</volume>
<issue>Pt A</issue>
<year>2014</year>
<fpage>117</fpage>
<lpage>122</lpage>
<pub-id pub-id-type="pmid">24825318</pub-id>
</element-citation>
</ref>
<ref id="bib0089">
<label>89</label>
<element-citation id="sbref0089" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Murphy</surname>
<given-names>M.P.</given-names>
</name>
</person-group>
<article-title>Alzheimer's disease and the amyloid-beta peptide</article-title>
<source/>J. Alzheimers Dis.
          <volume>19</volume>
<issue>1</issue>
<year>2010</year>
<fpage>311</fpage>
<lpage>323</lpage>
<pub-id pub-id-type="pmid">20061647</pub-id>
</element-citation>
</ref>
<ref id="bib0090">
<label>90</label>
<element-citation id="sbref0090" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Prediction and characterization of protein-protein interaction networks in swine</article-title>
<source/>Proteome Sci.
          <volume>10</volume>
<issue>1</issue>
<year>2012</year>
<fpage>2</fpage>
<pub-id pub-id-type="pmid">22230699</pub-id>
</element-citation>
</ref>
<ref id="bib0091">
<label>91</label>
<element-citation id="sbref0091" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease</article-title>
<source/>Neuroimage
          <volume>59</volume>
<issue>2</issue>
<year>2012</year>
<fpage>895</fpage>
<lpage>907</lpage>
<pub-id pub-id-type="pmid">21992749</pub-id>
</element-citation>
</ref>
<ref id="bib0092">
<label>92</label>
<element-citation id="sbref0092" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>The impact of marine shipping and its DECA control on air quality in the Pearl River Delta, China</article-title>
<source/>Sci. Total Environ.
          <volume>625</volume>
<year>2018</year>
<fpage>1476</fpage>
<lpage>1485</lpage>
<pub-id pub-id-type="pmid">29996444</pub-id>
</element-citation>
</ref>
<ref id="bib0093">
<label>93</label>
<element-citation id="sbref0093" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>A graph-based integration of multimodal brain imaging data for the detection of early mild cognitive impairment (E-MCI)</article-title>
<source/>Multimodal. Brain Image Anal.
          <volume>2013</volume>
<issue>8159</issue>
<year>2013</year>
<fpage>159</fpage>
<lpage>169</lpage>
</element-citation>
</ref>
<ref id="bib0094">
<label>94</label>
<element-citation id="sbref0094" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Young</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Accurate multimodal probabilistic prediction of conversion to Alzheimer's disease in patients with mild cognitive impairment</article-title>
<source/>Neuroimage Clin.
          <volume>2</volume>
<year>2013</year>
<fpage>735</fpage>
<lpage>745</lpage>
<pub-id pub-id-type="pmid">24179825</pub-id>
</element-citation>
</ref>
<ref id="bib0095">
<label>95</label>
<element-citation id="sbref0095" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lin</surname>
<given-names>S.Y.</given-names>
</name>
</person-group>
<article-title>Multiparametric graph theoretical analysis reveals altered structural and functional network topology in Alzheimer's disease</article-title>
<source/>Neuroimage Clin.
          <volume>22</volume>
<year>2019</year>
<object-id pub-id-type="publisher-id">101680</object-id>
</element-citation>
</ref>
<ref id="bib0096">
<label>96</label>
<element-citation id="sbref0096" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tromp</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Episodic memory in normal aging and Alzheimer disease: insights from imaging and behavioral studies</article-title>
<source/>Ageing Res. Rev.
          <volume>24</volume>
<issue>Pt B</issue>
<year>2015</year>
<fpage>232</fpage>
<lpage>262</lpage>
<pub-id pub-id-type="pmid">26318058</pub-id>
</element-citation>
</ref>
<ref id="bib0097">
<label>97</label>
<element-citation id="sbref0097" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Werheid</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Are faces special in Alzheimer's disease? Cognitive conceptualisation, neural correlates, and diagnostic relevance of impaired memory for faces and names</article-title>
<source/>Cortex
          <volume>43</volume>
<issue>7</issue>
<year>2007</year>
<fpage>898</fpage>
<lpage>906</lpage>
<pub-id pub-id-type="pmid">17941348</pub-id>
</element-citation>
</ref>
<ref id="bib0098">
<label>98</label>
<element-citation id="sbref0098" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cass</surname>
<given-names>S.P.</given-names>
</name>
</person-group>
<article-title>Alzheimer's disease and exercise: a literature review</article-title>
<source/>Curr. Sports Med. Rep.
          <volume>16</volume>
<issue>1</issue>
<year>2017</year>
<fpage>19</fpage>
<lpage>22</lpage>
<pub-id pub-id-type="pmid">28067736</pub-id>
</element-citation>
</ref>
<ref id="bib0099">
<label>99</label>
<element-citation id="sbref0099" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alzheimer's</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>2014 Alzheimer's disease facts and figures</article-title>
<source/>Alzheimers Dement.
          <volume>10</volume>
<issue>2</issue>
<year>2014</year>
<fpage>e47</fpage>
<lpage>e92</lpage>
<pub-id pub-id-type="pmid">24818261</pub-id>
</element-citation>
</ref>
<ref id="bib0100">
<label>100</label>
<element-citation id="sbref0100" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tucholka</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>An empirical comparison of surface-based and volume-based group studies in neuroimaging</article-title>
<source/>Neuroimage
          <volume>63</volume>
<issue>3</issue>
<year>2012</year>
<fpage>1443</fpage>
<lpage>1453</lpage>
<pub-id pub-id-type="pmid">22732555</pub-id>
</element-citation>
</ref>
<ref id="bib0101">
<label>101</label>
<element-citation id="sbref0101" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Henneman</surname>
<given-names>W.J.</given-names>
</name>
</person-group>
<article-title>Hippocampal atrophy rates in Alzheimer disease: added value over whole brain volume measures</article-title>
<source/>Neurology
          <volume>72</volume>
<issue>11</issue>
<year>2009</year>
<fpage>999</fpage>
<lpage>1007</lpage>
<pub-id pub-id-type="pmid">19289740</pub-id>
</element-citation>
</ref>
<ref id="bib0102">
<label>102</label>
<element-citation id="sbref0102" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kerchner</surname>
<given-names>G.A.</given-names>
</name>
</person-group>
<article-title>Hippocampal CA1 apical neuropil atrophy in mild Alzheimer disease visualized with 7-T MRI</article-title>
<source/>Neurology
          <volume>75</volume>
<issue>15</issue>
<year>2010</year>
<fpage>1381</fpage>
<lpage>1387</lpage>
<pub-id pub-id-type="pmid">20938031</pub-id>
</element-citation>
</ref>
<ref id="bib0103">
<label>103</label>
<element-citation id="sbref0103" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Winter</surname>
<given-names>F.L.</given-names>
</name>
</person-group>
<article-title>No association of lower hippocampal volume with alzheimer's disease pathology in late-life depression</article-title>
<source/>Am. J. Psychiatry
          <volume>174</volume>
<issue>3</issue>
<year>2017</year>
<fpage>237</fpage>
<lpage>245</lpage>
<pub-id pub-id-type="pmid">27539488</pub-id>
</element-citation>
</ref>
<ref id="bib0104">
<label>104</label>
<element-citation id="sbref0104" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Can multi-modal neuroimaging evidence from hippocampus provide biomarkers for the progression of amnestic mild cognitive impairment?</article-title>
<source/>Neurosci. Bull.
          <volume>31</volume>
<issue>1</issue>
<year>2015</year>
<fpage>128</fpage>
<lpage>140</lpage>
<pub-id pub-id-type="pmid">25595368</pub-id>
</element-citation>
</ref>
<ref id="bib0105">
<label>105</label>
<element-citation id="sbref0105" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zamboni</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Resting functional connectivity reveals residual functional activity in Alzheimer's disease</article-title>
<source/>Biol. Psychiatry
          <volume>74</volume>
<issue>5</issue>
<year>2013</year>
<fpage>375</fpage>
<lpage>383</lpage>
<pub-id pub-id-type="pmid">23726515</pub-id>
</element-citation>
</ref>
<ref id="bib0106">
<label>106</label>
<element-citation id="sbref0106" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhou</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Predicting regional neurodegeneration from the healthy brain functional connectome</article-title>
<source/>Neuron
          <volume>73</volume>
<issue>6</issue>
<year>2012</year>
<fpage>1216</fpage>
<lpage>1227</lpage>
<pub-id pub-id-type="pmid">22445348</pub-id>
</element-citation>
</ref>
<ref id="bib0107">
<label>107</label>
<element-citation id="sbref0107" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Disrupted functional brain connectome in individuals at risk for Alzheimer's disease</article-title>
<source/>Biol. Psychiatry
          <volume>73</volume>
<issue>5</issue>
<year>2013</year>
<fpage>472</fpage>
<lpage>481</lpage>
<pub-id pub-id-type="pmid">22537793</pub-id>
</element-citation>
</ref>
<ref id="bib0108">
<label>108</label>
<element-citation id="sbref0108" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jin</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Aberrant default mode network in subjects with amnestic mild cognitive impairment using resting-state functional MRI</article-title>
<source/>Magn. Reson. Imaging
          <volume>30</volume>
<issue>1</issue>
<year>2012</year>
<fpage>48</fpage>
<lpage>61</lpage>
<pub-id pub-id-type="pmid">21982164</pub-id>
</element-citation>
</ref>
<ref id="bib0109">
<label>109</label>
<element-citation id="sbref0109" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bayram</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Current understanding of magnetic resonance imaging biomarkers and memory in Alzheimer's disease</article-title>
<source/>Alzheimers Dement.
          <volume>4</volume>
<year>2018</year>
<fpage>395</fpage>
<lpage>413</lpage>
</element-citation>
</ref>
<ref id="bib0110">
<label>110</label>
<element-citation id="sbref0110" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Promteangtrong</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Multimodality imaging approach in Alzheimer disease. Part I: Structural MRI, functional MRI, diffusion tensor imaging and magnetization transfer imaging</article-title>
<source/>Dement. Neuropsychol.
          <volume>9</volume>
<issue>4</issue>
<year>2015</year>
<fpage>318</fpage>
<lpage>329</lpage>
<pub-id pub-id-type="pmid">29213981</pub-id>
</element-citation>
</ref>
<ref id="bib0111">
<label>111</label>
<element-citation id="sbref0111" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Waser</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Neuroimaging markers of global cognition in early Alzheimer's disease: a magnetic resonance imaging-electroencephalography study</article-title>
<source/>Brain Behav.
          <volume>9</volume>
<issue>1</issue>
<year>2019</year>
<fpage>e01197</fpage>
<pub-id pub-id-type="pmid">30592179</pub-id>
</element-citation>
</ref>
<ref id="bib0112">
<label>112</label>
<element-citation id="sbref0112" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Harman</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Technical note: can resting state functional MRI assist in routine clinical diagnosis?</article-title>
<source/>BJR Case Rep.
          <volume>4</volume>
<issue>4</issue>
<year>2018</year>
<object-id pub-id-type="publisher-id">20180030</object-id>
</element-citation>
</ref>
<ref id="bib0113">
<label>113</label>
<element-citation id="sbref0113" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Basheera</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Convolution neural network-based Alzheimer's disease classification using hybrid enhanced independent component analysis based segmented gray matter of T2 weighted magnetic resonance imaging with clinical valuation</article-title>
<source/>Alzheimers Dement.
          <volume>5</volume>
<year>2019</year>
<fpage>974</fpage>
<lpage>986</lpage>
</element-citation>
</ref>
<ref id="bib0114">
<label>114</label>
<element-citation id="sbref0114" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hirjak</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Multimodal Magnetic Resonance Imaging Data Fusion Reveals Distinct Patterns of Abnormal Brain Structure and Function in Catatonia</article-title>
<source/>Schizophr. Bull.
          <volume>46</volume>
<issue>1</issue>
<year>2020</year>
<fpage>202</fpage>
<lpage>210</lpage>
<pub-id pub-id-type="pmid">31174212</pub-id>
</element-citation>
</ref>
<ref id="bib0115">
<label>115</label>
<element-citation id="sbref0115" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chabiniok</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Multiphysics and multiscale modelling, data-model fusion and integration of organ physiology in the clinic: ventricular cardiac mechanics</article-title>
<source/>Interface Focus
          <volume>6</volume>
<issue>2</issue>
<year>2016</year>
<object-id pub-id-type="publisher-id">20150083</object-id>
</element-citation>
</ref>
<ref id="bib0116">
<label>116</label>
<element-citation id="sbref0116" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Adali</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Multi-modal data fusion using source separation: two effective models based on ICA and IVA and their properties</article-title>
<source/>Proc. IEEE Inst. Electr. Electron Eng.
          <volume>103</volume>
<issue>9</issue>
<year>2015</year>
<fpage>1478</fpage>
<lpage>1493</lpage>
<pub-id pub-id-type="pmid">26525830</pub-id>
</element-citation>
</ref>
<ref id="bib0117">
<label>117</label>
<element-citation id="sbref0117" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Marino</surname>
<given-names>B.L.B.</given-names>
</name>
</person-group>
<article-title>Parkinson's disease: a review from the pathophysiology to diagnosis, new perspectives for pharmacological treatment</article-title>
<source/>Mini. Rev. Med. Chem.
          <year>2019</year>
</element-citation>
</ref>
<ref id="bib0118">
<label>118</label>
<element-citation id="sbref0118" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Driver</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>Incidence and remaining lifetime risk of Parkinson disease in advanced age</article-title>
<source/>Neurology
          <volume>72</volume>
<issue>5</issue>
<year>2009</year>
<fpage>432</fpage>
<lpage>438</lpage>
<pub-id pub-id-type="pmid">19188574</pub-id>
</element-citation>
</ref>
<ref id="bib0119">
<label>119</label>
<element-citation id="sbref0119" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lee</surname>
<given-names>J.H.</given-names>
</name>
</person-group>
<article-title>The incidence rates and risk factors of Parkinson disease in patients with psoriasis: a nationwide population-based cohort study</article-title>
<source/>J. Am. Acad. Dermatol.
          <year>2019</year>
</element-citation>
</ref>
<ref id="bib0120">
<label>120</label>
<element-citation id="sbref0120" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van Den Eeden</surname>
<given-names>S.K.</given-names>
</name>
</person-group>
<article-title>Incidence of Parkinson's disease: variation by age, gender, and race/ethnicity</article-title>
<source/>Am. J. Epidemiol.
          <volume>157</volume>
<issue>11</issue>
<year>2003</year>
<fpage>1015</fpage>
<lpage>1022</lpage>
<pub-id pub-id-type="pmid">12777365</pub-id>
</element-citation>
</ref>
<ref id="bib0121">
<label>121</label>
<element-citation id="sbref0121" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bharti</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Neuroimaging advances in Parkinson's disease with freezing of gait: a systematic review</article-title>
<source/>Neuroimage Clin.
          <volume>24</volume>
<year>2019</year>
<object-id pub-id-type="publisher-id">102059</object-id>
</element-citation>
</ref>
<ref id="bib0122">
<label>122</label>
<element-citation id="sbref0122" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Al-Radaideh</surname>
<given-names>A.M.</given-names>
</name>
</person-group>
<article-title>The role of magnetic resonance imaging in the diagnosis of Parkinson's disease: a review</article-title>
<source/>Clin. Imaging
          <volume>40</volume>
<issue>5</issue>
<year>2016</year>
<fpage>987</fpage>
<lpage>996</lpage>
<pub-id pub-id-type="pmid">27288741</pub-id>
</element-citation>
</ref>
<ref id="bib0123">
<label>123</label>
<element-citation id="sbref0123" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alegret</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>MRI atrophy parameters related to cognitive and motor impairment in Parkinson's disease</article-title>
<source/>Neurologia
          <volume>16</volume>
<issue>2</issue>
<year>2001</year>
<fpage>63</fpage>
<lpage>69</lpage>
<pub-id pub-id-type="pmid">11257931</pub-id>
</element-citation>
</ref>
<ref id="bib0124">
<label>124</label>
<element-citation id="sbref0124" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Prasad</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Three-dimensional neuromelanin-sensitive magnetic resonance imaging of the substantia nigra in Parkinson's disease</article-title>
<source/>Eur. J. Neurol.
          <volume>25</volume>
<issue>4</issue>
<year>2018</year>
<fpage>680</fpage>
<lpage>686</lpage>
<pub-id pub-id-type="pmid">29341412</pub-id>
</element-citation>
</ref>
<ref id="bib0125">
<label>125</label>
<element-citation id="sbref0125" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Neuromelanin-sensitive MRI of the substantia nigra: an imaging biomarker to differentiate essential tremor from tremor-dominant Parkinson's disease</article-title>
<source/>Parkinsonism Relat. Disord.
          <volume>58</volume>
<year>2019</year>
<fpage>3</fpage>
<lpage>8</lpage>
<pub-id pub-id-type="pmid">30037690</pub-id>
</element-citation>
</ref>
<ref id="bib0126">
<label>126</label>
<element-citation id="sbref0126" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jin</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Combined visualization of nigrosome-1 and neuromelanin in the substantia nigra using 3T MRI for the differential diagnosis of essential tremor and de novo Parkinson's disease</article-title>
<source/>Front. Neurol.
          <volume>10</volume>
<year>2019</year>
<fpage>100</fpage>
<pub-id pub-id-type="pmid">30809189</pub-id>
</element-citation>
</ref>
<ref id="bib0127">
<label>127</label>
<element-citation id="sbref0127" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Takahashi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Comprehensive MRI quantification of the substantia nigra pars compacta in Parkinson's disease</article-title>
<source/>Eur. J. Radiol.
          <volume>109</volume>
<year>2018</year>
<fpage>48</fpage>
<lpage>56</lpage>
<pub-id pub-id-type="pmid">30527311</pub-id>
</element-citation>
</ref>
<ref id="bib0128">
<label>128</label>
<element-citation id="sbref0128" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Quantitative susceptibility mapping (QSM): decoding MRI data for a tissue magnetic biomarker</article-title>
<source/>Magn. Reson. Med.
          <volume>73</volume>
<issue>1</issue>
<year>2015</year>
<fpage>82-101</fpage>
<pub-id pub-id-type="pmid">25044035</pub-id>
</element-citation>
</ref>
<ref id="bib0129">
<label>129</label>
<element-citation id="sbref0129" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Burciu</surname>
<given-names>R.G.</given-names>
</name>
</person-group>
<article-title>Imaging of motor cortex physiology in Parkinson's disease</article-title>
<source/>Mov. Disord.
          <volume>33</volume>
<issue>11</issue>
<year>2018</year>
<fpage>1688</fpage>
<lpage>1699</lpage>
<pub-id pub-id-type="pmid">30280416</pub-id>
</element-citation>
</ref>
<ref id="bib0130">
<label>130</label>
<element-citation id="sbref0130" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Niethammer</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Functional neuroimaging in Parkinson's disease</article-title>
<source/>Cold Spring Harb. Perspect. Med.
          <volume>2</volume>
<issue>5</issue>
<year>2012</year>
<object-id pub-id-type="publisher-id">a009274</object-id>
</element-citation>
</ref>
<ref id="bib0131">
<label>131</label>
<element-citation id="sbref0131" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Evangelisti</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>L-dopa modulation of brain connectivity in parkinson's disease patients: a pilot EEG-fMRI study</article-title>
<source/>Front. Neurosci.
          <volume>13</volume>
<year>2019</year>
<fpage>611</fpage>
<pub-id pub-id-type="pmid">31258465</pub-id>
</element-citation>
</ref>
<ref id="bib0132">
<label>132</label>
<element-citation id="sbref0132" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tessitore</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Sensorimotor connectivity in Parkinson's disease: the role of functional neuroimaging</article-title>
<source/>Front. Neurol.
          <volume>5</volume>
<year>2014</year>
<fpage>180</fpage>
<pub-id pub-id-type="pmid">25309505</pub-id>
</element-citation>
</ref>
<ref id="bib0133">
<label>133</label>
<element-citation id="sbref0133" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Amboni</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Resting-state functional connectivity associated with mild cognitive impairment in Parkinson's disease</article-title>
<source/>J. Neurol.
          <volume>262</volume>
<issue>2</issue>
<year>2015</year>
<fpage>425</fpage>
<lpage>434</lpage>
<pub-id pub-id-type="pmid">25428532</pub-id>
</element-citation>
</ref>
<ref id="bib0134">
<label>134</label>
<element-citation id="sbref0134" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Borghammer</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Glucose metabolism in small subcortical structures in Parkinson's disease</article-title>
<source/>Acta Neurol. Scand.
          <volume>125</volume>
<issue>5</issue>
<year>2012</year>
<fpage>303</fpage>
<lpage>310</lpage>
<pub-id pub-id-type="pmid">21692755</pub-id>
</element-citation>
</ref>
<ref id="bib0135">
<label>135</label>
<element-citation id="sbref0135" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hilker</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Functional imaging of deep brain stimulation in idiopathic Parkinson's disease</article-title>
<source/>Nervenarzt
          <volume>81</volume>
<issue>10</issue>
<year>2010</year>
<fpage>1204</fpage>
<lpage>1207</lpage>
<pub-id pub-id-type="pmid">20798917</pub-id>
</element-citation>
</ref>
<ref id="bib0136">
<label>136</label>
<element-citation id="sbref0136" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berding</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Resting regional cerebral glucose metabolism in advanced Parkinson's disease studied in the off and on conditions with [(18)F]FDG-PET</article-title>
<source/>Mov. Disord.
          <volume>16</volume>
<issue>6</issue>
<year>2001</year>
<fpage>1014</fpage>
<lpage>1022</lpage>
<pub-id pub-id-type="pmid">11748732</pub-id>
</element-citation>
</ref>
<ref id="bib0137">
<label>137</label>
<element-citation id="sbref0137" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Son</surname>
<given-names>S.J.</given-names>
</name>
</person-group>
<article-title>Imaging analysis of Parkinson's disease patients using SPECT and tractography</article-title>
<source/>Sci. Rep.
          <volume>6</volume>
<year>2016</year>
<fpage>38070</fpage>
<pub-id pub-id-type="pmid">27901100</pub-id>
</element-citation>
</ref>
<ref id="bib0138">
<label>138</label>
<element-citation id="sbref0138" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ruppert</surname>
<given-names>M.C.</given-names>
</name>
</person-group>
<article-title>Network degeneration in Parkinson's disease: multimodal imaging of nigro-striato-cortical dysfunction</article-title>
<source/>Brain
          <year>2020</year>
</element-citation>
</ref>
<ref id="bib0139">
<label>139</label>
<element-citation id="sbref0139" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bowman</surname>
<given-names>F.D.</given-names>
</name>
</person-group>
<article-title>Multimodal imaging signatures of Parkinson's disease</article-title>
<source/>Front. Neurosci.
          <volume>10</volume>
<year>2016</year>
<fpage>131</fpage>
<pub-id pub-id-type="pmid">27147942</pub-id>
</element-citation>
</ref>
<ref id="bib0140">
<label>140</label>
<element-citation id="sbref0140" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Mental Illness</surname>
</name>
</person-group>
<year>2020</year>
<comment>Available from: <ext-link ext-link-type="uri" id="interref0007a" xlink:href="https://www.nimh.nih.gov/health/statistics/mental-illness.shtml">https://www.nimh.nih.gov/health/statistics/mental-illness.shtml</ext-link></comment>
</element-citation>
</ref>
<ref id="bib0141">
<label>141</label>
<element-citation id="sbref0141" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Mental Disorders Affect One in Four People</surname>
</name>
</person-group>
<year>2001</year>
<comment>[cited 2020 9/March]; Available from</comment>
<ext-link ext-link-type="uri" id="interref0007" xlink:href="https://www.who.int/whr/2001/media_centre/press_release">https://www.who.int/whr/2001/media_centre/press_release</ext-link>
</element-citation>
</ref>
<ref id="bib0142">
<label>142</label>
<element-citation id="sbref0142" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rehm</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Global burden of disease and the impact of mental and addictive disorders</article-title>
<source/>Curr. Psychiatry Rep.
          <volume>21</volume>
<issue>2</issue>
<year>2019</year>
<fpage>10</fpage>
<pub-id pub-id-type="pmid">30729322</pub-id>
</element-citation>
</ref>
<ref id="bib0143">
<label>143</label>
<element-citation id="sbref0143" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eaton</surname>
<given-names>W.W.</given-names>
</name>
</person-group>
<article-title>The burden of mental disorders</article-title>
<source/>Epidemiol. Rev.
          <volume>30</volume>
<year>2008</year>
<fpage>p. 1-14</fpage>
</element-citation>
</ref>
<ref id="bib0144">
<label>144</label>
<element-citation id="sbref0144" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Mental Illness Will Cost the World $16 USD Trillion by 2030</surname>
</name>
</person-group>
<year>2018</year>
<comment>[cited 2020 9/3]; Available from</comment>
<ext-link ext-link-type="uri" id="interref0008" xlink:href="https://www.psychiatrictimes.com/mental-health/mental-illness-will-cost-world-16-usd-trillion-2030">https://www.psychiatrictimes.com/mental-health/mental-illness-will-cost-world-16-usd-trillion-2030</ext-link>
</element-citation>
</ref>
<ref id="bib0145">
<label>145</label>
<element-citation id="sbref0145" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Silbersweig</surname>
<given-names>D.A.</given-names>
</name>
</person-group>
<article-title>Neuroimaging in psychiatry: a quarter century of progress</article-title>
<source/>Harv. Rev. Psychiatry
          <volume>25</volume>
<issue>5</issue>
<year>2017</year>
<fpage>195</fpage>
<lpage>197</lpage>
<pub-id pub-id-type="pmid">28885277</pub-id>
</element-citation>
</ref>
<ref id="bib0146">
<label>146</label>
<element-citation id="sbref0146" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cannon</surname>
<given-names>D.M.</given-names>
</name>
</person-group>
<article-title>Neuroimaging in psychiatry</article-title>
<source/>Ir. J. Psychol. Med.
          <volume>24</volume>
<issue>3</issue>
<year>2007</year>
<fpage>86</fpage>
<lpage>88</lpage>
<pub-id pub-id-type="pmid">30290486</pub-id>
</element-citation>
</ref>
<ref id="bib0147">
<label>147</label>
<element-citation id="sbref0147" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wibawa</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Understanding MRI in clinical psychiatry: perspectives from neuroimaging psychiatry registrars</article-title>
<source/>Aust. Psychiatry
          <volume>27</volume>
<issue>4</issue>
<year>2019</year>
<fpage>396</fpage>
<lpage>403</lpage>
</element-citation>
</ref>
<ref id="bib0148">
<label>148</label>
<element-citation id="sbref0148" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Todeva-Radneva</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>The value of neuroimaging techniques in the translation and trans-diagnostic validation of psychiatric diagnoses - selective review</article-title>
<source/>Curr. Top. Med. Chem.
          <year>2020</year>
</element-citation>
</ref>
<ref id="bib0149">
<label>149</label>
<element-citation id="sbref0149" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lai</surname>
<given-names>C.H.</given-names>
</name>
</person-group>
<article-title>Promising neuroimaging biomarkers in depression</article-title>
<source/>Psychiatry Investig.
          <volume>16</volume>
<issue>9</issue>
<year>2019</year>
<fpage>662</fpage>
<lpage>670</lpage>
</element-citation>
</ref>
<ref id="bib0150">
<label>150</label>
<element-citation id="sbref0150" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kessing</surname>
<given-names>L.V.</given-names>
</name>
</person-group>
<article-title>Rate and predictors of conversion from unipolar to bipolar disorder: a systematic review and meta-analysis</article-title>
<source/>Bipolar Disord.
          <volume>19</volume>
<issue>5</issue>
<year>2017</year>
<fpage>324</fpage>
<lpage>335</lpage>
<pub-id pub-id-type="pmid">28714575</pub-id>
</element-citation>
</ref>
<ref id="bib0151">
<label>151</label>
<element-citation id="sbref0151" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vieta</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Early intervention in bipolar disorder</article-title>
<source/>Am. J. Psychiatry
          <volume>175</volume>
<issue>5</issue>
<year>2018</year>
<fpage>411</fpage>
<lpage>426</lpage>
<pub-id pub-id-type="pmid">29361850</pub-id>
</element-citation>
</ref>
<ref id="bib0152">
<label>152</label>
<element-citation id="sbref0152" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kessler</surname>
<given-names>R.C.</given-names>
</name>
</person-group>
<article-title>Epidemiology of women and depression</article-title>
<source/>J. Affect. Disord.
          <volume>74</volume>
<issue>1</issue>
<year>2003</year>
<fpage>5</fpage>
<lpage>13</lpage>
<pub-id pub-id-type="pmid">12646294</pub-id>
</element-citation>
</ref>
<ref id="bib0153">
<label>153</label>
<element-citation id="sbref0153" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Andrews</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Why does the burden of disease persist? Relating the burden of anxiety and depression to effectiveness of treatment</article-title>
<source/>Bull. World Health Organ.
          <volume>78</volume>
<issue>4</issue>
<year>2000</year>
<fpage>446</fpage>
<lpage>454</lpage>
<pub-id pub-id-type="pmid">10885163</pub-id>
</element-citation>
</ref>
<ref id="bib0154">
<label>154</label>
<element-citation id="sbref0154" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schmaal</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Brain structural signatures of adolescent depressive symptom trajectories: a longitudinal magnetic resonance imaging study</article-title>
<source/>J. Am. Acad. Child Adolesc. Psychiatry
          <volume>56</volume>
<issue>7</issue>
<year>2017</year>
<fpage>593-601 e9</fpage>
<pub-id pub-id-type="pmid">28647011</pub-id>
</element-citation>
</ref>
<ref id="bib0155">
<label>155</label>
<element-citation id="sbref0155" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vassilopoulou</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>A magnetic resonance imaging study of hippocampal, amygdala and subgenual prefrontal cortex volumes in major depression subtypes: melancholic versus psychotic depression</article-title>
<source/>J. Affect. Disord.
          <volume>146</volume>
<issue>2</issue>
<year>2013</year>
<fpage>197</fpage>
<lpage>204</lpage>
<pub-id pub-id-type="pmid">23021193</pub-id>
</element-citation>
</ref>
<ref id="bib0156">
<label>156</label>
<element-citation id="sbref0156" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sacchet</surname>
<given-names>M.D.</given-names>
</name>
</person-group>
<article-title>Myelination of the brain in major depressive disorder: an in vivo quantitative magnetic resonance imaging study</article-title>
<source/>Sci. Rep.
          <volume>7</volume>
<issue>1</issue>
<year>2017</year>
<fpage>2200</fpage>
<pub-id pub-id-type="pmid">28526817</pub-id>
</element-citation>
</ref>
<ref id="bib0157">
<label>157</label>
<element-citation id="sbref0157" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nugent</surname>
<given-names>A.C.</given-names>
</name>
</person-group>
<article-title>Multimodal imaging reveals a complex pattern of dysfunction in corticolimbic pathways in major depressive disorder</article-title>
<source/>Hum. Brain Mapp.
          <volume>40</volume>
<issue>13</issue>
<year>2019</year>
<fpage>3940</fpage>
<lpage>3950</lpage>
<pub-id pub-id-type="pmid">31179620</pub-id>
</element-citation>
</ref>
<ref id="bib0158">
<label>158</label>
<element-citation id="sbref0158" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vasic</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Baseline brain perfusion and brain structure in patients with major depression: a multimodal magnetic resonance imaging study</article-title>
<source/>J. Psychiatry Neurosci.
          <volume>40</volume>
<issue>6</issue>
<year>2015</year>
<fpage>412</fpage>
<lpage>421</lpage>
<pub-id pub-id-type="pmid">26125119</pub-id>
</element-citation>
</ref>
<ref id="bib0159">
<label>159</label>
<element-citation id="sbref0159" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Finkelmeyer</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Altered hippocampal function in major depression despite intact structure and resting perfusion</article-title>
<source/>Psychol. Med.
          <volume>46</volume>
<issue>10</issue>
<year>2016</year>
<fpage>2157</fpage>
<lpage>2168</lpage>
<pub-id pub-id-type="pmid">27192934</pub-id>
</element-citation>
</ref>
<ref id="bib0160">
<label>160</label>
<element-citation id="sbref0160" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Development and evaluation of a multimodal marker of major depressive disorder</article-title>
<source/>Hum. Brain Mapp.
          <volume>39</volume>
<issue>11</issue>
<year>2018</year>
<fpage>4420</fpage>
<lpage>4439</lpage>
<pub-id pub-id-type="pmid">30113112</pub-id>
</element-citation>
</ref>
<ref id="bib0161">
<label>161</label>
<element-citation id="sbref0161" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Maglanoc</surname>
<given-names>L.A.</given-names>
</name>
</person-group>
<article-title>Multimodal fusion of structural and functional brain imaging in depression using linked independent component analysis</article-title>
<source/>Hum. Brain Mapp.
          <volume>41</volume>
<issue>1</issue>
<year>2020</year>
<fpage>241</fpage>
<lpage>255</lpage>
<pub-id pub-id-type="pmid">31571370</pub-id>
</element-citation>
</ref>
<ref id="bib0162">
<label>162</label>
<element-citation id="sbref0162" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Widespread decreased grey and white matter in paediatric obsessive-compulsive disorder (OCD): a voxel-based morphometric MRI study</article-title>
<source/>Psychiatry Res.
          <volume>213</volume>
<issue>1</issue>
<year>2013</year>
<fpage>11</fpage>
<lpage>17</lpage>
<pub-id pub-id-type="pmid">23701704</pub-id>
</element-citation>
</ref>
<ref id="bib0163">
<label>163</label>
<element-citation id="sbref0163" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lazaro</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Brain changes in children and adolescents with obsessive-compulsive disorder before and after treatment: a voxel-based morphometric MRI study</article-title>
<source/>Psychiatry Res.
          <volume>172</volume>
<issue>2</issue>
<year>2009</year>
<fpage>140</fpage>
<lpage>146</lpage>
<pub-id pub-id-type="pmid">19321314</pub-id>
</element-citation>
</ref>
<ref id="bib0164">
<label>164</label>
<element-citation id="sbref0164" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Qiu</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Abnormal regional spontaneous neuronal activity associated with symptom severity in treatment-naive patients with obsessive-compulsive disorder revealed by resting-state functional MRI</article-title>
<source/>Neurosci. Lett.
          <volume>640</volume>
<year>2017</year>
<fpage>99</fpage>
<lpage>104</lpage>
<pub-id pub-id-type="pmid">28104431</pub-id>
</element-citation>
</ref>
<ref id="bib0165">
<label>165</label>
<element-citation id="sbref0165" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lazaro</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Cerebral activation in children and adolescents with obsessive-compulsive disorder before and after treatment: a functional MRI study</article-title>
<source/>J. Psychiatr. Res.
          <volume>42</volume>
<issue>13</issue>
<year>2008</year>
<fpage>1051</fpage>
<lpage>1059</lpage>
<pub-id pub-id-type="pmid">18261744</pub-id>
</element-citation>
</ref>
<ref id="bib0166">
<label>166</label>
<element-citation id="sbref0166" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bu</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Investigating the predictive value of different resting-state functional MRI parameters in obsessive-compulsive disorder</article-title>
<source/>Transl. Psychiatry
          <volume>9</volume>
<issue>1</issue>
<year>2019</year>
<fpage>17</fpage>
<pub-id pub-id-type="pmid">30655506</pub-id>
</element-citation>
</ref>
<ref id="bib0167">
<label>167</label>
<element-citation id="sbref0167" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Park</surname>
<given-names>S.E.</given-names>
</name>
</person-group>
<article-title>Metabolic abnormality in the right dorsolateral prefrontal cortex in patients with obsessive-compulsive disorder: proton magnetic resonance spectroscopy</article-title>
<source/>Acta Neuropsychiatr.
          <volume>29</volume>
<issue>3</issue>
<year>2017</year>
<fpage>164</fpage>
<lpage>169</lpage>
<pub-id pub-id-type="pmid">27748207</pub-id>
</element-citation>
</ref>
<ref id="bib0168">
<label>168</label>
<element-citation id="sbref0168" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fan</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Abnormalities in metabolite concentrations in tourette's disorder and obsessive-compulsive disorder-A proton magnetic resonance spectroscopy study</article-title>
<source/>Psychoneuroendocrinology
          <volume>77</volume>
<year>2017</year>
<fpage>211</fpage>
<lpage>217</lpage>
<pub-id pub-id-type="pmid">28104554</pub-id>
</element-citation>
</ref>
<ref id="bib0169">
<label>169</label>
<element-citation id="sbref0169" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tukel</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Proton magnetic resonance spectroscopy in obsessive-compulsive disorder: evidence for reduced neuronal integrity in the anterior cingulate</article-title>
<source/>Psychiatry Res.
          <volume>224</volume>
<issue>3</issue>
<year>2014</year>
<fpage>275</fpage>
<lpage>280</lpage>
<pub-id pub-id-type="pmid">25241042</pub-id>
</element-citation>
</ref>
<ref id="bib0170">
<label>170</label>
<element-citation id="sbref0170" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brennan</surname>
<given-names>B.P.</given-names>
</name>
</person-group>
<article-title>A critical review of magnetic resonance spectroscopy studies of obsessive-compulsive disorder</article-title>
<source/>Biol. Psychiatry
          <volume>73</volume>
<issue>1</issue>
<year>2013</year>
<fpage>24</fpage>
<lpage>31</lpage>
<pub-id pub-id-type="pmid">22831979</pub-id>
</element-citation>
</ref>
<ref id="bib0171">
<label>171</label>
<element-citation id="sbref0171" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Investigation of anterior cingulate cortex gamma-aminobutyric acid and glutamate-glutamine levels in obsessive-compulsive disorder using magnetic resonance spectroscopy</article-title>
<source/>BMC Psychiatry
          <volume>19</volume>
<issue>1</issue>
<year>2019</year>
<fpage>164</fpage>
<pub-id pub-id-type="pmid">31146727</pub-id>
</element-citation>
</ref>
<ref id="bib0172">
<label>172</label>
<element-citation id="sbref0172" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<article-title>Brain gamma-aminobutyric acid (GABA) concentration of the prefrontal lobe in unmedicated patients with obsessive-compulsive disorder: a research of magnetic resonance spectroscopy</article-title>
<source/>Shanghai Arch. Psychiatry
          <volume>28</volume>
<issue>5</issue>
<year>2016</year>
<fpage>263</fpage>
<lpage>270</lpage>
<pub-id pub-id-type="pmid">28638200</pub-id>
</element-citation>
</ref>
<ref id="bib0173">
<label>173</label>
<element-citation id="sbref0173" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rosenberg</surname>
<given-names>D.R.</given-names>
</name>
</person-group>
<article-title>Reduced anterior cingulate glutamate in pediatric major depression: a magnetic resonance spectroscopy study</article-title>
<source/>Biol. Psychiatry
          <volume>58</volume>
<issue>9</issue>
<year>2005</year>
<fpage>700</fpage>
<lpage>704</lpage>
<pub-id pub-id-type="pmid">16084860</pub-id>
</element-citation>
</ref>
<ref id="bib0174">
<label>174</label>
<element-citation id="sbref0174" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lazaro</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Proton magnetic resonance spectroscopy in pediatric obsessive-compulsive disorder: longitudinal study before and after treatment</article-title>
<source/>Psychiatry Res.
          <volume>201</volume>
<issue>1</issue>
<year>2012</year>
<fpage>17</fpage>
<lpage>24</lpage>
<pub-id pub-id-type="pmid">22281202</pub-id>
</element-citation>
</ref>
<ref id="bib0175">
<label>175</label>
<element-citation id="sbref0175" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Whiteside</surname>
<given-names>S.P.H.</given-names>
</name>
</person-group>
<article-title>The effect of behavior therapy on caudate N-acetyl-l-aspartic acid in adults with obsessive-compulsive disorder</article-title>
<source/>Psychiatry Res.
          <volume>201</volume>
<issue>1</issue>
<year>2012</year>
<fpage>10</fpage>
<lpage>16</lpage>
<pub-id pub-id-type="pmid">22284151</pub-id>
</element-citation>
</ref>
<ref id="bib0176">
<label>176</label>
<element-citation id="sbref0176" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pico-Perez</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Modality-specific overlaps in brain structure and function in obsessive-compulsive disorder: multimodal meta-analysis of case-control MRI studies</article-title>
<source/>Neurosci. Biobehav. Rev.
          <volume>112</volume>
<year>2020</year>
<fpage>83</fpage>
<lpage>94</lpage>
<pub-id pub-id-type="pmid">32006553</pub-id>
</element-citation>
</ref>
<ref id="bib0177">
<label>177</label>
<element-citation id="sbref0177" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Moreira</surname>
<given-names>P.S.</given-names>
</name>
</person-group>
<article-title>The neural correlates of obsessive-compulsive disorder: a multimodal perspective</article-title>
<source/>Transl. Psychiatry
          <year>2017</year>
<fpage>7</fpage>
</element-citation>
</ref>
<ref id="bib0178">
<label>178</label>
<element-citation id="sbref0178" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Choi</surname>
<given-names>J.S.</given-names>
</name>
</person-group>
<article-title>Morphometric alterations of anterior superior temporal cortex in obsessive-compulsive disorder</article-title>
<source/>Depress. Anxiety
          <volume>23</volume>
<issue>5</issue>
<year>2006</year>
<fpage>290</fpage>
<lpage>296</lpage>
<pub-id pub-id-type="pmid">16688740</pub-id>
</element-citation>
</ref>
<ref id="bib0179">
<label>179</label>
<element-citation id="sbref0179" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fan</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Spontaneous neural activity in the right superior temporal gyrus and left middle temporal gyrus is associated with insight level in obsessive-compulsive disorder</article-title>
<source/>J. Affect. Disord.
          <volume>207</volume>
<year>2017</year>
<fpage>203</fpage>
<lpage>211</lpage>
<pub-id pub-id-type="pmid">27723545</pub-id>
</element-citation>
</ref>
<ref id="bib0180">
<label>180</label>
<element-citation id="sbref0180" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bruin</surname>
<given-names>W.</given-names>
</name>
</person-group>
<article-title>Diagnostic neuroimaging markers of obsessive-compulsive disorder: initial evidence from structural and functional MRI studies</article-title>
<source/>Prog. Neuropsychopharmacol. Biol. Psychiatry
          <volume>91</volume>
<year>2019</year>
<fpage>49</fpage>
<lpage>59</lpage>
<pub-id pub-id-type="pmid">30107192</pub-id>
</element-citation>
</ref>
<ref id="bib0181">
<label>181</label>
<element-citation id="sbref0181" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>de Salles Andrade</surname>
<given-names>J.B.</given-names>
</name>
</person-group>
<article-title>An MRI study of the metabolic and structural abnormalities in obsessive-compulsive disorder</article-title>
<source/>Front. Hum. Neurosci.
          <volume>13</volume>
<year>2019</year>
<fpage>186</fpage>
<pub-id pub-id-type="pmid">31333428</pub-id>
</element-citation>
</ref>
<ref id="bib0182">
<label>182</label>
<element-citation id="sbref0182" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>McCutcheon</surname>
<given-names>R.A.</given-names>
</name>
</person-group>
<article-title>Schizophrenia-an overview</article-title>
<source/>JAMA Psychiatry
          <year>2019</year>
<fpage>1</fpage>
<lpage>10</lpage>
</element-citation>
</ref>
<ref id="bib0183">
<label>183</label>
<element-citation id="sbref0183" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Davies</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>A meta-analytic review of the relationship between neurocognition, metacognition and functional outcome in schizophrenia</article-title>
<source/>J. Ment. Health
          <year>2018</year>
<fpage>1</fpage>
<lpage>11</lpage>
</element-citation>
</ref>
<ref id="bib0184">
<label>184</label>
<element-citation id="sbref0184" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zamanpoor</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Schizophrenia in a genomic era: a review from the pathogenesis, genetic and environmental etiology to diagnosis and treatment insights</article-title>
<source/>Psychiatr. Genet.
          <volume>30</volume>
<issue>1</issue>
<year>2020</year>
<fpage>1</fpage>
<lpage>9</lpage>
<pub-id pub-id-type="pmid">31764709</pub-id>
</element-citation>
</ref>
<ref id="bib0185">
<label>185</label>
<element-citation id="sbref0185" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tandon</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Schizophrenia, "Just the Facts": what we know in 2008 part 1: overview</article-title>
<source/>Schizophr. Res.
          <volume>100</volume>
<issue>1-3</issue>
<year>2008</year>
<fpage>4</fpage>
<lpage>19</lpage>
<pub-id pub-id-type="pmid">18291627</pub-id>
</element-citation>
</ref>
<ref id="bib0186">
<label>186</label>
<element-citation id="sbref0186" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>van Erp</surname>
<given-names>T.G.</given-names>
</name>
</person-group>
<article-title>Subcortical brain volume abnormalities in 2028 individuals with schizophrenia and 2540 healthy controls via the ENIGMA consortium</article-title>
<source/>Mol. Psychiatry
          <volume>21</volume>
<issue>4</issue>
<year>2016</year>
<fpage>585</fpage>
<pub-id pub-id-type="pmid">26283641</pub-id>
</element-citation>
</ref>
<ref id="bib0187">
<label>187</label>
<element-citation id="sbref0187" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cahn</surname>
<given-names>W.</given-names>
</name>
</person-group>
<article-title>Brain volume changes in first-episode schizophrenia: a 1-year follow-up study</article-title>
<source/>Arch. Gen. Psychiatry
          <volume>59</volume>
<issue>11</issue>
<year>2002</year>
<fpage>1002</fpage>
<lpage>1010</lpage>
<pub-id pub-id-type="pmid">12418933</pub-id>
</element-citation>
</ref>
<ref id="bib0188">
<label>188</label>
<element-citation id="sbref0188" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Peri</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Brain structural abnormalities at the onset of schizophrenia and bipolar disorder: a meta-analysis of controlled magnetic resonance imaging studies</article-title>
<source/>Curr. Pharm. Des.
          <volume>18</volume>
<issue>4</issue>
<year>2012</year>
<fpage>486</fpage>
<lpage>494</lpage>
<pub-id pub-id-type="pmid">22239579</pub-id>
</element-citation>
</ref>
<ref id="bib0189">
<label>189</label>
<element-citation id="sbref0189" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vita</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Progressive loss of cortical gray matter in schizophrenia: a meta-analysis and meta-regression of longitudinal MRI studies</article-title>
<source/>Transl. Psychiatry
          <volume>2</volume>
<year>2012</year>
<fpage>e19</fpage>
</element-citation>
</ref>
<ref id="bib0190">
<label>190</label>
<element-citation id="sbref0190" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thompson</surname>
<given-names>P.M.</given-names>
</name>
</person-group>
<article-title>Mapping adolescent brain change reveals dynamic wave of accelerated gray matter loss in very early-onset schizophrenia</article-title>
<source/>Proc. Natl. Acad. Sci. U. S. A.
          <volume>98</volume>
<issue>20</issue>
<year>2001</year>
<fpage>11650</fpage>
<lpage>11655</lpage>
<pub-id pub-id-type="pmid">11573002</pub-id>
</element-citation>
</ref>
<ref id="bib0191">
<label>191</label>
<element-citation id="sbref0191" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Karlsgodt</surname>
<given-names>K.H.</given-names>
</name>
</person-group>
<article-title>Diffusion imaging of white matter in schizophrenia: progress and future directions</article-title>
<source/>Biol. Psychiatry Cognit. Neurosci. Neuroimaging
          <volume>1</volume>
<issue>3</issue>
<year>2016</year>
<fpage>209</fpage>
<lpage>217</lpage>
<pub-id pub-id-type="pmid">27453952</pub-id>
</element-citation>
</ref>
<ref id="bib0192">
<label>192</label>
<element-citation id="sbref0192" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Peters</surname>
<given-names>B.D.</given-names>
</name>
</person-group>
<article-title>White matter fibertracking in first-episode schizophrenia, schizoaffective patients and subjects at ultra-high risk of psychosis</article-title>
<source/>Neuropsychobiology
          <volume>58</volume>
<issue>1</issue>
<year>2008</year>
<fpage>19</fpage>
<lpage>28</lpage>
<pub-id pub-id-type="pmid">18781087</pub-id>
</element-citation>
</ref>
<ref id="bib0193">
<label>193</label>
<element-citation id="sbref0193" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Price</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>The corpus callosum in first episode schizophrenia: a diffusion tensor imaging study</article-title>
<source/>J. Neurol. Neurosurg. Psychiatry
          <volume>76</volume>
<issue>4</issue>
<year>2005</year>
<fpage>585</fpage>
<lpage>587</lpage>
<pub-id pub-id-type="pmid">15774453</pub-id>
</element-citation>
</ref>
<ref id="bib0194">
<label>194</label>
<element-citation id="sbref0194" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kelly</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Widespread white matter microstructural differences in schizophrenia across 4322 individuals: results from the ENIGMA Schizophrenia DTI Working Group</article-title>
<source/>Mol. Psychiatry
          <volume>23</volume>
<issue>5</issue>
<year>2018</year>
<fpage>1261</fpage>
<lpage>1269</lpage>
<pub-id pub-id-type="pmid">29038599</pub-id>
</element-citation>
</ref>
<ref id="bib0195">
<label>195</label>
<element-citation id="sbref0195" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Birur</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Brain structure, function, and neurochemistry in schizophrenia and bipolar disorder-a systematic review of the magnetic resonance neuroimaging literature</article-title>
<source/>NPJ Schizophr.
          <volume>3</volume>
<year>2017</year>
<fpage>15</fpage>
<pub-id pub-id-type="pmid">28560261</pub-id>
</element-citation>
</ref>
<ref id="bib0196">
<label>196</label>
<element-citation id="sbref0196" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fox</surname>
<given-names>J.M.</given-names>
</name>
</person-group>
<article-title>Default mode functional connectivity is associated with social functioning in schizophrenia</article-title>
<source/>J. Abnorm. Psychol.
          <volume>126</volume>
<issue>4</issue>
<year>2017</year>
<fpage>392</fpage>
<lpage>405</lpage>
<pub-id pub-id-type="pmid">28358526</pub-id>
</element-citation>
</ref>
<ref id="bib0197">
<label>197</label>
<element-citation id="sbref0197" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Q.</given-names>
</name>
</person-group>
<article-title>Anatomical insights into disrupted small-world networks in schizophrenia</article-title>
<source/>Neuroimage
          <volume>59</volume>
<issue>2</issue>
<year>2012</year>
<fpage>1085</fpage>
<lpage>1093</lpage>
<pub-id pub-id-type="pmid">21963918</pub-id>
</element-citation>
</ref>
<ref id="bib0198">
<label>198</label>
<element-citation id="sbref0198" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tarumi</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Levels of glutamatergic neurometabolites in patients with severe treatment-resistant schizophrenia: a proton magnetic resonance spectroscopy study</article-title>
<source/>Neuropsychopharmacology
          <volume>45</volume>
<issue>4</issue>
<year>2020</year>
<fpage>632</fpage>
<lpage>640</lpage>
<pub-id pub-id-type="pmid">31842203</pub-id>
</element-citation>
</ref>
<ref id="bib0199">
<label>199</label>
<element-citation id="sbref0199" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Iwata</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Glutamatergic neurometabolite levels in patients with ultra-treatment-resistant schizophrenia: a cross-sectional 3T proton magnetic resonance spectroscopy study</article-title>
<source/>Biol. Psychiatry
          <volume>85</volume>
<issue>7</issue>
<year>2019</year>
<fpage>596</fpage>
<lpage>605</lpage>
<pub-id pub-id-type="pmid">30389132</pub-id>
</element-citation>
</ref>
<ref id="bib0200">
<label>200</label>
<element-citation id="sbref0200" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brugger</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Proton magnetic resonance spectroscopy and illness stage in schizophrenia–a systematic review and meta-analysis</article-title>
<source/>Biol. Psychiatry
          <volume>69</volume>
<issue>5</issue>
<year>2011</year>
<fpage>495</fpage>
<lpage>503</lpage>
<pub-id pub-id-type="pmid">21145039</pub-id>
</element-citation>
</ref>
<ref id="bib0201">
<label>201</label>
<element-citation id="sbref0201" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sui</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>In search of multimodal neuroimaging biomarkers of cognitive deficits in schizophrenia</article-title>
<source/>Biol. Psychiatry
          <volume>78</volume>
<issue>11</issue>
<year>2015</year>
<fpage>794</fpage>
<lpage>804</lpage>
<pub-id pub-id-type="pmid">25847180</pub-id>
</element-citation>
</ref>
<ref id="bib0202">
<label>202</label>
<element-citation id="sbref0202" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cadena</surname>
<given-names>E.J.</given-names>
</name>
</person-group>
<article-title>A longitudinal multimodal neuroimaging study to examine relationships between resting state glutamate and task related BOLD response in schizophrenia</article-title>
<source/>Front. Psychiatry
          <volume>9</volume>
<year>2018</year>
<fpage>632</fpage>
<pub-id pub-id-type="pmid">30555359</pub-id>
</element-citation>
</ref>
<ref id="bib0203">
<label>203</label>
<element-citation id="sbref0203" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Isobe</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Multimodal neuroimaging as a window into the pathological physiology of schizophrenia: current trends and issues</article-title>
<source/>Neurosci. Res.
          <volume>102</volume>
<year>2016</year>
<fpage>29</fpage>
<lpage>38</lpage>
<pub-id pub-id-type="pmid">26235681</pub-id>
</element-citation>
</ref>
<ref id="bib0204">
<label>204</label>
<element-citation id="sbref0204" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aine</surname>
<given-names>C.J.</given-names>
</name>
</person-group>
<article-title>Multimodal neuroimaging in schizophrenia: description and dissemination</article-title>
<source/>Neuroinformatics
          <volume>15</volume>
<issue>4</issue>
<year>2017</year>
<fpage>343</fpage>
<lpage>364</lpage>
<pub-id pub-id-type="pmid">28812221</pub-id>
</element-citation>
</ref>
<ref id="bib0205">
<label>205</label>
<element-citation id="sbref0205" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Comparison of cerebral blood flow acquired by simultaneous [15O] water positron emission tomography and arterial spin labeling magnetic resonance imaging</article-title>
<source/>J. Cereb. Blood Flow Metab.
          <volume>34</volume>
<issue>8</issue>
<year>2014</year>
<fpage>1373</fpage>
<lpage>1380</lpage>
<pub-id pub-id-type="pmid">24849665</pub-id>
</element-citation>
</ref>
<ref id="bib0206">
<label>206</label>
<element-citation id="sbref0206" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rosenkranz</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Present and future of simultaneous EEG-fMRI. Magnetic resonance materials in physics</article-title>
<source/>Biol. Med.
          <volume>23</volume>
<issue>5-6</issue>
<year>2010</year>
<fpage>309</fpage>
<lpage>316</lpage>
</element-citation>
</ref>
<ref id="bib0207">
<label>207</label>
<element-citation id="sbref0207" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goldman</surname>
<given-names>R.I.</given-names>
</name>
</person-group>
<article-title>Simultaneous EEG and fMRI of the alpha rhythm</article-title>
<source/>Neuroreport
          <volume>13</volume>
<issue>18</issue>
<year>2002</year>
<fpage>2487</fpage>
<pub-id pub-id-type="pmid">12499854</pub-id>
</element-citation>
</ref>
<ref id="bib0208">
<label>208</label>
<element-citation id="sbref0208" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laufs</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>EEG-correlated fMRI of human alpha activity</article-title>
<source/>Neuroimage
          <volume>19</volume>
<issue>4</issue>
<year>2003</year>
<fpage>1463</fpage>
<lpage>1476</lpage>
<pub-id pub-id-type="pmid">12948703</pub-id>
</element-citation>
</ref>
<ref id="bib0209">
<label>209</label>
<element-citation id="sbref0209" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ritter</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>simultaneous EEG–fMRI</article-title>
<source/>Neurosci. Biobehav. Rev.
          <volume>30</volume>
<issue>6</issue>
<year>2006</year>
<fpage>823</fpage>
<lpage>838</lpage>
<pub-id pub-id-type="pmid">16911826</pub-id>
</element-citation>
</ref>
<ref id="bib0210">
<label>210</label>
<element-citation id="sbref0210" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eickhoff</surname>
<given-names>S.B.</given-names>
</name>
</person-group>
<article-title>Assignment of functional activations to probabilistic cytoarchitectonic areas revisited</article-title>
<source/>Neuroimage
          <volume>36</volume>
<issue>3</issue>
<year>2007</year>
<fpage>511</fpage>
<lpage>521</lpage>
<pub-id pub-id-type="pmid">17499520</pub-id>
</element-citation>
</ref>
<ref id="bib0211">
<label>211</label>
<element-citation id="sbref0211" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Acton</surname>
<given-names>P.D.</given-names>
</name>
</person-group>
<article-title>Quantification in PET</article-title>
<source/>Radiol. Clin.
          <volume>42</volume>
<issue>6</issue>
<year>2004</year>
<fpage>1055</fpage>
<lpage>1062</lpage>
</element-citation>
</ref>
<ref id="bib0212">
<label>212</label>
<element-citation id="sbref0212" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zeeberg</surname>
<given-names>B.R.</given-names>
</name>
</person-group>
<article-title>Accuracy of in vivo neuroreceptor quantification by PET and review of steady-state, transient, double injection, and equilibrium models</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>7</volume>
<issue>3</issue>
<year>1988</year>
<fpage>203</fpage>
<lpage>212</lpage>
<pub-id pub-id-type="pmid">18230470</pub-id>
</element-citation>
</ref>
<ref id="bib0213">
<label>213</label>
<element-citation id="sbref0213" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oh</surname>
<given-names>S.H.</given-names>
</name>
</person-group>
<article-title>Distortion correction in EPI at ultra‐high‐field MRI using PSF mapping with optimal combination of shift detection dimension</article-title>
<source/>Magn. Reson. Med.
          <volume>68</volume>
<issue>4</issue>
<year>2012</year>
<fpage>1239</fpage>
<lpage>1246</lpage>
<pub-id pub-id-type="pmid">22213517</pub-id>
</element-citation>
</ref>
<ref id="bib0214">
<label>214</label>
<element-citation id="sbref0214" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Andersson</surname>
<given-names>J.L.</given-names>
</name>
</person-group>
<article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title>
<source/>Neuroimage
          <volume>20</volume>
<issue>2</issue>
<year>2003</year>
<fpage>870</fpage>
<lpage>888</lpage>
<pub-id pub-id-type="pmid">14568458</pub-id>
</element-citation>
</ref>
<ref id="bib0215">
<label>215</label>
<element-citation id="sbref0215" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Choi</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>A new intensity-hue-saturation fusion approach to image fusion with a tradeoff parameter</article-title>
<source/>IEEE Trans. Geosci. Remote Sens.
          <volume>44</volume>
<issue>6</issue>
<year>2006</year>
<fpage>1672</fpage>
<lpage>1682</lpage>
</element-citation>
</ref>
<ref id="bib0216">
<label>216</label>
<element-citation id="sbref0216" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Kaur</surname>
<given-names>G.</given-names>
</name>
</person-group>
<chapter-title>Survey on multifocus image fusion techniques</chapter-title>
<source/>2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)
          <year>2016</year>
<publisher-name>IEEE</publisher-name>
<fpage>1420</fpage>
<lpage>1424</lpage>
</element-citation>
</ref>
<ref id="bib0217">
<label>217</label>
<element-citation id="sbref0217" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Phamila</surname>
<given-names>Y.A.V.</given-names>
</name>
</person-group>
<article-title>Discrete Cosine Transform based fusion of multi-focus images for visual sensor networks</article-title>
<source/>Signal Process.
          <volume>95</volume>
<year>2014</year>
<fpage>161</fpage>
<lpage>170</lpage>
</element-citation>
</ref>
<ref id="bib0218">
<label>218</label>
<element-citation id="sbref0218" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cao</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Multi-focus image fusion based on spatial frequency in discrete cosine transform domain</article-title>
<source/>IEEE Signal Process Lett.
          <volume>22</volume>
<issue>2</issue>
<year>2014</year>
<fpage>220</fpage>
<lpage>224</lpage>
</element-citation>
</ref>
<ref id="bib0219">
<label>219</label>
<element-citation id="sbref0219" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Multifocus image fusion using region segmentation and spatial frequency</article-title>
<source/>Image Vision Comput.
          <volume>26</volume>
<issue>7</issue>
<year>2008</year>
<fpage>971</fpage>
<lpage>979</lpage>
</element-citation>
</ref>
<ref id="bib0220">
<label>220</label>
<element-citation id="sbref0220" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Multi-focus image fusion with dense SIFT</article-title>
<source/>Inf. Fusion
          <volume>23</volume>
<year>2015</year>
<fpage>139</fpage>
<lpage>155</lpage>
</element-citation>
</ref>
<ref id="bib0221">
<label>221</label>
<element-citation id="sbref0221" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pu</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Contrast-based image fusion using the discrete wavelet transform</article-title>
<source/>Opt. Eng.
          <year>2000</year>
<fpage>39</fpage>
</element-citation>
</ref>
<ref id="bib0222">
<label>222</label>
<element-citation id="sbref0222" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Singh</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>MHWT-a modified haar wavelet transformation for image fusion</article-title>
<source/>Int. J. Comput. Appl.
          <volume>79</volume>
<issue>1</issue>
<year>2013</year>
</element-citation>
</ref>
<ref id="bib0223">
<label>223</label>
<element-citation id="sbref0223" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Burt</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>The Laplacian pyramid as a compact image code</article-title>
<source/>IEEE Trans. Commun.
          <volume>31</volume>
<issue>4</issue>
<year>1983</year>
<fpage>532</fpage>
<lpage>540</lpage>
</element-citation>
</ref>
<ref id="bib0224">
<label>224</label>
<element-citation id="sbref0224" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vijayarajan</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Discrete wavelet transform based principal component averaging fusion for medical images</article-title>
<source/>AEU-Int. J. Electron. Commun.
          <volume>69</volume>
<issue>6</issue>
<year>2015</year>
<fpage>896</fpage>
<lpage>902</lpage>
</element-citation>
</ref>
<ref id="bib0225">
<label>225</label>
<element-citation id="sbref0225" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Singh</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Fusion of multimodal medical images using Daubechies complex wavelet transform–a multiresolution approach</article-title>
<source/>Inf. Fusion
          <volume>19</volume>
<year>2014</year>
<fpage>49</fpage>
<lpage>60</lpage>
</element-citation>
</ref>
<ref id="bib0226">
<label>226</label>
<element-citation id="sbref0226" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vulliemoz</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Simultaneous intracranial EEG and fMRI of interictal epileptic discharges in humans</article-title>
<source/>Neuroimage
          <volume>54</volume>
<issue>1</issue>
<year>2011</year>
<fpage>182</fpage>
<lpage>190</lpage>
<pub-id pub-id-type="pmid">20708083</pub-id>
</element-citation>
</ref>
<ref id="bib0227">
<label>227</label>
<element-citation id="sbref0227" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sui</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>A review of multivariate methods for multimodal fusion of brain imaging data</article-title>
<source/>J. Neurosci. Methods
          <volume>204</volume>
<issue>1</issue>
<year>2012</year>
<fpage>68</fpage>
<lpage>81</lpage>
<pub-id pub-id-type="pmid">22108139</pub-id>
</element-citation>
</ref>
<ref id="bib0228">
<label>228</label>
<element-citation id="sbref0228" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Zuzhang</surname>
<given-names>X</given-names>
</name>
</person-group>
<chapter-title>new_image_fusion</chapter-title>
<year>2020</year>
<comment>[cited 2020 2.23]</comment>
</element-citation>
</ref>
<ref id="bib0229">
<label>229</label>
<element-citation id="sbref0229" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Johnson</surname>
<given-names>K.A.</given-names>
</name>
</person-group>
<chapter-title>Brain Image</chapter-title>
<year>2020</year>
<comment>[cited 2020 02.24]; Available from</comment>
<ext-link ext-link-type="uri" id="interref0009" xlink:href="http://www.med.harvard.edu/AANLIB/cases/case9/mr1-tc1/020.html">http://www.med.harvard.edu/AANLIB/cases/case9/mr1-tc1/020.html</ext-link>
</element-citation>
</ref>
<ref id="bib0230">
<label>230</label>
<element-citation id="sbref0230" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shen</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Cross-scale coefficient selection for volumetric medical image fusion</article-title>
<source/>IEEE Trans. Biomed. Eng.
          <volume>60</volume>
<issue>4</issue>
<year>2012</year>
<fpage>1069</fpage>
<lpage>1079</lpage>
<pub-id pub-id-type="pmid">22868528</pub-id>
</element-citation>
</ref>
<ref id="bib0231">
<label>231</label>
<element-citation id="sbref0231" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lewis</surname>
<given-names>J.J.</given-names>
</name>
</person-group>
<article-title>Pixel-and region-based image fusion with complex wavelets</article-title>
<source/>Inf. Fusion
          <volume>8</volume>
<issue>2</issue>
<year>2007</year>
<fpage>119</fpage>
<lpage>130</lpage>
</element-citation>
</ref>
<ref id="bib0232">
<label>232</label>
<element-citation id="sbref0232" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nandi</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Principal component analysis in medical image processing: a study</article-title>
<source/>Int. J. Image Min.
          <volume>1</volume>
<issue>1</issue>
<year>2015</year>
<fpage>65</fpage>
<lpage>86</lpage>
</element-citation>
</ref>
<ref id="bib0233">
<label>233</label>
<element-citation id="sbref0233" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vijayarajan</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Iterative block level principal component averaging medical image fusion</article-title>
<source/>Optik
          <volume>125</volume>
<issue>17</issue>
<year>2014</year>
<fpage>4751</fpage>
<lpage>4757</lpage>
</element-citation>
</ref>
<ref id="bib0234">
<label>234</label>
<element-citation id="sbref0234" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>H.-q.</given-names>
</name>
</person-group>
<article-title>Multi-mode medical image fusion algorithm based on principal component analysis</article-title>
<issue-title>2009 International Symposium on Computer Network and Multimedia Technology</issue-title>
<source/>IEEE
          <year>2009</year>
<fpage>1</fpage>
<lpage>4</lpage>
</element-citation>
</ref>
<ref id="bib0235">
<label>235</label>
<mixed-citation id="sbref0235" publication-type="other">Krishn, A., et al., Medical Image Fusion Using Combination of PCA and Wavelet Analysis.</mixed-citation>
</ref>
<ref id="bib0236">
<label>236</label>
<element-citation id="sbref0236" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>EGGDD: an explicit dependency model for multi-modal medical image fusion in shift-invariant shearlet transform domain</article-title>
<source/>Inf. Fusion
          <volume>19</volume>
<year>2014</year>
<fpage>29</fpage>
<lpage>37</lpage>
</element-citation>
</ref>
<ref id="bib0237">
<label>237</label>
<element-citation id="sbref0237" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<chapter-title>Image fusion using the expectation-maximization algorithm and a hidden Markov model</chapter-title>
<source/>IEEE 60th Vehicular Technology Conference
          <year>2004</year>
<publisher-name>IEEE</publisher-name>
<fpage>4563</fpage>
<lpage>4567</lpage>
<comment>VTC2004-Fall. 2004. 2004</comment>
</element-citation>
</ref>
<ref id="bib0238">
<label>238</label>
<element-citation id="sbref0238" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Contourlet hidden Markov Tree and clarity-saliency driven PCNN based remote sensing images fusion</article-title>
<source/>Appl. Soft Comput.
          <volume>12</volume>
<issue>1</issue>
<year>2012</year>
<fpage>228</fpage>
<lpage>237</lpage>
</element-citation>
</ref>
<ref id="bib0239">
<label>239</label>
<element-citation id="sbref0239" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bhatnagar</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Human visual system inspired multi-modal medical image fusion framework</article-title>
<source/>Expert Syst. Appl.
          <volume>40</volume>
<issue>5</issue>
<year>2013</year>
<fpage>1708</fpage>
<lpage>1720</lpage>
</element-citation>
</ref>
<ref id="bib0240">
<label>240</label>
<element-citation id="sbref0240" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Daneshvar</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>MRI and PET image fusion by combining IHS and retina-inspired models</article-title>
<source/>Inf. Fusion
          <volume>11</volume>
<issue>2</issue>
<year>2010</year>
<fpage>114</fpage>
<lpage>123</lpage>
</element-citation>
</ref>
<ref id="bib0241">
<label>241</label>
<element-citation id="sbref0241" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jang</surname>
<given-names>J.H.</given-names>
</name>
</person-group>
<article-title>Contrast-enhanced fusion of multisensor images using subband-decomposed multiscale retinex</article-title>
<source/>IEEE Trans. Image Process.
          <volume>21</volume>
<issue>8</issue>
<year>2012</year>
<fpage>3479</fpage>
<lpage>3490</lpage>
<pub-id pub-id-type="pmid">22562762</pub-id>
</element-citation>
</ref>
<ref id="bib0242">
<label>242</label>
<element-citation id="sbref0242" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>S.M.</given-names>
</name>
</person-group>
<article-title>SUSAN—a new approach to low level image processing</article-title>
<source/>Int. J. Comput. Vision
          <volume>23</volume>
<issue>1</issue>
<year>1997</year>
<fpage>45</fpage>
<lpage>78</lpage>
</element-citation>
</ref>
<ref id="bib0243">
<label>243</label>
<element-citation id="sbref0243" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>He</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Multimodal medical image fusion based on IHS and PCA</article-title>
<source/>Proc. Eng.
          <volume>7</volume>
<year>2010</year>
<fpage>280</fpage>
<lpage>285</lpage>
</element-citation>
</ref>
<ref id="bib0244">
<label>244</label>
<element-citation id="sbref0244" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zheng</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>A new metric based on extended spatial frequency and its application to DWT based fusion algorithms</article-title>
<source/>Inf. Fusion
          <volume>8</volume>
<issue>2</issue>
<year>2007</year>
<fpage>177</fpage>
<lpage>192</lpage>
</element-citation>
</ref>
<ref id="bib0245">
<label>245</label>
<element-citation id="sbref0245" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Wencang</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<chapter-title>Medical image fusion method based on wavelet multi-resolution and entropy</chapter-title>
<source/>2008 IEEE International Conference on Automation and Logistics
          <year>2008</year>
<publisher-name>IEEE</publisher-name>
<fpage>2329</fpage>
<lpage>2333</lpage>
</element-citation>
</ref>
<ref id="bib0246">
<label>246</label>
<element-citation id="sbref0246" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Garg</surname>
<given-names>S.</given-names>
</name>
</person-group>
<chapter-title>Multilevel medical image fusion using segmented image by level set evolution with region competition</chapter-title>
<source/>2005 IEEE Engineering in Medicine and Biology 27th Annual Conference
          <year>2006</year>
<publisher-name>IEEE</publisher-name>
<fpage>7680</fpage>
<lpage>7683</lpage>
</element-citation>
</ref>
<ref id="bib0247">
<label>247</label>
<element-citation id="sbref0247" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>X.</given-names>
</name>
</person-group>
<chapter-title>Medical image fusion by multi-resolution analysis of wavelets transform</chapter-title>
<source/>Wavelet Analysis and Applications
          <year>2006</year>
<publisher-name>Springer</publisher-name>
<fpage>389</fpage>
<lpage>396</lpage>
</element-citation>
</ref>
<ref id="bib0248">
<label>248</label>
<element-citation id="sbref0248" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bhatnagar</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Directive contrast based multimodal medical image fusion in NSCT domain</article-title>
<source/>IEEE Trans. Multimed.
          <volume>15</volume>
<issue>5</issue>
<year>2013</year>
<fpage>1014</fpage>
<lpage>1024</lpage>
</element-citation>
</ref>
<ref id="bib0249">
<label>249</label>
<element-citation id="sbref0249" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhu</surname>
<given-names>X.L.</given-names>
</name>
</person-group>
<article-title>Investigation of remote sensing image fusion strategy applying PCA to wavelet packet analysis based on IHS transform</article-title>
<source/>J. Indian Soc. Remote Sens.
          <volume>47</volume>
<issue>3</issue>
<year>2019</year>
<fpage>413</fpage>
<lpage>425</lpage>
</element-citation>
</ref>
<ref id="bib0250">
<label>250</label>
<element-citation id="sbref0250" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Deepa</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>An intensity factorized thresholding based segmentation technique with gradient discrete wavelet fusion for diagnosing stroke and tumor in brain MRI</article-title>
<source/>Multidimen. Syst. Signal Process.
          <volume>30</volume>
<issue>4</issue>
<year>2019</year>
<fpage>2081</fpage>
<lpage>2112</lpage>
</element-citation>
</ref>
<ref id="bib0251">
<label>251</label>
<element-citation id="sbref0251" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Phillips</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Detection of Alzheimer's disease and mild cognitive impairment based on structural volumetric MR images using 3D-DWT and WTA-KSVM trained by PSOTVAC</article-title>
<source/>Biomed. Signal Process. Control
          <volume>21</volume>
<year>2015</year>
<fpage>58</fpage>
<lpage>73</lpage>
</element-citation>
</ref>
<ref id="bib0252">
<label>252</label>
<element-citation id="sbref0252" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Prakash</surname>
<given-names>O.</given-names>
</name>
</person-group>
<chapter-title>CT and MR images fusion based on stationary wavelet transform by modulus maxima</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Sethi</surname>
<given-names>I.K.</given-names>
</name>
</person-group>
<source/>Computational Vision and Robotics
          <year>2015</year>
<publisher-name>Springer-Verlag</publisher-name>
<publisher-loc>Berlin: Berlin</publisher-loc>
<fpage>199</fpage>
<lpage>204</lpage>
</element-citation>
</ref>
<ref id="bib0253">
<label>253</label>
<element-citation id="sbref0253" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Pawar</surname>
<given-names>G.A.</given-names>
</name>
</person-group>
<chapter-title>Multi-focal image fusion with convolutional sparse representation and stationary wavelet transform</chapter-title>
<source/>Computing, Communication and Signal Processing (ICCASP)
          <year>2019</year>
<publisher-name>Springer International Publishing Ag</publisher-name>
<publisher-loc>Cham</publisher-loc>
<fpage>865</fpage>
<lpage>873</lpage>
</element-citation>
</ref>
<ref id="bib0254">
<label>254</label>
<element-citation id="sbref0254" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Detection of dendritic spines using wavelet packet entropy and fuzzy support vector machine</article-title>
<source/>CNS Neurol. Disord.
          <volume>16</volume>
<issue>2</issue>
<year>2017</year>
<fpage>116</fpage>
<lpage>121</lpage>
</element-citation>
</ref>
<ref id="bib0255">
<label>255</label>
<element-citation id="sbref0255" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Preclinical diagnosis of magnetic resonance (MR) brain images via discrete wavelet packet transform with Tsallis entropy and generalized eigenvalue proximal support vector machine (GEPSVM)</article-title>
<source/>Entropy
          <volume>17</volume>
<issue>4</issue>
<year>2015</year>
<fpage>1795</fpage>
<lpage>1813</lpage>
</element-citation>
</ref>
<ref id="bib0256">
<label>256</label>
<element-citation id="sbref0256" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Sreekala</surname>
<given-names>K.</given-names>
</name>
</person-group>
<chapter-title>Wavelet packet transform based fusion of misaligned images</chapter-title>
<source/>International Conference on Circuit, Power and Computing Technologies
          <year>2016</year>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Karnataka, India</publisher-loc>
</element-citation>
</ref>
<ref id="bib0257">
<label>257</label>
<element-citation id="sbref0257" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shah</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Fusion of surveillance images in infrared and visible band using curvelet, wavelet and wavelet packet transform</article-title>
<source/>Int. J. Wavel. Multiresol. Inf. Process.
          <volume>8</volume>
<issue>2</issue>
<year>2010</year>
<fpage>271</fpage>
<lpage>292</lpage>
</element-citation>
</ref>
<ref id="bib0258">
<label>258</label>
<element-citation id="sbref0258" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Choubey</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Novel data-access scheme and efficient parallel architecture for multi-level lifting 2-D DWT</article-title>
<source/>Circ. Syst. Signal Process.
          <volume>37</volume>
<issue>10</issue>
<year>2018</year>
<fpage>4482</fpage>
<lpage>4503</lpage>
</element-citation>
</ref>
<ref id="bib0259">
<label>259</label>
<element-citation id="sbref0259" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shiralashetti</surname>
<given-names>S.C.</given-names>
</name>
</person-group>
<article-title>Wavelet-based lifting scheme for the numerical solution of some class of nonlinear partial differential equations</article-title>
<source/>Int. J. Wavel. Multiresol. Inf. Process.
          <volume>16</volume>
<issue>5</issue>
<year>2018</year>
<fpage>14</fpage>
<comment>1850046</comment>
</element-citation>
</ref>
<ref id="bib0260">
<label>260</label>
<element-citation id="sbref0260" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Prakash</surname>
<given-names>O.</given-names>
</name>
</person-group>
<article-title>Multiscale fusion of multimodal medical images using lifting scheme based biorthogonal wavelet transform</article-title>
<source/>Optik
          <volume>182</volume>
<year>2019</year>
<fpage>995</fpage>
<lpage>1014</lpage>
</element-citation>
</ref>
<ref id="bib0261">
<label>261</label>
<element-citation id="sbref0261" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Haouam</surname>
<given-names>I.</given-names>
</name>
</person-group>
<chapter-title>MRI image compression using level set method and biorthogonal CDF wavelet based on lifting scheme</chapter-title>
<source/>International Conference on Signal, Image, Vision and Their Applications
          <year>2018</year>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Guelma, Algeria</publisher-loc>
</element-citation>
</ref>
<ref id="bib0262">
<label>262</label>
<element-citation id="sbref0262" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zemouri</surname>
<given-names>E.T.</given-names>
</name>
</person-group>
<article-title>Nonsubsampled contourlet transform and k-means clustering for degraded document image binarization</article-title>
<source/>J. Electron. Imaging
          <volume>28</volume>
<issue>4</issue>
<year>2019</year>
<fpage>19</fpage>
<comment>Article ID. 043021</comment>
</element-citation>
</ref>
<ref id="bib0263">
<label>263</label>
<element-citation id="sbref0263" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ramlal</surname>
<given-names>S.D.</given-names>
</name>
</person-group>
<article-title>An improved multimodal medical image fusion scheme based on hybrid combination of nonsubsampled contourlet transform and stationary wavelet transform</article-title>
<source/>Int. J. Imaging Syst. Technol.
          <volume>29</volume>
<issue>2</issue>
<year>2019</year>
<fpage>146</fpage>
<lpage>160</lpage>
</element-citation>
</ref>
<ref id="bib0264">
<label>264</label>
<element-citation id="sbref0264" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>L.L.</given-names>
</name>
</person-group>
<article-title>A practical medical image enhancement algorithm based on nonsubsampled contourlet transform</article-title>
<source/>J. Med. Imaging Health Inform.
          <volume>9</volume>
<issue>5</issue>
<year>2019</year>
<fpage>1046</fpage>
<lpage>1056</lpage>
</element-citation>
</ref>
<ref id="bib0265">
<label>265</label>
<element-citation id="sbref0265" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>Multi-modality anatomical and functional medical image fusion based on simplified-spatial frequency-pulse coupled neural networks and region energy-weighted average strategy in non-sub sampled contourlet transform domain</article-title>
<source/>J. Med. Imaging Health Inform.
          <volume>9</volume>
<issue>5</issue>
<year>2019</year>
<fpage>1017</fpage>
<lpage>1027</lpage>
</element-citation>
</ref>
<ref id="bib0266">
<label>266</label>
<element-citation id="sbref0266" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>L.L.</given-names>
</name>
</person-group>
<article-title>A novel medical image fusion approach based on nonsubsampled shearlet transform</article-title>
<source/>J. Med. Imaging Health Inform.
          <volume>9</volume>
<issue>9</issue>
<year>2019</year>
<fpage>1815</fpage>
<lpage>1826</lpage>
</element-citation>
</ref>
<ref id="bib0267">
<label>267</label>
<element-citation id="sbref0267" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vishwakarma</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Image fusion using adjustable non-subsampled shearlet transform</article-title>
<source/>IEEE Trans. Instrum. Meas.
          <volume>68</volume>
<issue>9</issue>
<year>2019</year>
<fpage>3367</fpage>
<lpage>3378</lpage>
</element-citation>
</ref>
<ref id="bib0268">
<label>268</label>
<element-citation id="sbref0268" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Akbarpour</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Medical image fusion based on nonsubsampled shearlet transform and principal component averaging</article-title>
<source/>Int. J. Wavel. Multiresol. Inf. Process.
          <volume>17</volume>
<issue>4</issue>
<year>2019</year>
<fpage>21</fpage>
<comment>Article ID. 1950023</comment>
</element-citation>
</ref>
<ref id="bib0269">
<label>269</label>
<element-citation id="sbref0269" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Pixel-level image fusion with simultaneous orthogonal matching pursuit</article-title>
<source/>Inf. Fusion
          <volume>13</volume>
<issue>1</issue>
<year>2012</year>
<fpage>10</fpage>
<lpage>19</lpage>
</element-citation>
</ref>
<ref id="bib0270">
<label>270</label>
<element-citation id="sbref0270" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Multimodal image fusion with joint sparsity model</article-title>
<source/>Opt. Eng.
          <volume>50</volume>
<issue>6</issue>
<year>2011</year>
<object-id pub-id-type="publisher-id">067007</object-id>
</element-citation>
</ref>
<ref id="bib0271">
<label>271</label>
<element-citation id="sbref0271" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yu</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Image features extraction and fusion based on joint sparse representation</article-title>
<source/>IEEE J. Sel. Top. Signal Process.
          <volume>5</volume>
<issue>5</issue>
<year>2011</year>
<fpage>1074</fpage>
<lpage>1082</lpage>
</element-citation>
</ref>
<ref id="bib0272">
<label>272</label>
<element-citation id="sbref0272" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xu</surname>
<given-names>Z.P.</given-names>
</name>
</person-group>
<article-title>Medical image fusion using multi-level local extrema</article-title>
<source/>Inf. Fusion
          <volume>19</volume>
<year>2014</year>
<fpage>38</fpage>
<lpage>48</lpage>
</element-citation>
</ref>
<ref id="bib0273">
<label>273</label>
<element-citation id="sbref0273" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhu</surname>
<given-names>H.R.</given-names>
</name>
</person-group>
<article-title>Infrared and visible image fusion based on contrast enhancement and multi-scale edge-preserving decomposition</article-title>
<source/>J. Electron. Inf. Technol.
          <volume>40</volume>
<issue>6</issue>
<year>2018</year>
<fpage>1294</fpage>
<lpage>1300</lpage>
</element-citation>
</ref>
<ref id="bib0274">
<label>274</label>
<element-citation id="sbref0274" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kou</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Edge-preserving smoothing pyramid based multi-scale exposure fusion</article-title>
<source/>J. Visual Commun. Image Represent.
          <volume>53</volume>
<year>2018</year>
<fpage>235</fpage>
<lpage>244</lpage>
</element-citation>
</ref>
<ref id="bib0275">
<label>275</label>
<element-citation id="sbref0275" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Petrović</surname>
<given-names>V.</given-names>
</name>
</person-group>
<article-title>Subjective tests for image fusion evaluation and objective metric validation</article-title>
<source/>Inf. Fusion
          <volume>8</volume>
<issue>2</issue>
<year>2007</year>
<fpage>208</fpage>
<lpage>216</lpage>
</element-citation>
</ref>
<ref id="bib0276">
<label>276</label>
<element-citation id="sbref0276" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sheikh</surname>
<given-names>H.R.</given-names>
</name>
</person-group>
<article-title>Image information and visual quality</article-title>
<source/>IEEE Trans. Image Process.
          <volume>15</volume>
<issue>2</issue>
<year>2006</year>
<fpage>430</fpage>
<lpage>444</lpage>
<pub-id pub-id-type="pmid">16479813</pub-id>
</element-citation>
</ref>
<ref id="bib0277">
<label>277</label>
<element-citation id="sbref0277" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>User models of subjective image quality assessment on virtual viewpoint in free-viewpoint video system</article-title>
<source/>Multimed. Tools Appl.
          <volume>75</volume>
<issue>20</issue>
<year>2016</year>
<fpage>12499</fpage>
<lpage>12519</lpage>
</element-citation>
</ref>
<ref id="bib0278">
<label>278</label>
<element-citation id="sbref0278" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Du</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>An overview of multi-modal medical image fusion</article-title>
<source/>Neurocomputing
          <volume>215</volume>
<year>2016</year>
<fpage>3</fpage>
<lpage>20</lpage>
</element-citation>
</ref>
<ref id="bib0279">
<label>279</label>
<element-citation id="sbref0279" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Du</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>An overview of multi-modal medical image fusion</article-title>
<source/>Neurocomputing
          <volume>215</volume>
<year>2016</year>
<fpage>3</fpage>
<lpage>20</lpage>
</element-citation>
</ref>
<ref id="bib0280">
<label>280</label>
<element-citation id="sbref0280" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Contourlet-based image quality assessment for synthesised virtual image</article-title>
<source/>Electron. Lett.
          <volume>46</volume>
<issue>7</issue>
<year>2010</year>
<fpage>492</fpage>
<lpage>493</lpage>
</element-citation>
</ref>
<ref id="bib0281">
<label>281</label>
<element-citation id="sbref0281" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<article-title>Image quality assessment: from error visibility to structural similarity</article-title>
<source/>IEEE Trans. Image Process.
          <volume>13</volume>
<issue>4</issue>
<year>2004</year>
<fpage>600</fpage>
<lpage>612</lpage>
<pub-id pub-id-type="pmid">15376593</pub-id>
</element-citation>
</ref>
<ref id="bib0282">
<label>282</label>
<element-citation id="sbref0282" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Miao</surname>
<given-names>Q.G.</given-names>
</name>
</person-group>
<article-title>A novel algorithm of image fusion using shearlets</article-title>
<source/>Opt. Commun.
          <volume>284</volume>
<issue>6</issue>
<year>2011</year>
<fpage>1540</fpage>
<lpage>1547</lpage>
</element-citation>
</ref>
<ref id="bib0283">
<label>283</label>
<element-citation id="sbref0283" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hossny</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Comments on 'Information measure for performance of image fusion'</article-title>
<source/>Electron. Lett.
          <volume>44</volume>
<issue>18</issue>
<year>2008</year>
<fpage>1066</fpage>
<lpage>1067</lpage>
</element-citation>
</ref>
<ref id="bib0284">
<label>284</label>
<element-citation id="sbref0284" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Horibe</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Entropy and correlation</article-title>
<source/>IEEE Trans. Syst. Man Cybern.
          <volume>SMC-15</volume>
<issue>5</issue>
<year>1985</year>
<fpage>641</fpage>
<lpage>642</lpage>
</element-citation>
</ref>
<ref id="bib0285">
<label>285</label>
<element-citation id="sbref0285" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eskicioglu</surname>
<given-names>A.M.</given-names>
</name>
</person-group>
<article-title>Image quality measures and their performance</article-title>
<source/>IEEE Trans. Commun.
          <volume>43</volume>
<issue>12</issue>
<year>1995</year>
<fpage>2959</fpage>
<lpage>2965</lpage>
</element-citation>
</ref>
<ref id="bib0286">
<label>286</label>
<element-citation id="sbref0286" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mittal</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Making a "completely blind" image quality analyzer</article-title>
<source/>IEEE Signal Process. Lett..
          <volume>20</volume>
<issue>3</issue>
<year>2013</year>
<fpage>209</fpage>
<lpage>212</lpage>
</element-citation>
</ref>
<ref id="bib0287">
<label>287</label>
<element-citation id="sbref0287" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Herzog</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>The current state, challenges and perspectives of MR-PET</article-title>
<source/>Neuroimage
          <volume>49</volume>
<issue>3</issue>
<year>2010</year>
<fpage>2072</fpage>
<lpage>2208</lpage>
<pub-id pub-id-type="pmid">19853045</pub-id>
</element-citation>
</ref>
<ref id="bib0288">
<label>288</label>
<element-citation id="sbref0288" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schlemmer</surname>
<given-names>H.-P.W.</given-names>
</name>
</person-group>
<article-title>Simultaneous MR/PET imaging of the human brain: feasibility study</article-title>
<source/>Radiology
          <volume>248</volume>
<issue>3</issue>
<year>2008</year>
<fpage>1028</fpage>
<lpage>1035</lpage>
<pub-id pub-id-type="pmid">18710991</pub-id>
</element-citation>
</ref>
<ref id="bib0289">
<label>289</label>
<element-citation id="sbref0289" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Grazioso</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>APD-based PET for combined MR-PET imaging</article-title>
<source/>Proc. Intl. Soc. Mag. Reson. Med
          <year>2005</year>
<fpage>408</fpage>
</element-citation>
</ref>
<ref id="bib0290">
<label>290</label>
<element-citation id="sbref0290" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hamilton</surname>
<given-names>B.E.</given-names>
</name>
</person-group>
<article-title>Comparative analysis of ferumoxytol and gadoteridol enhancement using T1-and T2-weighted MRI in neuroimaging</article-title>
<source/>Am. J. Roentgenol.
          <volume>197</volume>
<issue>4</issue>
<year>2011</year>
<fpage>981</fpage>
<lpage>988</lpage>
<pub-id pub-id-type="pmid">21940589</pub-id>
</element-citation>
</ref>
<ref id="bib0291">
<label>291</label>
<element-citation id="sbref0291" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Just</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Tissue characterization with T1, T2, and proton density values: results in 160 patients with brain tumors</article-title>
<source/>Radiology
          <volume>169</volume>
<issue>3</issue>
<year>1988</year>
<fpage>779</fpage>
<lpage>785</lpage>
<pub-id pub-id-type="pmid">3187000</pub-id>
</element-citation>
</ref>
<ref id="bib0292">
<label>292</label>
<element-citation id="sbref0292" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xie</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Alcoholism identification based on an AlexNet transfer learning model</article-title>
<source/>Front.Psychiatry
          <volume>10</volume>
<year>2019</year>
<comment>Article ID. 205</comment>
</element-citation>
</ref>
<ref id="bib0293">
<label>293</label>
<element-citation id="sbref0293" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dawood</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Novel imaging techniques to study postmortem human fetal anatomy: a systematic review on microfocus-CT and ultra-high-field MRI</article-title>
<source/>Eur. Radiol.
          <volume>30</volume>
<issue>4</issue>
<year>2020</year>
<fpage>2280</fpage>
<lpage>2292</lpage>
<pub-id pub-id-type="pmid">31834508</pub-id>
</element-citation>
</ref>
<ref id="bib0294">
<label>294</label>
<element-citation id="sbref0294" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tuzzi</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Ultra-high field mri in Alzheimer's disease: effective transverse relaxation rate and quantitative susceptibility mapping of human brain in vivo and ex vivo compared to histology</article-title>
<source/>J. Alzheimers Dis.
          <volume>73</volume>
<issue>4</issue>
<year>2020</year>
<fpage>1481</fpage>
<lpage>1499</lpage>
<pub-id pub-id-type="pmid">31958079</pub-id>
</element-citation>
</ref>
<ref id="bib0295">
<label>295</label>
<element-citation id="sbref0295" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Buxton</surname>
<given-names>R.B.</given-names>
</name>
</person-group>
<chapter-title>Introduction to Functional Magnetic Resonance Imaging: Principles and Techniques</chapter-title>
<year>2009</year>
<publisher-name>Cambridge University Press</publisher-name>
</element-citation>
</ref>
<ref id="bib0296">
<label>296</label>
<element-citation id="sbref0296" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rosenkranz</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Present and future of simultaneous EEG-fMRI</article-title>
<source/>Magn. Reson. Mater. Phys. Biol. Med,
          <volume>23</volume>
<issue>5</issue>
<year>2010</year>
<fpage>309</fpage>
<lpage>316</lpage>
</element-citation>
</ref>
<ref id="bib0297">
<label>297</label>
<element-citation id="sbref0297" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laufs</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>A personalized history of EEG–fMRI integration</article-title>
<source/>Neuroimage
          <volume>62</volume>
<issue>2</issue>
<year>2012</year>
<fpage>1056</fpage>
<lpage>1067</lpage>
<pub-id pub-id-type="pmid">22266176</pub-id>
</element-citation>
</ref>
<ref id="bib0298">
<label>298</label>
<element-citation id="sbref0298" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Medič</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Off-resonance frequency filtered magnetic resonance imaging</article-title>
<source/>Magn. Reson. Imaging
          <volume>28</volume>
<issue>4</issue>
<year>2010</year>
<fpage>527</fpage>
<lpage>536</lpage>
<pub-id pub-id-type="pmid">20129750</pub-id>
</element-citation>
</ref>
<ref id="bib0299">
<label>299</label>
<element-citation id="sbref0299" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Hellier</surname>
<given-names>P.</given-names>
</name>
</person-group>
<chapter-title>Multimodal non-rigid warping for correction of distortions in functional MRI</chapter-title>
<source/>International Conference on Medical Image Computing and Computer-Assisted Intervention
          <year>2000</year>
<publisher-name>Springer</publisher-name>
<fpage>512</fpage>
<lpage>520</lpage>
</element-citation>
</ref>
<ref id="bib0300">
<label>300</label>
<element-citation id="sbref0300" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Holland</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Efficient correction of inhomogeneous static magnetic field-induced distortion in Echo Planar Imaging</article-title>
<source/>Neuroimage
          <volume>50</volume>
<issue>1</issue>
<year>2010</year>
<fpage>175</fpage>
<lpage>183</lpage>
<pub-id pub-id-type="pmid">19944768</pub-id>
</element-citation>
</ref>
<ref id="bib0301">
<label>301</label>
<element-citation id="sbref0301" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>Z.</given-names>
</name>
</person-group>
<article-title>From simultaneous to synergistic MR‐PET brain imaging: a review of hybrid MR‐PET imaging methodologies</article-title>
<source/>Hum. Brain Mapp.
          <volume>39</volume>
<issue>12</issue>
<year>2018</year>
<fpage>5126</fpage>
<lpage>5144</lpage>
<pub-id pub-id-type="pmid">30076750</pub-id>
</element-citation>
</ref>
<ref id="bib0302">
<label>302</label>
<element-citation id="sbref0302" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ullisch</surname>
<given-names>M.G.</given-names>
</name>
</person-group>
<article-title>MR-based PET motion correction procedure for simultaneous MR-PET neuroimaging of human brain</article-title>
<source/>PLoS ONE
          <volume>7</volume>
<issue>11</issue>
<year>2012</year>
<fpage>e48149</fpage>
<pub-id pub-id-type="pmid">23189127</pub-id>
</element-citation>
</ref>
<ref id="bib0303">
<label>303</label>
<element-citation id="sbref0303" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ehrhardt</surname>
<given-names>M.J.</given-names>
</name>
</person-group>
<article-title>Joint reconstruction of PET-MRI by exploiting structural similarity</article-title>
<source/>Inverse Problems
          <volume>31</volume>
<issue>1</issue>
<year>2014</year>
<object-id pub-id-type="publisher-id">015001</object-id>
</element-citation>
</ref>
<ref id="bib0304">
<label>304</label>
<element-citation id="sbref0304" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Iglesias</surname>
<given-names>J.E.</given-names>
</name>
</person-group>
<article-title>Multi-atlas segmentation of biomedical images: a survey</article-title>
<source/>Med. Image Anal.
          <volume>24</volume>
<issue>1</issue>
<year>2015</year>
<fpage>205</fpage>
<lpage>219</lpage>
<pub-id pub-id-type="pmid">26201875</pub-id>
</element-citation>
</ref>
<ref id="bib0305">
<label>305</label>
<element-citation id="sbref0305" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Evans</surname>
<given-names>A.C.</given-names>
</name>
</person-group>
<article-title>Brain templates and atlases</article-title>
<source/>Neuroimage
          <volume>62</volume>
<issue>2</issue>
<year>2012</year>
<fpage>911</fpage>
<lpage>922</lpage>
<pub-id pub-id-type="pmid">22248580</pub-id>
</element-citation>
</ref>
<ref id="bib0306">
<label>306</label>
<element-citation id="sbref0306" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Talairach</surname>
<given-names>J.</given-names>
</name>
</person-group>
<chapter-title>Atlas d'anatomie Stereotaxique du Telencephale: Etudes Anatomo-Radiologiques</chapter-title>
<year>1957</year>
<publisher-name>Masson</publisher-name>
</element-citation>
</ref>
<ref id="bib0307">
<label>307</label>
<element-citation id="sbref0307" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Talairach</surname>
<given-names>J.</given-names>
</name>
<collab>Tournoux</collab>
</person-group>
<chapter-title>Co-planar stereotaxic atlas of the human brain</chapter-title>
<source/>3-Dimensional Proportional System: An Approach to Cerebral Imaging
          <year>1988</year>
<comment>1988</comment>
</element-citation>
</ref>
<ref id="bib0308">
<label>308</label>
<element-citation id="sbref0308" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tzourio-Mazoyer</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>
<source/>Neuroimage
          <volume>15</volume>
<issue>1</issue>
<year>2002</year>
<fpage>273</fpage>
<lpage>289</lpage>
<pub-id pub-id-type="pmid">11771995</pub-id>
</element-citation>
</ref>
<ref id="bib0309">
<label>309</label>
<element-citation id="sbref0309" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Collins</surname>
<given-names>D.L.</given-names>
</name>
</person-group>
<article-title>Design and construction of a realistic digital brain phantom</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>17</volume>
<issue>3</issue>
<year>1998</year>
<fpage>463</fpage>
<lpage>468</lpage>
<pub-id pub-id-type="pmid">9735909</pub-id>
</element-citation>
</ref>
<ref id="bib0310">
<label>310</label>
<element-citation id="sbref0310" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Heckemann</surname>
<given-names>R.A.</given-names>
</name>
</person-group>
<article-title>Automatic anatomical brain MRI segmentation combining label propagation and decision fusion</article-title>
<source/>Neuroimage
          <volume>33</volume>
<issue>1</issue>
<year>2006</year>
<fpage>115</fpage>
<lpage>126</lpage>
<pub-id pub-id-type="pmid">16860573</pub-id>
</element-citation>
</ref>
<ref id="bib0311">
<label>311</label>
<element-citation id="sbref0311" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>Y.Y.</given-names>
</name>
</person-group>
<article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>20</volume>
<issue>1</issue>
<year>2001</year>
<fpage>45</fpage>
<lpage>57</lpage>
<pub-id pub-id-type="pmid">11293691</pub-id>
</element-citation>
</ref>
<ref id="bib0312">
<label>312</label>
<element-citation id="sbref0312" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ashburner</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Unified segmentation</article-title>
<source/>Neuroimage
          <volume>26</volume>
<issue>3</issue>
<year>2005</year>
<fpage>839</fpage>
<lpage>851</lpage>
<pub-id pub-id-type="pmid">15955494</pub-id>
</element-citation>
</ref>
<ref id="bib0313">
<label>313</label>
<element-citation id="sbref0313" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cabezas</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>A review of atlas-based segmentation for magnetic resonance brain images</article-title>
<source/>Comput. Methods Programs Biomed.
          <volume>104</volume>
<issue>3</issue>
<year>2011</year>
<fpage>E158</fpage>
<lpage>E177</lpage>
<pub-id pub-id-type="pmid">21871688</pub-id>
</element-citation>
</ref>
<ref id="bib0314">
<label>314</label>
<element-citation id="sbref0314" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ashburner</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Voxel-based morphometry - the methods</article-title>
<source/>Neuroimage
          <volume>11</volume>
<issue>6</issue>
<year>2000</year>
<fpage>805</fpage>
<lpage>821</lpage>
<pub-id pub-id-type="pmid">10860804</pub-id>
</element-citation>
</ref>
<ref id="bib0315">
<label>315</label>
<element-citation id="sbref0315" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hill</surname>
<given-names>D.L.G.</given-names>
</name>
</person-group>
<article-title>Medical image registration</article-title>
<source/>Phys. Med. Biol.
          <volume>46</volume>
<issue>3</issue>
<year>2001</year>
<fpage>R1</fpage>
<lpage>R45</lpage>
<pub-id pub-id-type="pmid">11277237</pub-id>
</element-citation>
</ref>
<ref id="bib0316">
<label>316</label>
<element-citation id="sbref0316" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rohlfing</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Evaluation of atlas selection strategies for atlas-based image segmentation with application to confocal microscopy images of bee brains</article-title>
<source/>Neuroimage
          <volume>21</volume>
<issue>4</issue>
<year>2004</year>
<fpage>1428</fpage>
<lpage>1442</lpage>
<pub-id pub-id-type="pmid">15050568</pub-id>
</element-citation>
</ref>
<ref id="bib0317">
<label>317</label>
<element-citation id="sbref0317" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Klein</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Mindboggle: automated brain labeling with multiple atlases</article-title>
<source/>BMC Med. Imaging
          <volume>5</volume>
<year>2005</year>
<fpage>7</fpage>
<pub-id pub-id-type="pmid">16202176</pub-id>
</element-citation>
</ref>
<ref id="bib0318">
<label>318</label>
<element-citation id="sbref0318" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Artaechevarria</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Combination strategies in multi-atlas image segmentation: application to brain MR Data</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>28</volume>
<issue>8</issue>
<year>2009</year>
<fpage>1266</fpage>
<lpage>1277</lpage>
<pub-id pub-id-type="pmid">19228554</pub-id>
</element-citation>
</ref>
<ref id="bib0319">
<label>319</label>
<element-citation id="sbref0319" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rohlfing</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Performance-based classifier combination in atlas-based image segmentation using expectation-maximization parameter estimation</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>23</volume>
<issue>8</issue>
<year>2004</year>
<fpage>983</fpage>
<lpage>994</lpage>
<pub-id pub-id-type="pmid">15338732</pub-id>
</element-citation>
</ref>
<ref id="bib0320">
<label>320</label>
<element-citation id="sbref0320" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rohlfing</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Multi-classifier framework for atlas-based image segmentation</article-title>
<source/>Pattern Recognit. Lett.
          <volume>26</volume>
<issue>13</issue>
<year>2005</year>
<fpage>2070</fpage>
<lpage>2079</lpage>
</element-citation>
</ref>
<ref id="bib0321">
<label>321</label>
<element-citation id="sbref0321" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Maes</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Multimodality image registration by maximization of mutual information</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>16</volume>
<issue>2</issue>
<year>1997</year>
<fpage>187</fpage>
<lpage>198</lpage>
<pub-id pub-id-type="pmid">9101328</pub-id>
</element-citation>
</ref>
<ref id="bib0322">
<label>322</label>
<element-citation id="sbref0322" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sdika</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Combining atlas based segmentation and intensity classification with nearest neighbor transform and accuracy weighted vote</article-title>
<source/>Med. Image Anal.
          <volume>14</volume>
<issue>2</issue>
<year>2010</year>
<fpage>219</fpage>
<lpage>226</lpage>
<pub-id pub-id-type="pmid">20056473</pub-id>
</element-citation>
</ref>
<ref id="bib0323">
<label>323</label>
<element-citation id="sbref0323" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gholipour</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Multi-atlas multi-shape segmentation of fetal brain MRI for volumetric and morphometric analysis of ventriculomegaly</article-title>
<source/>Neuroimage
          <volume>60</volume>
<issue>3</issue>
<year>2012</year>
<fpage>1819</fpage>
<lpage>1831</lpage>
<pub-id pub-id-type="pmid">22500924</pub-id>
</element-citation>
</ref>
<ref id="bib0324">
<label>324</label>
<element-citation id="sbref0324" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gorthi</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Weighted shape-based averaging with neighborhood prior model for multiple atlas fusion-based medical image segmentation</article-title>
<source/>IEEE Signal Process Lett.
          <volume>20</volume>
<issue>11</issue>
<year>2013</year>
<fpage>1036</fpage>
<lpage>1039</lpage>
</element-citation>
</ref>
<ref id="bib0325">
<label>325</label>
<element-citation id="sbref0325" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Garcia-Pedrajas</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>An empirical study of binary classifier fusion methods for multiclass classification</article-title>
<source/>Inf. Fusion
          <volume>12</volume>
<issue>2</issue>
<year>2011</year>
<fpage>111</fpage>
<lpage>130</lpage>
</element-citation>
</ref>
<ref id="bib0326">
<label>326</label>
<element-citation id="sbref0326" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nweke</surname>
<given-names>H.F.</given-names>
</name>
</person-group>
<article-title>Data fusion and multiple classifier systems for human activity detection and health monitoring: review and open research directions</article-title>
<source/>Inf. Fusion
          <volume>46</volume>
<year>2019</year>
<fpage>147</fpage>
<lpage>170</lpage>
</element-citation>
</ref>
<ref id="bib0327">
<label>327</label>
<element-citation id="sbref0327" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yilmaz</surname>
<given-names>M.B.</given-names>
</name>
</person-group>
<article-title>Score level fusion of classifiers in off-line signature verification</article-title>
<source/>Inf. Fusion
          <volume>32</volume>
<year>2016</year>
<fpage>109</fpage>
<lpage>119</lpage>
</element-citation>
</ref>
<ref id="bib0328">
<label>328</label>
<element-citation id="sbref0328" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Viswanath</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Fusion of multiple approximate nearest neighbor classifiers for fast and efficient classification</article-title>
<source/>Inf. Fusion
          <volume>5</volume>
<issue>4</issue>
<year>2004</year>
<fpage>239</fpage>
<lpage>250</lpage>
</element-citation>
</ref>
<ref id="bib0329">
<label>329</label>
<element-citation id="sbref0329" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Castillo-Barnes</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Robust ensemble classification methodology for I123-Ioflupane SPECT images and multiple heterogeneous biomarkers in the diagnosis of Parkinson's disease</article-title>
<source/>Front. Neuroinform.
          <volume>12</volume>
<year>2018</year>
<fpage>16</fpage>
<comment>Article ID. 53</comment>
<pub-id pub-id-type="pmid">29706879</pub-id>
</element-citation>
</ref>
<ref id="bib0330">
<label>330</label>
<element-citation id="sbref0330" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ramirez</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Ensemble of random forests One vs. Rest classifiers for MCI and AD prediction using ANOVA cortical and subcortical feature selection and partial least squares</article-title>
<source/>J. Neurosci. Methods
          <volume>302</volume>
<year>2018</year>
<fpage>47</fpage>
<lpage>57</lpage>
<pub-id pub-id-type="pmid">29242123</pub-id>
</element-citation>
</ref>
<ref id="bib0331">
<label>331</label>
<element-citation id="sbref0331" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lam</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Optimal combinations of pattern classifiers</article-title>
<source/>Pattern Recognit. Lett.
          <volume>16</volume>
<issue>9</issue>
<year>1995</year>
<fpage>945</fpage>
<lpage>954</lpage>
</element-citation>
</ref>
<ref id="bib0332">
<label>332</label>
<element-citation id="sbref0332" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Woods</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Combination of multiple classifiers using local accuracy estimates</article-title>
<source/>IEEE Trans. Pattern Anal. Mach. Intell.
          <volume>19</volume>
<issue>4</issue>
<year>1997</year>
<fpage>405</fpage>
<lpage>410</lpage>
</element-citation>
</ref>
<ref id="bib0333">
<label>333</label>
<element-citation id="sbref0333" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sedvall</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Imaging of neurotransmitter receptors in the living human-brain</article-title>
<source/>Arch. Gen. Psychiatry
          <volume>43</volume>
<issue>10</issue>
<year>1986</year>
<fpage>995</fpage>
<lpage>1005</lpage>
<pub-id pub-id-type="pmid">3019270</pub-id>
</element-citation>
</ref>
<ref id="bib0334">
<label>334</label>
<element-citation id="sbref0334" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shiri</surname>
<given-names>I.</given-names>
</name>
</person-group>
<article-title>Direct attenuation correction of brain PET images using only emission data via a deep convolutional encoder-decoder (Deep-DAC)</article-title>
<source/>Eur. Radiol.
          <volume>29</volume>
<issue>12</issue>
<year>2019</year>
<fpage>6867</fpage>
<lpage>6879</lpage>
<pub-id pub-id-type="pmid">31227879</pub-id>
</element-citation>
</ref>
<ref id="bib0335">
<label>335</label>
<element-citation id="sbref0335" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sarikaya</surname>
<given-names>I.</given-names>
</name>
</person-group>
<article-title>PET studies in epilepsy</article-title>
<source/>Am. J. Nucl. Med. Mol. Imaging
          <volume>5</volume>
<issue>5</issue>
<year>2015</year>
<fpage>416</fpage>
<lpage>430</lpage>
<pub-id pub-id-type="pmid">26550535</pub-id>
</element-citation>
</ref>
<ref id="bib0336">
<label>336</label>
<element-citation id="sbref0336" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jones</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>History and future technical innovation in positron emission tomography</article-title>
<source/>J Med. Imaging
          <volume>4</volume>
<issue>1</issue>
<year>2017</year>
<fpage>17</fpage>
<comment>Article ID. 011013</comment>
</element-citation>
</ref>
<ref id="bib0337">
<label>337</label>
<element-citation id="sbref0337" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Hasegawa</surname>
<given-names>B.H.</given-names>
</name>
</person-group>
<chapter-title>Dual-modality imaging: more than the sum of its components</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>35</fpage>
<lpage>81</lpage>
<comment>Editor.</comment>
</element-citation>
</ref>
<ref id="bib0338">
<label>338</label>
<element-citation id="sbref0338" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lillington</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>PET/MRI attenuation estimation in the lung: a review of past, present, and potential techniques</article-title>
<source/>Med. Phys.
          <volume>47</volume>
<issue>2</issue>
<year>2020</year>
<fpage>790</fpage>
<lpage>811</lpage>
<pub-id pub-id-type="pmid">31794071</pub-id>
</element-citation>
</ref>
<ref id="bib0339">
<label>339</label>
<element-citation id="sbref0339" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<chapter-title>Overview of nuclear medical imaging: physics and instrumentation</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>1</fpage>
<lpage>34</lpage>
<comment>Editor.</comment>
</element-citation>
</ref>
<ref id="bib0340">
<label>340</label>
<element-citation id="sbref0340" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bettinardi</surname>
<given-names>V.</given-names>
</name>
</person-group>
<article-title>PET quantification: strategies for partial volume correction</article-title>
<source/>Clin. Transl. Imaging
          <volume>2</volume>
<issue>3</issue>
<year>2014</year>
<fpage>199</fpage>
<lpage>218</lpage>
</element-citation>
</ref>
<ref id="bib0341">
<label>341</label>
<element-citation id="sbref0341" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dickson</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Quantitative SPECT: the time is now</article-title>
<source/>Ejnmmi Phys.
          <volume>6</volume>
<year>2019</year>
<fpage>7</fpage>
<comment>64</comment>
<pub-id pub-id-type="pmid">31025215</pub-id>
</element-citation>
</ref>
<ref id="bib0342">
<label>342</label>
<element-citation id="sbref0342" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nordberg</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>The use of PET in Alzheimer disease</article-title>
<source/>Nat. Rev. Neurol.
          <volume>6</volume>
<issue>2</issue>
<year>2010</year>
<fpage>78</fpage>
<lpage>87</lpage>
<pub-id pub-id-type="pmid">20139997</pub-id>
</element-citation>
</ref>
<ref id="bib0343">
<label>343</label>
<element-citation id="sbref0343" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Okamura</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>Brain imaging: applications of tau PET imaging</article-title>
<source/>Nat. Rev. Neurol.
          <volume>13</volume>
<issue>4</issue>
<year>2017</year>
<fpage>197</fpage>
<lpage>198</lpage>
<pub-id pub-id-type="pmid">28303915</pub-id>
</element-citation>
</ref>
<ref id="bib0344">
<label>344</label>
<element-citation id="sbref0344" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Seibyl</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Impact of training method on the robustness of the visual assessment of 18F-Florbetaben PET scans: results from a phase-3 study</article-title>
<source/>J. Nucl. Med.
          <volume>57</volume>
<issue>6</issue>
<year>2016</year>
<fpage>900</fpage>
<lpage>906</lpage>
<pub-id pub-id-type="pmid">26823561</pub-id>
</element-citation>
</ref>
<ref id="bib0345">
<label>345</label>
<element-citation id="sbref0345" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Joshi</surname>
<given-names>A.D.</given-names>
</name>
</person-group>
<article-title>A semiautomated method for quantification of F 18 florbetapir PET images</article-title>
<source/>J. Nucl. Med.
          <volume>56</volume>
<issue>11</issue>
<year>2015</year>
<fpage>1736</fpage>
<lpage>1741</lpage>
<pub-id pub-id-type="pmid">26338898</pub-id>
</element-citation>
</ref>
<ref id="bib0346">
<label>346</label>
<element-citation id="sbref0346" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Marcoux</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>An automated pipeline for the analysis of PET data on the cortical surface</article-title>
<source/>Front. Neuroinform.
          <volume>12</volume>
<year>2018</year>
<fpage>13</fpage>
<comment>Article ID. 94</comment>
<pub-id pub-id-type="pmid">29628885</pub-id>
</element-citation>
</ref>
<ref id="bib0347">
<label>347</label>
<element-citation id="sbref0347" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tahmi</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>A fully automatic technique for precise localization and quantification of amyloid-beta PET scans</article-title>
<source/>J. Nucl. Med.
          <volume>60</volume>
<issue>12</issue>
<year>2019</year>
<fpage>1771</fpage>
<lpage>1779</lpage>
<pub-id pub-id-type="pmid">31171596</pub-id>
</element-citation>
</ref>
<ref id="bib0348">
<label>348</label>
<element-citation id="sbref0348" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Foster</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>A review on segmentation of positron emission tomography images</article-title>
<source/>Comput. Biol. Med.
          <volume>50</volume>
<year>2014</year>
<fpage>76</fpage>
<lpage>96</lpage>
<pub-id pub-id-type="pmid">24845019</pub-id>
</element-citation>
</ref>
<ref id="bib0349">
<label>349</label>
<element-citation id="sbref0349" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zasadny</surname>
<given-names>K.R.</given-names>
</name>
</person-group>
<article-title>Standardized uptake values of normal tissues at PET with 2-[fluorine-18]-fluoro-2-deoxy-D-glucose: variations with body weight and a method for correction</article-title>
<source/>Radiology
          <volume>189</volume>
<issue>3</issue>
<year>1993</year>
<fpage>847</fpage>
<lpage>850</lpage>
<pub-id pub-id-type="pmid">8234714</pub-id>
</element-citation>
</ref>
<ref id="bib0350">
<label>350</label>
<element-citation id="sbref0350" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>C.K.</given-names>
</name>
</person-group>
<article-title>Standardized uptake values of FDG: body surface area correction is preferable to body weight correction</article-title>
<source/>J. Nucl. Med.
          <volume>35</volume>
<issue>1</issue>
<year>1994</year>
<fpage>164</fpage>
<lpage>167</lpage>
<pub-id pub-id-type="pmid">8271040</pub-id>
</element-citation>
</ref>
<ref id="bib0351">
<label>351</label>
<element-citation id="sbref0351" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Basu</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Quantitative techniques in PET-CT imaging</article-title>
<source/>Curr. Med. Imaging Reviews
          <volume>7</volume>
<issue>3</issue>
<year>2011</year>
<fpage>216</fpage>
<lpage>233</lpage>
</element-citation>
</ref>
<ref id="bib0352">
<label>352</label>
<element-citation id="sbref0352" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huang</surname>
<given-names>S.-C.</given-names>
</name>
</person-group>
<article-title>Anatomy of SUV</article-title>
<source/>Nucl. Med. Biol.
          <volume>27</volume>
<issue>7</issue>
<year>2000</year>
<fpage>643</fpage>
<lpage>646</lpage>
<pub-id pub-id-type="pmid">11091106</pub-id>
</element-citation>
</ref>
<ref id="bib0353">
<label>353</label>
<element-citation id="sbref0353" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fahey</surname>
<given-names>F.H.</given-names>
</name>
</person-group>
<article-title>Variability in PET quantitation within a multicenter consortium</article-title>
<source/>Med. Phys.
          <volume>37</volume>
<issue>7</issue>
<year>2010</year>
<fpage>3660</fpage>
<lpage>3666</lpage>
<pub-id pub-id-type="pmid">20831073</pub-id>
</element-citation>
</ref>
<ref id="bib0354">
<label>354</label>
<element-citation id="sbref0354" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Holman</surname>
<given-names>B.F.</given-names>
</name>
</person-group>
<article-title>Improved correction for the tissue fraction effect in lung PET/CT imaging</article-title>
<source/>Phys. Med. Biol.
          <volume>60</volume>
<issue>18</issue>
<year>2015</year>
<fpage>7387</fpage>
<lpage>7402</lpage>
<pub-id pub-id-type="pmid">26350580</pub-id>
</element-citation>
</ref>
<ref id="bib0355">
<label>355</label>
<element-citation id="sbref0355" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rahmim</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Resolution modeling in PET imaging: theory, practice, benefits, and pitfalls</article-title>
<source/>Med. Phys.
          <volume>40</volume>
<issue>6</issue>
<year>2013</year>
<object-id pub-id-type="publisher-id">064301</object-id>
</element-citation>
</ref>
<ref id="bib0356">
<label>356</label>
<element-citation id="sbref0356" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bailey</surname>
<given-names>D.L.</given-names>
</name>
</person-group>
<article-title>Quantitative SPECT/CT: SPECT joins PET as a quantitative imaging modality</article-title>
<source/>Eur. J. Nucl. Med. Mol. Imaging
          <volume>41</volume>
<year>2014</year>
<fpage>S17</fpage>
<lpage>S25</lpage>
<pub-id pub-id-type="pmid">24037503</pub-id>
</element-citation>
</ref>
<ref id="bib0357">
<label>357</label>
<element-citation id="sbref0357" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ritt</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Absolute quantification in SPECT</article-title>
<source/>Eur. J. Nucl. Med. Mol. Imaging
          <volume>38</volume>
<issue>1</issue>
<year>2011</year>
<fpage>69</fpage>
<lpage>77</lpage>
</element-citation>
</ref>
<ref id="bib0358">
<label>358</label>
<element-citation id="sbref0358" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.R.</given-names>
</name>
</person-group>
<article-title>Partial volume correction for PET quantification and its impact on brain network in Alzheimer's disease</article-title>
<source/>Sci. Rep.
          <volume>7</volume>
<year>2017</year>
<fpage>14</fpage>
<comment>Article ID. 13035</comment>
<pub-id pub-id-type="pmid">28144039</pub-id>
</element-citation>
</ref>
<ref id="bib0359">
<label>359</label>
<element-citation id="sbref0359" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aston</surname>
<given-names>J.A.D.</given-names>
</name>
</person-group>
<article-title>Positron emission tomography partial volume correction: estimation and algorithms</article-title>
<source/>J. Cereb. Blood Flow Metab.
          <volume>22</volume>
<issue>8</issue>
<year>2002</year>
<fpage>1019</fpage>
<lpage>1034</lpage>
<pub-id pub-id-type="pmid">12172388</pub-id>
</element-citation>
</ref>
<ref id="bib0360">
<label>360</label>
<element-citation id="sbref0360" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Soret</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Partial-volume effect in PET tumor imaging</article-title>
<source/>J. Nucl. Med.
          <volume>48</volume>
<issue>6</issue>
<year>2007</year>
<fpage>932</fpage>
<lpage>945</lpage>
<pub-id pub-id-type="pmid">17504879</pub-id>
</element-citation>
</ref>
<ref id="bib0361">
<label>361</label>
<element-citation id="sbref0361" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Rousset</surname>
<given-names>O.G.</given-names>
</name>
</person-group>
<chapter-title>Correction for partial volume effects in emission tomography</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>236</fpage>
<lpage>271</lpage>
</element-citation>
</ref>
<ref id="bib0362">
<label>362</label>
<element-citation id="sbref0362" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rousset</surname>
<given-names>O.</given-names>
</name>
</person-group>
<article-title>Partial volume correction strategies in PET</article-title>
<source/>PET Clin.
          <volume>2</volume>
<issue>2</issue>
<year>2007</year>
<fpage>235</fpage>
<lpage>249</lpage>
<pub-id pub-id-type="pmid">27157875</pub-id>
</element-citation>
</ref>
<ref id="bib0363">
<label>363</label>
<element-citation id="sbref0363" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Erlandsson</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>A review of partial volume correction techniques for emission tomography and their applications in neurology, cardiology and oncology</article-title>
<source/>Phys. Med. Biol.
          <volume>57</volume>
<issue>21</issue>
<year>2012</year>
<fpage>R119</fpage>
<lpage>R159</lpage>
<pub-id pub-id-type="pmid">23073343</pub-id>
</element-citation>
</ref>
<ref id="bib0364">
<label>364</label>
<element-citation id="sbref0364" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Videen</surname>
<given-names>T.O.</given-names>
</name>
</person-group>
<article-title>Regional correction of positron emission tomography data for the effects of cerebral atrophy</article-title>
<source/>J. Cereb. Blood Flow Metab.
          <volume>8</volume>
<issue>5</issue>
<year>1988</year>
<fpage>662</fpage>
<lpage>670</lpage>
<pub-id pub-id-type="pmid">3262114</pub-id>
</element-citation>
</ref>
<ref id="bib0365">
<label>365</label>
<element-citation id="sbref0365" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meltzer</surname>
<given-names>C.C.</given-names>
</name>
</person-group>
<article-title>Correction of PET data for partial volume effects in human cerebral-cortex by MR imaging</article-title>
<source/>J. Comput. Assist. Tomogr.
          <volume>14</volume>
<issue>4</issue>
<year>1990</year>
<fpage>561</fpage>
<lpage>570</lpage>
<pub-id pub-id-type="pmid">2370355</pub-id>
</element-citation>
</ref>
<ref id="bib0366">
<label>366</label>
<element-citation id="sbref0366" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mullergartner</surname>
<given-names>H.W.</given-names>
</name>
</person-group>
<article-title>Measurement of radiotracer concentration in brain gray-matter using positron emission tomography - MRI-based correction for partial volume effects</article-title>
<source/>J. Cereb. Blood Flow Metab.
          <volume>12</volume>
<issue>4</issue>
<year>1992</year>
<fpage>571</fpage>
<lpage>583</lpage>
<pub-id pub-id-type="pmid">1618936</pub-id>
</element-citation>
</ref>
<ref id="bib0367">
<label>367</label>
<element-citation id="sbref0367" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meltzer</surname>
<given-names>C.C.</given-names>
</name>
</person-group>
<article-title>MR-based correction of brain PET measurements for heterogeneous gray matter radioactivity distribution</article-title>
<source/>J. Cereb. Blood Flow Metab.
          <volume>16</volume>
<issue>4</issue>
<year>1996</year>
<fpage>650</fpage>
<lpage>658</lpage>
<pub-id pub-id-type="pmid">8964805</pub-id>
</element-citation>
</ref>
<ref id="bib0368">
<label>368</label>
<element-citation id="sbref0368" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Hutton</surname>
<given-names>B.F.</given-names>
</name>
</person-group>
<chapter-title>Iterative reconstruction methods</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>107</fpage>
<lpage>140</lpage>
</element-citation>
</ref>
<ref id="bib0369">
<label>369</label>
<element-citation id="sbref0369" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Srinivas</surname>
<given-names>S.M.</given-names>
</name>
</person-group>
<article-title>A recovery coefficient method for partial volume correction of PET images</article-title>
<source/>Ann. Nucl. Med.
          <volume>23</volume>
<issue>4</issue>
<year>2009</year>
<fpage>341</fpage>
<lpage>348</lpage>
<pub-id pub-id-type="pmid">19367446</pub-id>
</element-citation>
</ref>
<ref id="bib0370">
<label>370</label>
<element-citation id="sbref0370" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Catana</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>PET/MRI for neurologic applications</article-title>
<source/>J. Nucl. Med.
          <volume>53</volume>
<issue>12</issue>
<year>2012</year>
<fpage>1916</fpage>
<lpage>1925</lpage>
<pub-id pub-id-type="pmid">23143086</pub-id>
</element-citation>
</ref>
<ref id="bib0371">
<label>371</label>
<element-citation id="sbref0371" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huesman</surname>
<given-names>R.H.</given-names>
</name>
</person-group>
<article-title>A new fast algorithm for the evaluation of regions of interest and statistical uncertainty in computed-tomography</article-title>
<source/>Phys. Med. Biol.
          <volume>29</volume>
<issue>5</issue>
<year>1984</year>
<fpage>543</fpage>
<lpage>552</lpage>
<pub-id pub-id-type="pmid">6610883</pub-id>
</element-citation>
</ref>
<ref id="bib0372">
<label>372</label>
<element-citation id="sbref0372" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Muzic</surname>
<given-names>R.F.</given-names>
</name>
</person-group>
<article-title>A method to correct for scatter, spillover, and partial volume effects in region of interest analysis in PET</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>17</volume>
<issue>2</issue>
<year>1998</year>
<fpage>202</fpage>
<lpage>213</lpage>
<pub-id pub-id-type="pmid">9688152</pub-id>
</element-citation>
</ref>
<ref id="bib0373">
<label>373</label>
<element-citation id="sbref0373" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Carson</surname>
<given-names>R.E.</given-names>
</name>
</person-group>
<article-title>A maximum likelihood method for region-of-interest evaluation in emission tomography</article-title>
<source/>J. Comput. Assist. Tomogr.
          <volume>10</volume>
<issue>4</issue>
<year>1986</year>
<fpage>654</fpage>
<lpage>663</lpage>
<pub-id pub-id-type="pmid">3488338</pub-id>
</element-citation>
</ref>
<ref id="bib0374">
<label>374</label>
<element-citation id="sbref0374" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rousset</surname>
<given-names>O.G.</given-names>
</name>
</person-group>
<article-title>Correction for partial volume effects in PET: principle and validation</article-title>
<source/>J. Nucl. Med.
          <volume>39</volume>
<issue>5</issue>
<year>1998</year>
<fpage>904</fpage>
<lpage>911</lpage>
<pub-id pub-id-type="pmid">9591599</pub-id>
</element-citation>
</ref>
<ref id="bib0375">
<label>375</label>
<element-citation id="sbref0375" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Frouin</surname>
<given-names>V.</given-names>
</name>
</person-group>
<article-title>Correction of partial-volume effect for PET striatal imaging: fast implementation and study of robustness</article-title>
<source/>J. Nucl. Med.
          <volume>43</volume>
<issue>12</issue>
<year>2002</year>
<fpage>1715</fpage>
<lpage>1726</lpage>
<pub-id pub-id-type="pmid">12468524</pub-id>
</element-citation>
</ref>
<ref id="bib0376">
<label>376</label>
<element-citation id="sbref0376" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Du</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Partial volume effect compensation for quantitative brain SPECT imaging</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>24</volume>
<issue>8</issue>
<year>2005</year>
<fpage>969</fpage>
<lpage>976</lpage>
<pub-id pub-id-type="pmid">16092329</pub-id>
</element-citation>
</ref>
<ref id="bib0377">
<label>377</label>
<element-citation id="sbref0377" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sattarivand</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Symmetric geometric transfer matrix partial volume correction for PET imaging: principle, validation and robustness</article-title>
<source/>Phys. Med. Biol.
          <volume>57</volume>
<issue>21</issue>
<year>2012</year>
<fpage>7101</fpage>
<lpage>7116</lpage>
<pub-id pub-id-type="pmid">23051703</pub-id>
</element-citation>
</ref>
<ref id="bib0378">
<label>378</label>
<element-citation id="sbref0378" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sureau</surname>
<given-names>F.C.</given-names>
</name>
</person-group>
<article-title>Impact of image-space resolution modeling for studies with the high-resolution research tomograph</article-title>
<source/>J. Nucl. Med.
          <volume>49</volume>
<issue>6</issue>
<year>2008</year>
<fpage>1000</fpage>
<lpage>1008</lpage>
<pub-id pub-id-type="pmid">18511844</pub-id>
</element-citation>
</ref>
<ref id="bib0379">
<label>379</label>
<element-citation id="sbref0379" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Akamatsu</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Improvement in PET/CT image quality with a combination of point-spread function and time-of-flight in relation to reconstruction parameters</article-title>
<source/>J. Nucl. Med.
          <volume>53</volume>
<issue>11</issue>
<year>2012</year>
<fpage>1716</fpage>
<lpage>1722</lpage>
<pub-id pub-id-type="pmid">22952340</pub-id>
</element-citation>
</ref>
<ref id="bib0380">
<label>380</label>
<element-citation id="sbref0380" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Andersen</surname>
<given-names>F.L.</given-names>
</name>
</person-group>
<article-title>Clinical evaluation of PET image reconstruction using a spatial resolution model</article-title>
<source/>Eur. J. Radiol.
          <volume>82</volume>
<issue>5</issue>
<year>2013</year>
<fpage>862</fpage>
<lpage>869</lpage>
<pub-id pub-id-type="pmid">23254158</pub-id>
</element-citation>
</ref>
<ref id="bib0381">
<label>381</label>
<element-citation id="sbref0381" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bowen</surname>
<given-names>S.L.</given-names>
</name>
</person-group>
<article-title>Influence of the partial volume correction method on F-18-fluorodeoxyglucose brain kinetic modelling from dynamic PET images reconstructed with resolution model based OSEM</article-title>
<source/>Phys. Med. Biol.
          <volume>58</volume>
<issue>20</issue>
<year>2013</year>
<fpage>7081</fpage>
<lpage>7106</lpage>
<pub-id pub-id-type="pmid">24052021</pub-id>
</element-citation>
</ref>
<ref id="bib0382">
<label>382</label>
<element-citation id="sbref0382" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Sibarita</surname>
<given-names>J.-B.</given-names>
</name>
</person-group>
<chapter-title>Deconvolution microscopy</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Rietdorf</surname>
<given-names>J</given-names>
</name>
</person-group>
<source/>Microscopy Techniques
          <year>2005</year>
<publisher-name>Springer Berlin Heidelberg</publisher-name>
<publisher-loc>Berlin, Heidelberg</publisher-loc>
<fpage>201</fpage>
<lpage>243</lpage>
<comment>Editor.</comment>
</element-citation>
</ref>
<ref id="bib0383">
<label>383</label>
<element-citation id="sbref0383" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Boussion</surname>
<given-names>N.</given-names>
</name>
</person-group>
<article-title>A multiresolution image based approach for correction of partial volume effects in emission tomography</article-title>
<source/>Phys. Med. Biol.
          <volume>51</volume>
<issue>7</issue>
<year>2006</year>
<fpage>1857</fpage>
<lpage>1876</lpage>
<pub-id pub-id-type="pmid">16552110</pub-id>
</element-citation>
</ref>
<ref id="bib0384">
<label>384</label>
<element-citation id="sbref0384" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Quarantelli</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Integrated software for the analysis of brain PET/SPECT studies with partial-volume-effect correction</article-title>
<source/>J. Nucl. Med.
          <volume>45</volume>
<issue>2</issue>
<year>2004</year>
<fpage>192</fpage>
<lpage>201</lpage>
<pub-id pub-id-type="pmid">14960635</pub-id>
</element-citation>
</ref>
<ref id="bib0385">
<label>385</label>
<element-citation id="sbref0385" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Svarer</surname>
<given-names>C.</given-names>
</name>
</person-group>
<article-title>MR-based automatic delineation of volumes of interest in human brain PET images using probability maps</article-title>
<source/>Neuroimage
          <volume>24</volume>
<issue>4</issue>
<year>2005</year>
<fpage>969</fpage>
<lpage>979</lpage>
<pub-id pub-id-type="pmid">15670674</pub-id>
</element-citation>
</ref>
<ref id="bib0386">
<label>386</label>
<element-citation id="sbref0386" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<chapter-title>Attenuation correction strategies in emission tomography</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>167</fpage>
<lpage>204</lpage>
</element-citation>
</ref>
<ref id="bib0387">
<label>387</label>
<element-citation id="sbref0387" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mehranian</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Vision 20/20: magnetic resonance imaging-guided attenuation correction in PET/MRI: challenges, solutions, and opportunities</article-title>
<source/>Med. Phys.
          <volume>43</volume>
<issue>3</issue>
<year>2016</year>
<fpage>1130</fpage>
<lpage>1155</lpage>
<pub-id pub-id-type="pmid">26936700</pub-id>
</element-citation>
</ref>
<ref id="bib0388">
<label>388</label>
<element-citation id="sbref0388" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hofmann</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Towards quantitative PET/MRI: a review of MR-based attenuation correction techniques</article-title>
<source/>Eur. J. Nucl. Med. Mol. Imaging
          <volume>36</volume>
<year>2009</year>
<fpage>93</fpage>
<lpage>104</lpage>
</element-citation>
</ref>
<ref id="bib0389">
<label>389</label>
<element-citation id="sbref0389" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Attenuation compensation in cerebral 3D PET: effect of the attenuation map on absolute and relative quantitation</article-title>
<source/>Eur. J. Nucl. Med. Mol. Imaging
          <volume>31</volume>
<issue>1</issue>
<year>2004</year>
<fpage>52</fpage>
<lpage>63</lpage>
<pub-id pub-id-type="pmid">14574512</pub-id>
</element-citation>
</ref>
<ref id="bib0390">
<label>390</label>
<element-citation id="sbref0390" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weinzapfel</surname>
<given-names>B.T.</given-names>
</name>
</person-group>
<article-title>Automated PET attenuation correction model for functional brain imaging</article-title>
<source/>J. Nucl. Med.
          <volume>42</volume>
<issue>3</issue>
<year>2001</year>
<fpage>483</fpage>
<lpage>491</lpage>
<pub-id pub-id-type="pmid">11337527</pub-id>
</element-citation>
</ref>
<ref id="bib0391">
<label>391</label>
<element-citation id="sbref0391" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Watabe</surname>
<given-names>H.</given-names>
</name>
</person-group>
<chapter-title>Acquisition of attenuation map for brain PET study using optical tracking system</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Seibert</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<series>IEEE Nuclear Science Symposium, Conference Records</series>
<volume>vols 1-4</volume>
<year>2002</year>
<publisher-name>Ieee</publisher-name>
<publisher-loc>New York</publisher-loc>
<fpage>1458</fpage>
<lpage>1461</lpage>
<comment>Editor.</comment>
</element-citation>
</ref>
<ref id="bib0392">
<label>392</label>
<element-citation id="sbref0392" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nuyts</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Simultaneous maximum a posteriori reconstruction of attenuation and activity distributions from emission sinograms</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>18</volume>
<issue>5</issue>
<year>1999</year>
<fpage>393</fpage>
<lpage>403</lpage>
<pub-id pub-id-type="pmid">10416801</pub-id>
</element-citation>
</ref>
<ref id="bib0393">
<label>393</label>
<element-citation id="sbref0393" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nuyts</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Completion of a truncated attenuation image from the attenuated PET emission data</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>32</volume>
<issue>2</issue>
<year>2013</year>
<fpage>237</fpage>
<lpage>246</lpage>
<pub-id pub-id-type="pmid">23014717</pub-id>
</element-citation>
</ref>
<ref id="bib0394">
<label>394</label>
<element-citation id="sbref0394" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rezaei</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>ML-reconstruction for TOF-PET with simultaneous estimation of the attenuation factors</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>33</volume>
<issue>7</issue>
<year>2014</year>
<fpage>1563</fpage>
<lpage>1572</lpage>
<pub-id pub-id-type="pmid">24760903</pub-id>
</element-citation>
</ref>
<ref id="bib0395">
<label>395</label>
<element-citation id="sbref0395" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Benoit</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Optimized MLAA for quantitative non-TOF PET/MR of the brain</article-title>
<source/>Phys. Med. Biol.
          <volume>61</volume>
<issue>24</issue>
<year>2016</year>
<fpage>8854</fpage>
<lpage>8874</lpage>
<pub-id pub-id-type="pmid">27910823</pub-id>
</element-citation>
</ref>
<ref id="bib0396">
<label>396</label>
<element-citation id="sbref0396" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ladefoged</surname>
<given-names>C.N.</given-names>
</name>
</person-group>
<article-title>A multi-centre evaluation of eleven clinically feasible brain PET/MRI attenuation correction techniques using a large cohort of patients</article-title>
<source/>Neuroimage
          <volume>147</volume>
<year>2017</year>
<fpage>346</fpage>
<lpage>359</lpage>
<pub-id pub-id-type="pmid">27988322</pub-id>
</element-citation>
</ref>
<ref id="bib0397">
<label>397</label>
<element-citation id="sbref0397" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bailey</surname>
<given-names>D.L.</given-names>
</name>
</person-group>
<article-title>Transmission scanning in emission tomography</article-title>
<source/>Eur. J. Nucl. Med.
          <volume>25</volume>
<issue>7</issue>
<year>1998</year>
<fpage>774</fpage>
<lpage>787</lpage>
<pub-id pub-id-type="pmid">9662601</pub-id>
</element-citation>
</ref>
<ref id="bib0398">
<label>398</label>
<element-citation id="sbref0398" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ichihara</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Evaluation of SPET quantification of simultaneous emission and transmission imaging of the brain using a multidetector SPET system with the TEW scatter compensation method and fan-beam collimation</article-title>
<source/>Eur. J. Nucl. Med.
          <volume>23</volume>
<issue>10</issue>
<year>1996</year>
<fpage>1292</fpage>
<lpage>1299</lpage>
<pub-id pub-id-type="pmid">8781132</pub-id>
</element-citation>
</ref>
<ref id="bib0399">
<label>399</label>
<element-citation id="sbref0399" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van Laere</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>Nonuniform transmission in brain SPECT using 201Tl, 153Gd, and 99mTc static line sources: anthropomorphic dosimetry studies and influence on brain quantification</article-title>
<source/>J. Nucl. Med.
          <volume>41</volume>
<issue>12</issue>
<year>2000</year>
<fpage>2051</fpage>
<lpage>2062</lpage>
<pub-id pub-id-type="pmid">11138692</pub-id>
</element-citation>
</ref>
<ref id="bib0400">
<label>400</label>
<element-citation id="sbref0400" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brown</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Investigation of the relationship between linear attenuation coefficients and CT Hounsfield units using radionuclides for SPECT</article-title>
<source/>Appl. Radiat. Isot.
          <volume>66</volume>
<issue>9</issue>
<year>2008</year>
<fpage>1206</fpage>
<lpage>1212</lpage>
<pub-id pub-id-type="pmid">18662614</pub-id>
</element-citation>
</ref>
<ref id="bib0401">
<label>401</label>
<element-citation id="sbref0401" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Patton</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>Image fusion using an integrated, dual-head coincidence camera with x-ray tube-based attenuation maps</article-title>
<source/>J. Nucl. Med.
          <volume>41</volume>
<issue>8</issue>
<year>2000</year>
<fpage>1364</fpage>
<lpage>1368</lpage>
<pub-id pub-id-type="pmid">10945529</pub-id>
</element-citation>
</ref>
<ref id="bib0402">
<label>402</label>
<element-citation id="sbref0402" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kamel</surname>
<given-names>E.M.</given-names>
</name>
</person-group>
<article-title>Impact of metallic dental implants on CT-based attenuation correction in a combined PET/CT scanner</article-title>
<source/>Eur. Radiol.
          <volume>13</volume>
<issue>4</issue>
<year>2003</year>
<fpage>724</fpage>
<lpage>728</lpage>
<pub-id pub-id-type="pmid">12664109</pub-id>
</element-citation>
</ref>
<ref id="bib0403">
<label>403</label>
<element-citation id="sbref0403" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kinahan</surname>
<given-names>P.E.</given-names>
</name>
</person-group>
<article-title>X-ray-based attenuation correction for positron emission tomography/computed tomography scanners</article-title>
<source/>Semin. Nucl. Med.
          <volume>33</volume>
<issue>3</issue>
<year>2003</year>
<fpage>166</fpage>
<lpage>179</lpage>
<pub-id pub-id-type="pmid">12931319</pub-id>
</element-citation>
</ref>
<ref id="bib0404">
<label>404</label>
<element-citation id="sbref0404" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Carney</surname>
<given-names>J.P.J.</given-names>
</name>
</person-group>
<article-title>Method for transforming CT images for attenuation correction in PET/CT imaging</article-title>
<source/>Med. Phys.
          <volume>33</volume>
<issue>4</issue>
<year>2006</year>
<fpage>976</fpage>
<lpage>983</lpage>
<pub-id pub-id-type="pmid">16696474</pub-id>
</element-citation>
</ref>
<ref id="bib0405">
<label>405</label>
<element-citation id="sbref0405" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wollenweber</surname>
<given-names>S.D.</given-names>
</name>
</person-group>
<article-title>Evaluation of an atlas-based PET head attenuation correction using PET/CT &amp; MR patient data</article-title>
<source/>IEEE Trans. Nucl. Sci.
          <volume>60</volume>
<issue>5</issue>
<year>2013</year>
<fpage>3383</fpage>
<lpage>3390</lpage>
</element-citation>
</ref>
<ref id="bib0406">
<label>406</label>
<element-citation id="sbref0406" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stodilka</surname>
<given-names>R.Z.</given-names>
</name>
</person-group>
<article-title>Scatter and attenuation correction for brain SPECT using attenuation distributions inferred from a head atlas</article-title>
<source/>J. Nucl. Med.
          <volume>41</volume>
<issue>9</issue>
<year>2000</year>
<fpage>1569</fpage>
<lpage>1578</lpage>
<pub-id pub-id-type="pmid">10994740</pub-id>
</element-citation>
</ref>
<ref id="bib0407">
<label>407</label>
<element-citation id="sbref0407" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Magnetic resonance imaging-guided attenuation and scatter corrections in three-dimensional brain positron emission tomography</article-title>
<source/>Med. Phys.
          <volume>30</volume>
<issue>5</issue>
<year>2003</year>
<fpage>937</fpage>
<lpage>948</lpage>
<pub-id pub-id-type="pmid">12773003</pub-id>
</element-citation>
</ref>
<ref id="bib0408">
<label>408</label>
<element-citation id="sbref0408" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Wagenknecht</surname>
<given-names>G.</given-names>
</name>
</person-group>
<chapter-title>Knowledge-based segmentation of attenuation-relevant regions of the head in T1-weighted MR images for attenuation correction in MR/PET systems</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Yu</surname>
<given-names>B.</given-names>
</name>
</person-group>
<source/>2009 IEEE Nuclear Science Symposium Conference Record, vols 1-5
          <year>2009</year>
<publisher-name>Ieee</publisher-name>
<publisher-loc>New York</publisher-loc>
<fpage>3338</fpage>
</element-citation>
</ref>
<ref id="bib0409">
<label>409</label>
<element-citation id="sbref0409" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Quantitative evaluation of atlas-based attenuation correction for brain PET in an integrated time-of-flight PET/MR imaging system</article-title>
<source/>Radiology
          <volume>284</volume>
<issue>1</issue>
<year>2017</year>
<fpage>169</fpage>
<lpage>179</lpage>
<pub-id pub-id-type="pmid">28234560</pub-id>
</element-citation>
</ref>
<ref id="bib0410">
<label>410</label>
<element-citation id="sbref0410" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bal</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Evaluation of MLACF based calculated attenuation brain PET imaging for FDG patient studies</article-title>
<source/>Phys. Med. Biol.
          <volume>62</volume>
<issue>7</issue>
<year>2017</year>
<fpage>2542</fpage>
<lpage>2558</lpage>
<pub-id pub-id-type="pmid">28165328</pub-id>
</element-citation>
</ref>
<ref id="bib0411">
<label>411</label>
<element-citation id="sbref0411" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Joint correction of attenuation and scatter in image space using deep convolutional neural networks for dedicated brain (18)F-FDG PET</article-title>
<source/>Phys. Med. Biol.
          <volume>64</volume>
<issue>7</issue>
<year>2019</year>
<object-id pub-id-type="publisher-id">075019</object-id>
</element-citation>
</ref>
<ref id="bib0412">
<label>412</label>
<element-citation id="sbref0412" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Le Goff-Rougetet</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Segmented MR images for brain attenuation correction in PET</article-title>
<source/>Med. Imaging.
          <volume>2167</volume>
<year>1994</year>
<comment>SPIE</comment>
</element-citation>
</ref>
<ref id="bib0413">
<label>413</label>
<element-citation id="sbref0413" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keereman</surname>
<given-names>V.</given-names>
</name>
</person-group>
<article-title>MRI-based attenuation correction for pet/mri using ultrashort echo time sequences</article-title>
<source/>J. Nucl. Med.
          <volume>51</volume>
<issue>5</issue>
<year>2010</year>
<fpage>812</fpage>
<lpage>818</lpage>
<pub-id pub-id-type="pmid">20439508</pub-id>
</element-citation>
</ref>
<ref id="bib0414">
<label>414</label>
<element-citation id="sbref0414" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Martinez-Moller</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Tissue classification as a potential approach for attenuation correction in whole-body PET/MRI: evaluation with PET/CT data</article-title>
<source/>J. Nucl. Med.
          <volume>50</volume>
<issue>4</issue>
<year>2009</year>
<fpage>520</fpage>
<lpage>526</lpage>
<pub-id pub-id-type="pmid">19289430</pub-id>
</element-citation>
</ref>
<ref id="bib0415">
<label>415</label>
<element-citation id="sbref0415" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berker</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>MRI-based attenuation correction for hybrid PET/MRI systems: a 4-class tissue segmentation technique using a combined ultrashort-echo-time/dixon MRI sequence</article-title>
<source/>J. Nucl. Med.
          <volume>53</volume>
<issue>5</issue>
<year>2012</year>
<fpage>796</fpage>
<lpage>804</lpage>
<pub-id pub-id-type="pmid">22505568</pub-id>
</element-citation>
</ref>
<ref id="bib0416">
<label>416</label>
<element-citation id="sbref0416" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Andersen</surname>
<given-names>F.L.</given-names>
</name>
</person-group>
<article-title>Combined PET/MR imaging in neurology: MR-based attenuation correction implies a strong spatial bias when ignoring bone</article-title>
<source/>Neuroimage
          <volume>84</volume>
<year>2014</year>
<fpage>206</fpage>
<lpage>216</lpage>
<pub-id pub-id-type="pmid">23994317</pub-id>
</element-citation>
</ref>
<ref id="bib0417">
<label>417</label>
<element-citation id="sbref0417" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Kazerooni</surname>
<given-names>A.F.</given-names>
</name>
</person-group>
<chapter-title>Generation of MR-based attenuation correction map of PET images in the brain employing joint segmentation of skull and soft-tissue from single short-TE MR imaging modality</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Gao</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Shi</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>S.</given-names>
</name>
</person-group>
<source/>Computational Methods for Molecular Imaging
          <year>2015</year>
<publisher-name>Springer International Publishing: Cham</publisher-name>
<fpage>139</fpage>
<lpage>147</lpage>
<comment>Editors.</comment>
</element-citation>
</ref>
<ref id="bib0418">
<label>418</label>
<element-citation id="sbref0418" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Khateri</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Generation of a four-class attenuation map for MRI-based attenuation correction of PET data in the head area using a novel combination of STE/Dixon-MRI and FCM clustering</article-title>
<source/>Mol. Imaging Biol.
          <volume>17</volume>
<issue>6</issue>
<year>2015</year>
<fpage>884</fpage>
<lpage>892</lpage>
<pub-id pub-id-type="pmid">25917750</pub-id>
</element-citation>
</ref>
<ref id="bib0419">
<label>419</label>
<element-citation id="sbref0419" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wiesinger</surname>
<given-names>F.</given-names>
</name>
</person-group>
<article-title>Zero TE MR bone imaging in the head</article-title>
<source/>Magn. Reson. Med.
          <volume>75</volume>
<issue>1</issue>
<year>2016</year>
<fpage>107</fpage>
<lpage>114</lpage>
<pub-id pub-id-type="pmid">25639956</pub-id>
</element-citation>
</ref>
<ref id="bib0420">
<label>420</label>
<element-citation id="sbref0420" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Evaluation of sinus/edge-corrected zero-echo-time-based attenuation correction in brain PET/MRI</article-title>
<source/>J. Nucl. Med.
          <volume>58</volume>
<issue>11</issue>
<year>2017</year>
<fpage>1873</fpage>
<lpage>1879</lpage>
<pub-id pub-id-type="pmid">28473594</pub-id>
</element-citation>
</ref>
<ref id="bib0421">
<label>421</label>
<element-citation id="sbref0421" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Delso</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Improving PET/MR brain quantitation with template-enhanced ZTE</article-title>
<source/>Neuroimage
          <volume>181</volume>
<year>2018</year>
<fpage>403</fpage>
<lpage>413</lpage>
<pub-id pub-id-type="pmid">30010010</pub-id>
</element-citation>
</ref>
<ref id="bib0422">
<label>422</label>
<element-citation id="sbref0422" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sousa</surname>
<given-names>J.M.</given-names>
</name>
</person-group>
<article-title>Evaluation of zero-echo-time attenuation correction for integrated PET/MR brain imaging-comparison to head atlas and (68)Ge-transmission-based attenuation correction</article-title>
<source/>EJNMMI Phys.
          <volume>5</volume>
<issue>1</issue>
<year>2018</year>
<fpage>20</fpage>
<pub-id pub-id-type="pmid">30345471</pub-id>
</element-citation>
</ref>
<ref id="bib0423">
<label>423</label>
<element-citation id="sbref0423" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sgard</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>ZTE MR-based attenuation correction in brain FDG-PET/MR: performance in patients with cognitive impairment</article-title>
<source/>Eur. Radiol.
          <volume>30</volume>
<issue>3</issue>
<year>2020</year>
<fpage>1770</fpage>
<lpage>1779</lpage>
<pub-id pub-id-type="pmid">31748857</pub-id>
</element-citation>
</ref>
<ref id="bib0424">
<label>424</label>
<element-citation id="sbref0424" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roy</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>PET attenuation correction using synthetic CT from ultrashort echo-time MR imaging</article-title>
<source/>J. Nucl. Med.
          <volume>55</volume>
<issue>12</issue>
<year>2014</year>
<fpage>2071</fpage>
<lpage>2077</lpage>
<pub-id pub-id-type="pmid">25413135</pub-id>
</element-citation>
</ref>
<ref id="bib0425">
<label>425</label>
<element-citation id="sbref0425" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Poynton</surname>
<given-names>C.B.</given-names>
</name>
</person-group>
<article-title>Probabilistic atlas-based segmentation of combined T1-weighted and DUTE MRI for calculation of head attenuation maps in integrated PET/MRI scanners</article-title>
<source/>Am. J. Nucl. Med. Mol. Imaging
          <volume>4</volume>
<issue>2</issue>
<year>2014</year>
<fpage>160</fpage>
<lpage>171</lpage>
<pub-id pub-id-type="pmid">24753982</pub-id>
</element-citation>
</ref>
<ref id="bib0426">
<label>426</label>
<element-citation id="sbref0426" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Delso</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Cluster-based segmentation of dual-echo ultra-short echo time images for PET/MR bone localization</article-title>
<source/>EJNMMI Phys.
          <volume>1</volume>
<issue>1</issue>
<year>2014</year>
<fpage>7</fpage>
<pub-id pub-id-type="pmid">26501449</pub-id>
</element-citation>
</ref>
<ref id="bib0427">
<label>427</label>
<element-citation id="sbref0427" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Johanson</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Improved quality of computed tomography substitute derived from magnetic resonance (MR) data by incorporation of spatial information - potential application for MR-only radiotherapy and attenuation correction in positron emission tomography</article-title>
<source/>Acta Oncol. (Madr)
          <volume>52</volume>
<issue>7</issue>
<year>2013</year>
<fpage>1369</fpage>
<lpage>1373</lpage>
</element-citation>
</ref>
<ref id="bib0428">
<label>428</label>
<element-citation id="sbref0428" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chang</surname>
<given-names>L.-T.</given-names>
</name>
</person-group>
<article-title>A method for attenuation correction in radionuclide computed tomography</article-title>
<source/>IEEE Trans. Nucl. Sci.
          <volume>25</volume>
<issue>1</issue>
<year>1978</year>
<fpage>638</fpage>
<lpage>643</lpage>
</element-citation>
</ref>
<ref id="bib0429">
<label>429</label>
<element-citation id="sbref0429" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shepp</surname>
<given-names>L.A.</given-names>
</name>
</person-group>
<article-title>Maximum likelihood reconstruction for emission tomography</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>1</volume>
<issue>2</issue>
<year>1982</year>
<fpage>113</fpage>
<lpage>122</lpage>
<pub-id pub-id-type="pmid">18238264</pub-id>
</element-citation>
</ref>
<ref id="bib0430">
<label>430</label>
<element-citation id="sbref0430" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lange</surname>
<given-names>K.</given-names>
</name>
</person-group>
<article-title>EM reconstruction algorithms for emission and transmission tomography</article-title>
<source/>J. Comput. Assist. Tomogr.
          <volume>8</volume>
<issue>2</issue>
<year>1984</year>
<fpage>306</fpage>
<lpage>316</lpage>
<pub-id pub-id-type="pmid">6608535</pub-id>
</element-citation>
</ref>
<ref id="bib0431">
<label>431</label>
<element-citation id="sbref0431" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gullberg</surname>
<given-names>G.T.</given-names>
</name>
</person-group>
<article-title>An attenuated projector-backprojector for iterative SPECT reconstruction</article-title>
<source/>Phys. Med. Biol.
          <volume>30</volume>
<issue>8</issue>
<year>1985</year>
<fpage>799</fpage>
<lpage>816</lpage>
<pub-id pub-id-type="pmid">3840265</pub-id>
</element-citation>
</ref>
<ref id="bib0432">
<label>432</label>
<element-citation id="sbref0432" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Scatter compensation techniques in PET</article-title>
<source/>PET Clin
          <volume>2</volume>
<issue>2</issue>
<year>2007</year>
<fpage>219</fpage>
<lpage>234</lpage>
<pub-id pub-id-type="pmid">27157874</pub-id>
</element-citation>
</ref>
<ref id="bib0433">
<label>433</label>
<element-citation id="sbref0433" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hutton</surname>
<given-names>B.F.</given-names>
</name>
</person-group>
<article-title>Review and current status of SPECT scatter correction</article-title>
<source/>Phys. Med. Biol.
          <volume>56</volume>
<issue>14</issue>
<year>2011</year>
<fpage>R85</fpage>
<lpage>R112</lpage>
<pub-id pub-id-type="pmid">21701055</pub-id>
</element-citation>
</ref>
<ref id="bib0434">
<label>434</label>
<element-citation id="sbref0434" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<chapter-title>Scatter correction strategies in emission tomography</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Zaidi</surname>
<given-names>H.</given-names>
</name>
</person-group>
<source/>Quantitative Analysis in Nuclear Medicine Imaging
          <year>2006</year>
<publisher-name>Springer US</publisher-name>
<publisher-loc>Boston, MA</publisher-loc>
<fpage>205</fpage>
<lpage>235</lpage>
<comment>Editor</comment>
</element-citation>
</ref>
<ref id="bib0435">
<label>435</label>
<element-citation id="sbref0435" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kupferschlaeger</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Absolute quantification in SPECT - a phantom study</article-title>
<source/>Eur. J. Nucl. Med. Mol. Imaging
          <volume>42</volume>
<year>2015</year>
<fpage>S148</fpage>
<lpage>S149</lpage>
</element-citation>
</ref>
<ref id="bib0436">
<label>436</label>
<element-citation id="sbref0436" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jaszczak</surname>
<given-names>R.J.</given-names>
</name>
</person-group>
<article-title>Improved SPECT quantification using compensation for scattered photons</article-title>
<source/>J. Nucl. Med.
          <volume>25</volume>
<issue>8</issue>
<year>1984</year>
<fpage>893</fpage>
<lpage>900</lpage>
<pub-id pub-id-type="pmid">6611390</pub-id>
</element-citation>
</ref>
<ref id="bib0437">
<label>437</label>
<element-citation id="sbref0437" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Grootoonk</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Correction for scatter in 3D brain PET using a dual energy window method</article-title>
<source/>Phys. Med. Biol.
          <volume>41</volume>
<issue>12</issue>
<year>1996</year>
<fpage>2757</fpage>
<lpage>2774</lpage>
<pub-id pub-id-type="pmid">8971967</pub-id>
</element-citation>
</ref>
<ref id="bib0438">
<label>438</label>
<element-citation id="sbref0438" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ichihara</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Compton scatter compensation using the triple-energy window method for single- and dual-isotope SPECT</article-title>
<source/>J. Nucl. Med.
          <volume>34</volume>
<issue>12</issue>
<year>1993</year>
<fpage>2216</fpage>
<lpage>2221</lpage>
<pub-id pub-id-type="pmid">8254414</pub-id>
</element-citation>
</ref>
<ref id="bib0439">
<label>439</label>
<element-citation id="sbref0439" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shao</surname>
<given-names>L.</given-names>
</name>
</person-group>
<article-title>Triple energy window scatter correction technique in PET</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>13</volume>
<issue>4</issue>
<year>1994</year>
<fpage>641</fpage>
<lpage>648</lpage>
<pub-id pub-id-type="pmid">18218542</pub-id>
</element-citation>
</ref>
<ref id="bib0440">
<label>440</label>
<element-citation id="sbref0440" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Koral</surname>
<given-names>K.F.</given-names>
</name>
</person-group>
<article-title>SPECT Compton-scattering correction by analysis of energy spectra</article-title>
<source/>J. Nucl. Med.
          <volume>29</volume>
<issue>2</issue>
<year>1988</year>
<fpage>195</fpage>
<lpage>202</lpage>
<pub-id pub-id-type="pmid">3258023</pub-id>
</element-citation>
</ref>
<ref id="bib0441">
<label>441</label>
<element-citation id="sbref0441" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bentourkia</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Energy dependence of scatter components in multispectral PET imaging</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>14</volume>
<issue>1</issue>
<year>1995</year>
<fpage>138</fpage>
<lpage>145</lpage>
<pub-id pub-id-type="pmid">18215818</pub-id>
</element-citation>
</ref>
<ref id="bib0442">
<label>442</label>
<element-citation id="sbref0442" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hasegawa</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>A Monte Carlo simulation study on coarse septa for scatter correction in 3-D PET</article-title>
<source/>IEEE Trans. Nucl. Sci.
          <volume>49</volume>
<issue>5</issue>
<year>2002</year>
<fpage>2133</fpage>
<lpage>2138</lpage>
</element-citation>
</ref>
<ref id="bib0443">
<label>443</label>
<element-citation id="sbref0443" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chuang</surname>
<given-names>K.S.</given-names>
</name>
</person-group>
<article-title>Novel scatter correction for three-dimensional positron emission tomography by use of a beam stopper device</article-title>
<source/>Nucl. Instrum. Methods Phys. Res.h Section A
          <volume>551</volume>
<issue>2-3</issue>
<year>2005</year>
<fpage>540</fpage>
<lpage>552</lpage>
</element-citation>
</ref>
<ref id="bib0444">
<label>444</label>
<element-citation id="sbref0444" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>H.T.</given-names>
</name>
</person-group>
<chapter-title>A fast, energy-dependent scatter reduction method for 3D PET imaging</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Metzler</surname>
<given-names>S.D.</given-names>
</name>
</person-group>
<source/>IEEE Nuclear Science Symposium, Conference Record, Vols 1-5
          <year>2004</year>
<publisher-name>IEEE</publisher-name>
<publisher-loc>Portland, OR</publisher-loc>
<fpage>2630</fpage>
<lpage>2634</lpage>
</element-citation>
</ref>
<ref id="bib0445">
<label>445</label>
<element-citation id="sbref0445" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Popescu</surname>
<given-names>L.M.</given-names>
</name>
</person-group>
<article-title>PET energy-based scatter estimation and image reconstruction with energy-dependent corrections</article-title>
<source/>Phys. Med. Biol.
          <volume>51</volume>
<issue>11</issue>
<year>2006</year>
<fpage>2919</fpage>
<lpage>2937</lpage>
<pub-id pub-id-type="pmid">16723775</pub-id>
</element-citation>
</ref>
<ref id="bib0446">
<label>446</label>
<element-citation id="sbref0446" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bailey</surname>
<given-names>D.L.</given-names>
</name>
</person-group>
<article-title>A convolution-subtraction scatter correction method for 3d PET</article-title>
<source/>Phys. Med. Biol.
          <volume>39</volume>
<issue>3</issue>
<year>1994</year>
<fpage>411</fpage>
<lpage>424</lpage>
<pub-id pub-id-type="pmid">15551590</pub-id>
</element-citation>
</ref>
<ref id="bib0447">
<label>447</label>
<element-citation id="sbref0447" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meikle</surname>
<given-names>S.R.</given-names>
</name>
</person-group>
<article-title>A transmission-dependent method for scatter correction in SPECT</article-title>
<source/>J. Nucl. Med.
          <volume>35</volume>
<issue>2</issue>
<year>1994</year>
<fpage>360</fpage>
<lpage>367</lpage>
<pub-id pub-id-type="pmid">8295011</pub-id>
</element-citation>
</ref>
<ref id="bib0448">
<label>448</label>
<element-citation id="sbref0448" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lubberink</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Non-stationary convolution subtraction scatter correction with a dual-exponential scatter kernel for the Hamamatsu SHR-7700 animal PET scanner</article-title>
<source/>Phys. Med. Biol.
          <volume>49</volume>
<issue>5</issue>
<year>2004</year>
<fpage>833</fpage>
<lpage>842</lpage>
<comment>Article ID. Pii s0031-9155(04)69622-9</comment>
<pub-id pub-id-type="pmid">15070206</pub-id>
</element-citation>
</ref>
<ref id="bib0449">
<label>449</label>
<element-citation id="sbref0449" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bendriem</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>A technique for the correction of scattered radiation in a PET system using time-of-flight information</article-title>
<source/>J. Comput. Assist. Tomogr.
          <volume>10</volume>
<issue>2</issue>
<year>1986</year>
<fpage>287</fpage>
<lpage>295</lpage>
<pub-id pub-id-type="pmid">3485124</pub-id>
</element-citation>
</ref>
<ref id="bib0450">
<label>450</label>
<element-citation id="sbref0450" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Levin</surname>
<given-names>C.S.</given-names>
</name>
</person-group>
<article-title>A Monte-Carlo correction for the effect of compton-scattering in 3-d PET brain imaging</article-title>
<source/>IEEE Trans. Nucl. Sci.
          <volume>42</volume>
<issue>4</issue>
<year>1995</year>
<fpage>1181</fpage>
<lpage>1185</lpage>
</element-citation>
</ref>
<ref id="bib0451">
<label>451</label>
<element-citation id="sbref0451" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Watson</surname>
<given-names>C.C.</given-names>
</name>
<name>
<surname>New, faster, image-base</surname>
</name>
</person-group>
<article-title>d scatter correction for 3D PET</article-title>
<source/>IEEE Trans. Nucl. Sci.
          <volume>47</volume>
<issue>4</issue>
<year>2000</year>
<fpage>1587</fpage>
<lpage>1594</lpage>
</element-citation>
</ref>
<ref id="bib0452">
<label>452</label>
<element-citation id="sbref0452" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Accorsi</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Optimization of a fully 3D single scatter simulation algorithm for 3D PET</article-title>
<source/>Phys. Med. Biol.
          <volume>49</volume>
<issue>12</issue>
<year>2004</year>
<fpage>2577</fpage>
<lpage>2598</lpage>
<comment>Article ID. PII s0031-9155(04)70798-8</comment>
<pub-id pub-id-type="pmid">15272675</pub-id>
</element-citation>
</ref>
<ref id="bib0453">
<label>453</label>
<element-citation id="sbref0453" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Beekman</surname>
<given-names>F.J.</given-names>
</name>
</person-group>
<article-title>Efficient fully 3-D iterative SPECT reconstruction with Monte Carlo-based scatter compensation</article-title>
<source/>IEEE Trans. Med. Imaging
          <volume>21</volume>
<issue>8</issue>
<year>2002</year>
<fpage>867</fpage>
<lpage>877</lpage>
<pub-id pub-id-type="pmid">12472260</pub-id>
</element-citation>
</ref>
<ref id="bib0454">
<label>454</label>
<element-citation id="sbref0454" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cot</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Absolute quantification in dopaminergic neurotransmission SPECT using a Monte Carlo-based scatter correction and fully 3-dimensional reconstruction</article-title>
<source/>J. Nucl. Med.
          <volume>46</volume>
<issue>9</issue>
<year>2005</year>
<fpage>1497</fpage>
<lpage>1504</lpage>
<pub-id pub-id-type="pmid">16157533</pub-id>
</element-citation>
</ref>
<ref id="bib0455">
<label>455</label>
<element-citation id="sbref0455" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lazaro</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Fully 3D Monte Carlo reconstruction in SPECT: a feasibility study</article-title>
<source/>Phys. Med. Biol.
          <volume>50</volume>
<issue>16</issue>
<year>2005</year>
<fpage>3739</fpage>
<lpage>3754</lpage>
<pub-id pub-id-type="pmid">16077224</pub-id>
</element-citation>
</ref>
<ref id="bib0456">
<label>456</label>
<element-citation id="sbref0456" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Salas-Gonzalez</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>Linear intensity normalization of FP-CIT SPECT brain images using the alpha-stable distribution</article-title>
<source/>Neuroimage
          <volume>65</volume>
<year>2013</year>
<fpage>449</fpage>
<lpage>455</lpage>
<pub-id pub-id-type="pmid">23063448</pub-id>
</element-citation>
</ref>
<ref id="bib0457">
<label>457</label>
<element-citation id="sbref0457" publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Castillo-Barnes</surname>
<given-names>D.</given-names>
</name>
</person-group>
<chapter-title>On a heavy-tailed intensity normalization of the parkinson's progression markers initiative brain database</chapter-title>
<person-group person-group-type="editor">
<name>
<surname>Vicente</surname>
<given-names>J.M.F.</given-names>
</name>
</person-group>
<source/>Natural and Artificial Computation for Biomedicine and Neuroscience, Pt I
          <year>2017</year>
<publisher-name>Springer International Publishing Ag</publisher-name>
<publisher-loc>Cham</publisher-loc>
<fpage>298</fpage>
<lpage>304</lpage>
</element-citation>
</ref>
<ref id="bib0458">
<label>458</label>
<element-citation id="sbref0458" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brahim</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>Comparison between different intensity normalization methods in 123I-Ioflupane imaging for the automatic detection of parkinsonism</article-title>
<source/>PLoS ONE
          <volume>10</volume>
<issue>6</issue>
<year>2015</year>
<object-id pub-id-type="publisher-id">e0130274</object-id>
</element-citation>
</ref>
<ref id="bib0459">
<label>459</label>
<element-citation id="sbref0459" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>D'Andrea</surname>
<given-names>A.</given-names>
</name>
</person-group>
<article-title>The role of multimodality imaging in COVID-19 patients: from diagnosis to clinical monitoring and prognosis</article-title>
<source/>Giornale Italiano Di Cardio Logia
          <volume>21</volume>
<issue>5</issue>
<year>2020</year>
<fpage>345</fpage>
<lpage>353</lpage>
</element-citation>
</ref>
<ref id="bib0460">
<label>460</label>
<element-citation id="sbref0460" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Górriz</surname>
<given-names>J.M.</given-names>
</name>
</person-group>
<article-title>Artificial intelligence within the interplay between natural and artificial Computation: advances in data science, trends and applications</article-title>
<source/>Neurocomputing
          <year>2020</year>
<pub-id pub-id-type="doi">10.1016/j.neucom.2020.05.078</pub-id>
</element-citation>
</ref>
</ref-list>
<ack id="ack0001">
<title>Acknowledgments</title>
<p>This study was partly supported by <funding-source id="gs0001">Royal Society International Exchanges</funding-source> Cost Share Award, UK (RP202G0230); <funding-source id="gs0002">Medical Research Council Confidence</funding-source> in Concept Award, UK (MC_PC_17171); <funding-source id="gs0003">Hope Foundation for Cancer Research</funding-source>, UK (RM60G0680); <funding-source id="gs0004">British Heart Foundation Accelerator Award</funding-source>, UK; the <funding-source id="gs0005"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100010198</institution-id><institution>MINECO</institution></institution-wrap></funding-source>/ <funding-source id="gs0006">FEDER</funding-source> under the RTI2018-098913-B100 and A-TIC-080-UGR18 projects; <funding-source id="gs0007">FPU predoctoral</funding-source> grant (FPU 18/04902) from <funding-source id="gs0008">Ministerio de Universidades, Spain</funding-source>; <funding-source id="gs0009"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source> (CDLS-2020-03); Key Laboratory of Child Development and Learning Science (Southeast University), <funding-source id="gs0010"><institution-wrap><institution-id institution-id-type="doi">10.13039/100009950</institution-id><institution>Ministry of Education</institution></institution-wrap></funding-source>; <funding-source id="gs0011">Guangxi Key Laboratory of Trusted Software</funding-source> (kx201901).</p>
</ack>
</back>
</article>
</pmc-articleset>