<?xml version="1.0" ?>
<!DOCTYPE pmc-articleset PUBLIC "-//NLM//DTD ARTICLE SET 2.0//EN" "https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd">

<pmc-articleset><article article-type="research-article" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<?properties open_access?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">Genomics Proteomics Bioinformatics</journal-id>
<journal-id journal-id-type="iso-abbrev">Genomics Proteomics Bioinformatics</journal-id>
<journal-title-group>
<journal-title>Genomics, Proteomics &amp; Bioinformatics</journal-title>
</journal-title-group>
<issn pub-type="ppub">1672-0229</issn>
<issn pub-type="epub">2210-3244</issn>
<publisher>
<publisher-name>Elsevier</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="pmid">32035227</article-id>
<article-id pub-id-type="pmc">7056933</article-id>
<article-id pub-id-type="publisher-id">S1672-0229(20)30004-8</article-id>
<article-id pub-id-type="doi">10.1016/j.gpb.2019.04.003</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original Research</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>DeepCPI: A Deep Learning-based Framework for Large-scale <italic>in silico</italic> Drug Screening</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" id="au005">
<name>
<surname>Wan</surname>
<given-names>Fangping</given-names>
</name>
<xref ref-type="aff" rid="af005">1</xref>
<xref ref-type="fn" rid="fn1">#</xref>
<xref ref-type="fn" rid="fn2">a</xref>
</contrib>
<contrib contrib-type="author" id="au010">
<name>
<surname>Zhu</surname>
<given-names>Yue</given-names>
</name>
<xref ref-type="aff" rid="af010">2</xref>
<xref ref-type="fn" rid="fn1">#</xref>
<xref ref-type="fn" rid="fn3">b</xref>
</contrib>
<contrib contrib-type="author" id="au015">
<name>
<surname>Hu</surname>
<given-names>Hailin</given-names>
</name>
<xref ref-type="aff" rid="af015">3</xref>
<xref ref-type="fn" rid="fn1">#</xref>
<xref ref-type="fn" rid="fn4">c</xref>
</contrib>
<contrib contrib-type="author" id="au020">
<name>
<surname>Dai</surname>
<given-names>Antao</given-names>
</name>
<xref ref-type="aff" rid="af010">2</xref>
<xref ref-type="fn" rid="fn5">d</xref>
</contrib>
<contrib contrib-type="author" id="au025">
<name>
<surname>Cai</surname>
<given-names>Xiaoqing</given-names>
</name>
<xref ref-type="aff" rid="af010">2</xref>
<xref ref-type="fn" rid="fn6">e</xref>
</contrib>
<contrib contrib-type="author" id="au030">
<name>
<surname>Chen</surname>
<given-names>Ligong</given-names>
</name>
<xref ref-type="aff" rid="af020">4</xref>
<xref ref-type="fn" rid="fn7">f</xref>
</contrib>
<contrib contrib-type="author" id="au035">
<name>
<surname>Gong</surname>
<given-names>Haipeng</given-names>
</name>
<xref ref-type="aff" rid="af025">5</xref>
<xref ref-type="fn" rid="fn8">g</xref>
</contrib>
<contrib contrib-type="author" id="au040">
<name>
<surname>Xia</surname>
<given-names>Tian</given-names>
</name>
<xref ref-type="aff" rid="af030">6</xref>
<xref ref-type="fn" rid="fn9">h</xref>
</contrib>
<contrib contrib-type="author" id="au045">
<name>
<surname>Yang</surname>
<given-names>Dehua</given-names>
</name>
<email>dhyang@simm.ac.cn</email>
<xref ref-type="aff" rid="af010">2</xref>
<xref ref-type="corresp" rid="cor1">⁎</xref>
<xref ref-type="fn" rid="fn10">i</xref>
</contrib>
<contrib contrib-type="author" id="au050">
<name>
<surname>Wang</surname>
<given-names>Ming-Wei</given-names>
</name>
<email>mwwang@simm.ac.cn</email>
<xref ref-type="aff" rid="af010">2</xref>
<xref ref-type="aff" rid="af035">7</xref>
<xref ref-type="aff" rid="af040">8</xref>
<xref ref-type="corresp" rid="cor1">⁎</xref>
<xref ref-type="fn" rid="fn11">j</xref>
</contrib>
<contrib contrib-type="author" id="au055">
<name>
<surname>Zeng</surname>
<given-names>Jianyang</given-names>
</name>
<email>zengjy321@tsinghua.edu.cn</email>
<xref ref-type="aff" rid="af005">1</xref>
<xref ref-type="aff" rid="af045">9</xref>
<xref ref-type="corresp" rid="cor1">⁎</xref>
<xref ref-type="fn" rid="fn12">k</xref>
</contrib>
</contrib-group>
<aff id="af005"><label>1</label>Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China</aff>
<aff id="af010"><label>2</label>The National Center for Drug Screening and the CAS Key Laboratory of Receptor Research, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Shanghai 201203, China</aff>
<aff id="af015"><label>3</label>School of Medicine, Tsinghua University, Beijing 100084, China</aff>
<aff id="af020"><label>4</label>School of Pharmaceutical Sciences, Tsinghua University, Beijing 100084, China</aff>
<aff id="af025"><label>5</label>School of Life Science, Tsinghua University, Beijing 100084, China</aff>
<aff id="af030"><label>6</label>Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan 430074, China</aff>
<aff id="af035"><label>7</label>School of Life Science and Technology, ShanghaiTech University, Shanghai 201210, China</aff>
<aff id="af040"><label>8</label>Shanghai Medical College, Fudan University, Shanghai 200032, China</aff>
<aff id="af045"><label>9</label>MOE Key Laboratory of Bioinformatics, Tsinghua University, Beijing 100084, China</aff>
<author-notes>
<corresp id="cor1"><label>⁎</label>Corresponding authors. <email>dhyang@simm.ac.cn</email><email>mwwang@simm.ac.cn</email><email>zengjy321@tsinghua.edu.cn</email></corresp>
<fn id="fn1">
<label>#</label>
<p id="np010">Equal contribution.</p>
</fn>
<fn id="fn2">
<label>a</label>
<p id="np015">ORCID: 0000-0003-1647-3278.</p>
</fn>
<fn id="fn3">
<label>b</label>
<p id="np020">ORCID: 0000-0002-1106-1832.</p>
</fn>
<fn id="fn4">
<label>c</label>
<p id="np025">ORCID: 0000-0002-5768-4437.</p>
</fn>
<fn id="fn5">
<label>d</label>
<p id="np030">ORCID: 0000-0003-4241-5346.</p>
</fn>
<fn id="fn6">
<label>e</label>
<p id="np035">ORCID: 0000-0002-8995-4518.</p>
</fn>
<fn id="fn7">
<label>f</label>
<p id="np040">ORCID: 0000-0002-7893-7173.</p>
</fn>
<fn id="fn8">
<label>g</label>
<p id="np045">ORCID: 0000-0002-5532-1640.</p>
</fn>
<fn id="fn9">
<label>h</label>
<p id="np050">ORCID: 0000-0001-5984-9162.</p>
</fn>
<fn id="fn10">
<label>i</label>
<p id="np055">ORCID: 0000-0003-3028-3243.</p>
</fn>
<fn id="fn11">
<label>j</label>
<p id="np060">ORCID: 0000-0001-6550-9017.</p>
</fn>
<fn id="fn12">
<label>k</label>
<p id="np065">ORCID: 0000-0003-0950-7716.</p>
</fn>
</author-notes>
<pub-date pub-type="pmc-release">
<day>06</day>
<month>2</month>
<year>2020</year>
</pub-date>
<!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
<pub-date pub-type="ppub">
<month>10</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>06</day>
<month>2</month>
<year>2020</year>
</pub-date>
<volume>17</volume>
<issue>5</issue>
<fpage>478</fpage>
<lpage>495</lpage>
<history>
<date date-type="received">
<day>19</day>
<month>3</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>4</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-statement>© 2019 The Authors</copyright-statement>
<copyright-year>2019</copyright-year>
<license license-type="CC BY-NC-ND" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
<license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
</license>
</permissions>
<abstract id="ab005">
<p>Accurate identification of compound–protein interactions (CPIs) <italic>in silico</italic> may deepen our understanding of the underlying mechanisms of drug action and thus remarkably facilitate drug discovery and development. Conventional similarity- or docking-based computational methods for predicting CPIs rarely exploit latent features from currently available large-scale unlabeled compound and protein data and often limit their usage to relatively small-scale datasets. In the present study, we propose DeepCPI, a novel general and scalable computational framework that combines effective feature embedding (a technique of representation learning) with powerful deep learning methods to accurately predict CPIs at a large scale. DeepCPI automatically learns the implicit yet expressive low-dimensional features of compounds and proteins from a massive amount of unlabeled data. Evaluations of the measured CPIs in large-scale databases, such as ChEMBL and BindingDB, as well as of the known drug–target interactions from DrugBank, demonstrated the superior predictive performance of DeepCPI. Furthermore, several interactions among small-molecule compounds and three G protein-coupled receptor targets (glucagon-like peptide-1 receptor, glucagon receptor, and vasoactive intestinal peptide receptor) predicted using DeepCPI were experimentally validated. The present study suggests that DeepCPI is a useful and powerful tool for drug discovery and repositioning. The source code of DeepCPI can be downloaded from <ext-link ext-link-type="uri" id="ir005" xlink:href="https://github.com/FangpingWan/DeepCPI">https://github.com/FangpingWan/DeepCPI</ext-link>.</p>
</abstract>
<kwd-group id="kg005">
<title>Keywords</title>
<kwd>Deep learning</kwd>
<kwd>Machine learning</kwd>
<kwd>Drug discovery</kwd>
<kwd><italic>In silico</italic> drug screening</kwd>
<kwd>Compound–protein interaction prediction</kwd>
</kwd-group>
</article-meta>
<notes>
<p id="ms005">Handled by Yi Xing</p>
</notes>
</front>
<body>
<sec id="s0005">
<title>Introduction</title>
<p id="p0005">Identification of compound–protein interactions (CPIs; or drug–target interactions, DTIs) is crucial for drug discovery and development and provides valuable insights into the understanding of drug actions and off-target adverse events <xref ref-type="bibr" rid="b0005">[1]</xref>, <xref ref-type="bibr" rid="b0010">[2]</xref>. Inspired by the concept of polypharmacology, <italic>i.e.</italic>, a single drug may interact with multiple targets <xref ref-type="bibr" rid="b0015">[3]</xref>, drug developers are actively seeking novel ways to better characterize CPIs or identify novel uses of the existing drugs (<italic>i.e.</italic>, drug repositioning or drug repurposing) <xref ref-type="bibr" rid="b0015">[3]</xref>, <xref ref-type="bibr" rid="b0020">[4]</xref> to markedly reduce the time and cost required for drug development <xref ref-type="bibr" rid="b0025">[5]</xref>.</p>
<p id="p0010">Numerous computational methods have been proposed to predict potential CPIs <italic>in silico</italic> to narrow the large search space of possible interacting compound–protein pairs and facilitate drug discovery and development <xref ref-type="bibr" rid="b0030">[6]</xref>, <xref ref-type="bibr" rid="b0035">[7]</xref>, <xref ref-type="bibr" rid="b0040">[8]</xref>, <xref ref-type="bibr" rid="b0045">[9]</xref>, <xref ref-type="bibr" rid="b0050">[10]</xref>, <xref ref-type="bibr" rid="b0055">[11]</xref>, <xref ref-type="bibr" rid="b0060">[12]</xref>. Although successful results can be obtained using the existing prediction approaches, several challenges remain unaddressed. First, most of the conventional prediction methods only employ a simple and direct representation of features from the labeled data (<italic>e.g.</italic>, established CPIs and available protein structure information) to assess similarities among compounds and proteins and infer unknown CPIs. For instance, a kernel describing the similarities among drug–protein interaction profiles <xref ref-type="bibr" rid="b0040">[8]</xref> and the graph-based method SIMCOMP <xref ref-type="bibr" rid="b0065">[13]</xref> were used to compare different drugs and compounds. In addition, the normalized Smith–Waterman score <xref ref-type="bibr" rid="b0045">[9]</xref> is typically applied to assess the similarities among targets (proteins). Meanwhile, large amounts of available unlabeled data of compounds and proteins enable an implicit and useful representation of features that may effectively be used to define their similarities. Such an implicit representation of protein or compound features encoded by large-scale unlabeled data is not exploited well by most of the existing methods to predict new CPIs. Second, an increasing number of established DTIs or compound–protein-binding affinities (<italic>e.g.</italic>, 1 million bioassays over 2 million compounds and 10,000 protein targets in PubChem <xref ref-type="bibr" rid="b0070">[14]</xref>) raises serious scalability issues concerning the conventional prediction methods. For instance, many similarity-based methods <xref ref-type="bibr" rid="b0035">[7]</xref>, <xref ref-type="bibr" rid="b0045">[9]</xref> require the computation of pairwise similarity scores between proteins, which is generally impractical in the setting of large-scale data. The aforementioned computational challenges highlight the need of more efficient schemes to accurately capture the hidden features of proteins and compounds from massive unlabeled data as well as the need of more advanced and scalable learning models that enable predictions from large-scale training datasets.</p>
<p id="p0015">In machine learning communities, representation learning and deep learning (DL) are the two popular methods at present for efficiently extracting features and addressing the scalability issues in large-scale data analyses. Representation learning aims to automatically learn data representations (features) from relatively raw data that can be more effectively and easily exploited by the downstream machine learning models to improve the learning performance <xref ref-type="bibr" rid="b0075">[15]</xref>, <xref ref-type="bibr" rid="b0080">[16]</xref>. Meanwhile, DL aims to extract high-level feature abstractions from input data, typically using several layers of non-linear transformations, and is a dominant method used in numerous complex learning tasks with large-scale samples in the data science field, such as computer vision, speech recognition, natural language processing (NLP), game playing, and bioinformatics <xref ref-type="bibr" rid="b0085">[17]</xref>, <xref ref-type="bibr" rid="b0090">[18]</xref>, <xref ref-type="bibr" rid="b0095">[19]</xref>. Although several DL models have been used to address various learning problems in drug discovery <xref ref-type="bibr" rid="b0100">[20]</xref>, <xref ref-type="bibr" rid="b0105">[21]</xref>, <xref ref-type="bibr" rid="b0110">[22]</xref>, they rarely fully exploit the currently available large-scale protein and compound data to predict CPI. For example, the computational approaches proposed in the literature <xref ref-type="bibr" rid="b0100">[20]</xref>, <xref ref-type="bibr" rid="b0105">[21]</xref> only use the hand-designed features of compounds and do not take into account the features of targets. Furthermore, these approaches generally fail to predict potential interacting compounds for a given novel target (<italic>i.e.</italic>, without known interacting compounds in the training data); this type of prediction is generally more urgent than the prediction of novel compounds for targets with known interacting compounds. Although a new approach—AtomNet—has been developed <xref ref-type="bibr" rid="b0115">[23]</xref> to overcome these limitations, it can only be used to predict interacting drug partners of targets with known structures, which is often not the case in the clinical practice. In addition, despite the promising predictive performance of conventional approaches reported on benchmark datasets <xref ref-type="bibr" rid="b0120">[24]</xref>, <xref ref-type="bibr" rid="b0125">[25]</xref>, <xref ref-type="bibr" rid="b0130">[26]</xref>, few efforts have been made to explore the extent to which these advanced learning techniques can promote the efficiency in the real drug discovery scenario.</p>
<p id="p0020">In this article, we propose DeepCPI, a novel framework that combines unsupervised representation learning with powerful DL techniques for predicting structure-free CPIs. DeepCPI first uses the latent semantic analysis <xref ref-type="bibr" rid="b0135">[27]</xref> and Word2vec <xref ref-type="bibr" rid="b0080">[16]</xref>, <xref ref-type="bibr" rid="b0140">[28]</xref>, <xref ref-type="bibr" rid="b0145">[29]</xref> methods to learn the feature embeddings (<italic>i.e.</italic>, low-dimensional feature representations) of compounds and proteins in an unsupervised manner from large compound and protein corpora, respectively. Subsequently, given a compound–protein pair, the feature embeddings of both compound and protein are fed into a multimodal deep neural network (DNN) classifier to predict their interaction probability. We tested DeepCPI on several benchmark datasets, including the large-scale compound–protein affinity databases (<italic>e.g.</italic>, ChEMBL and BindingDB), as well as the known DTIs from DrugBank. Comparisons with several conventional methods demonstrated the superior performance of DeepCPI in numerous practical scenarios. Moreover, starting from the virtual screening initialized by DeepCPI, we identified several novel interactions of small-molecule compounds with various targets in the G protein-coupled receptor (GPCR) family, including glucagon-like peptide-1 receptor (GLP-1R), glucagon receptor (GCGR), and vasoactive intestinal peptide receptor (VIPR). Collectively, our computational test and laboratory experimentation results demonstrate that DeepCPI is a useful and powerful tool for the prediction of novel CPIs and can thus aid in drug discovery and repositioning endeavors.</p>
</sec>
<sec id="s0010">
<title>Results</title>
<sec id="s0015">
<title>DeepCPI framework</title>
<p id="p0025">The DeepCPI framework comprises two main steps (<xref ref-type="fig" rid="f0005">Figure 1</xref>): (1) representation learning for both compounds and proteins and (2) predicting CPIs (or DTIs) through a multimodal DNN. More specifically, in the first step, we use several NLP techniques to extract the useful features of compounds and proteins from the corresponding large-scale unlabeled corpora (<xref ref-type="sec" rid="s0165">Figures S1 and S2</xref>; Materials and methods). Here, compounds and their basic structures are regarded as “documents” and “words”, respectively, whereas protein sequences and all possible three non-overlapping amino acid residues are regarded as “sentences” and “words,” respectively. Subsequently, the feature embedding techniques, including latent semantic analysis <xref ref-type="bibr" rid="b0135">[27]</xref> and Word2vec <xref ref-type="bibr" rid="b0080">[16]</xref>, <xref ref-type="bibr" rid="b0140">[28]</xref>, are applied to automatically learn the implicit yet expressive low-dimensional representations (<italic>i.e.</italic>, vectors) of compound and protein features from the corresponding large-scale unlabeled corpora. In the second step, the derived low-dimensional feature vectors of compounds and proteins are fed into a multimodal DNN classifier to make the predictions. Further details of the individual modules of DeepCPI, including the extraction of compound and protein features, DNN model, and implementation procedure, are described in Materials and methods.<fig id="f0005"><label>Figure 1</label><caption><p><bold>Schematic of the DeepCPI workflow</bold> First, motivated by the current NLP techniques, the unsupervised representation learning strategies (including latent semantic analysis and Word2vec) are used to obtain low-dimensional representations of compound and protein features from massive unlabeled data. Subsequently, these extracted low-dimensional feature representations of compounds and proteins are fed to a multimodal DNN to make the prediction. NLP, natural language processing; DNN, deep neural network.</p></caption><graphic xlink:href="gr1"></graphic></fig></p>
</sec>
<sec id="s0020">
<title>Predictive performance evaluation</title>
<p id="p0030">We mainly evaluated DeepCPI using compound–protein pairs extracted from the currently available databases, such as ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref> and BindingDB <xref ref-type="bibr" rid="b0155">[31]</xref>. We first used the bioactivity data retrieved from ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref> to assess the predictive performance of DeepCPI. Specifically, the compound–protein pairs with half maximal inhibitory concentrations (<inline-formula><mml:math altimg="si1.svg" id="M1"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula>) or inhibition constants (<inline-formula><mml:math altimg="si2.svg" id="M2"><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) <inline-formula><mml:math altimg="si3.svg" id="M3"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> were selected as positive examples, whereas pairs with <inline-formula><mml:math altimg="si1.svg" id="M4"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math altimg="si4.svg" id="M5"><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>30</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> were used as negative examples. This data preprocessing step yielded 360,867 positive examples and 93,925 negative examples. To justify our criteria of selecting positive and negative examples, we mapped the known interacting drug–target pairs extracted from DrugBank <xref ref-type="bibr" rid="b0160">[32]</xref> (released on November 11, 2015) to the corresponding compound–protein pairs in ChEMBL (Materials and methods). The binding affinities or potencies (measured by <inline-formula><mml:math altimg="si1.svg" id="M6"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math altimg="si2.svg" id="M7"><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>) of majority of the known interacting drug–target pairs were <inline-formula><mml:math altimg="si3.svg" id="M8"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> (&gt;60% and 70% pairs for <inline-formula><mml:math altimg="si1.svg" id="M9"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math altimg="si2.svg" id="M10"><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, respectively) (<xref ref-type="sec" rid="s0165">Figure S3</xref>). Reportedly, <inline-formula><mml:math altimg="si5.svg" id="M11"><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> is a widely-used and good indicator of strong binding affinities among compounds and proteins <xref ref-type="bibr" rid="b0165">[33]</xref>. Therefore, we considered <inline-formula><mml:math altimg="si1.svg" id="M12"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math altimg="si6.svg" id="M13"><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> as a reasonable criterion for selecting positive examples. There is no well-defined dichotomy between high and low binding affinities; thus, we used a threshold of <inline-formula><mml:math altimg="si7.svg" id="M14"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>30</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> (<italic>i.e.</italic>, markedly higher than <inline-formula><mml:math altimg="si8.svg" id="M15"><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula>) to select negative examples, which is consistent with the method reported elsewhere <xref ref-type="bibr" rid="b0115">[23]</xref>.</p>
<p id="p0035">To evaluate the predictive performance of DeepCPI, we considered several challenging and realistic scenarios. A computational experiment was first conducted in which we randomly selected 20% pairs from ChEMBL as the training data and the remaining pairs as the test data. This scenario mimicked a practical situation in which CPIs are relatively sparsely labeled. ChEMBL may contain similar (redundant) proteins and compounds, which may lead to over-optimistic performance resulting from easy predictions. Therefore, we minimized this effect by only retaining proteins whose sequence identity scores were <inline-formula><mml:math altimg="si9.svg" id="M16"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.40</mml:mn></mml:mrow></mml:math></inline-formula> and compounds whose chemical structure similarity scores were <inline-formula><mml:math altimg="si10.svg" id="M17"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.55</mml:mn></mml:mrow></mml:math></inline-formula> (as computed based on the Jaccard similarity between their Morgan fingerprints). More specifically, for each group of proteins or compounds with sequence identity scores <inline-formula><mml:math altimg="si11.svg" id="M18"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.40</mml:mn></mml:mrow></mml:math></inline-formula> or chemical structure similarity scores <inline-formula><mml:math altimg="si12.svg" id="M19"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.55</mml:mn></mml:mrow></mml:math></inline-formula>, we only retained the protein or compound having the highest number of interactions and discarded the rest of the proteins or compounds in that group. The basic statistics of the ChEMBL and BindingDB datasets used in our performance evaluation are summarized in <xref ref-type="sec" rid="s0165">Tables S1 and S2</xref>, respectively.</p>
<p id="p0040">Conventional cross validation, particularly leave-one-out cross validation (LOOCV), may not be an appropriate method to evaluate the performance of a CPI prediction algorithm, if the training data contain many compounds or proteins with only one interaction <xref ref-type="bibr" rid="b0170">[34]</xref>. In such a case, training methods may learn to exploit the bias toward the proteins or compounds with a single interaction to boost the performance of LOOCV. Thus, separating the single interaction from other types of interactions during cross validation is essential <xref ref-type="bibr" rid="b0170">[34]</xref>. Given a compound–protein interacting pair from a dataset, if the compound or protein only appeared in this interaction, we considered this pair as unique (Materials and methods). In this test scenario, we used non-unique examples as the training data and tested the predictive performance on unique pairs.</p>
<p id="p0045">In all computational tests, three baseline methods were used for comparisons (Materials and methods). The first two were a random forest and a single-layer neural network (SLNN) with our feature extraction schemes. These were used to demonstrate the need for the DNN model. The third one was a DNN with conventional features (<italic>i.e.</italic>, Morgan fingerprints <xref ref-type="bibr" rid="b0175">[35]</xref> with a radius of three for compounds and pairwise Smith–Waterman scores for proteins in the training data), which was used to demonstrate the need for our feature embedding methods. Moreover, we compared DeepCPI with two state-of-the-art network-based DTI prediction methods—DTINet <xref ref-type="bibr" rid="b0060">[12]</xref> and NetLapRLS <xref ref-type="bibr" rid="b0050">[10]</xref> (Materials and methods)—in a setting where redundant proteins and compounds were removed; these two methods were not used in other scenarios (<xref ref-type="sec" rid="s0165">Figure S4</xref>) as the cubic time and quadratic space complexities concerning the large number of compounds exceeded the limit of our server. We observed that DeepCPI significantly outperformed the network-based methods (<xref ref-type="fig" rid="f0010">Figure 2</xref>A–D). Compared to these two network-based methods, DeepCPI achieved better time and space complexities (Materials and methods), demonstrating its superiority over network-based frameworks when handling large-scale data. In addition, DeepCPI outperformed the other three baseline methods (<xref ref-type="fig" rid="f0010">Figure 2</xref>A–D and <xref ref-type="sec" rid="s0165">Figure S4</xref>) and exhibited a better prediction accuracy and generalization ability of the combination of DL and our feature extraction schemes.<fig id="f0010"><label>Figure 2</label><caption><p><bold>Performance evaluation of DeepCPI</bold></p><p>Performance of DeepCPI was evaluated in terms of AUROC and AUPRC using different training dataset and test dataset in comparison with other state-of-the-art methods. The methods for comparison include network-based NetLapRLS and DTINet, SLNN, DNN with conventional features, and random forest. Comparison of AUROC (<bold>A</bold>) and AUPRC (<bold>B</bold>) across different methods that were trained on 20% randomly selected compound–protein pairs from ChEMBL and tested on the remaining pairs from ChEMBL. Comparison of AUROC (<bold>C</bold>) and AUPRC (<bold>D</bold>) across different methods that were trained on non-unique pairs from ChEMBL and tested on unique pairs from ChEMBL. Comparison of AUROC (<bold>E</bold>) and AUPRC (<bold>F</bold>) across different methods that were trained on ChEMBL data and tested on BindingDB data (in which the overlapping compound–protein pairs were removed). In all cases, the redundant proteins and compounds were removed. The results are summarized over 10 trials and expressed as means ± SD. SLNN, single-layer neural network; DNN, deep neural network; AUROC, area under receiver operating characteristic curve; AUPRC, area under precision-recall curve.</p></caption><graphic xlink:href="gr2"></graphic></fig></p>
<p id="p0050">Furthermore, we conducted two supplementary tests to assess the predictive performance of DeepCPI on BindingDB (<xref ref-type="sec" rid="s0165">Tables S1 and S2</xref>). BindingDB stores the binding affinities of proteins and drug-like small molecules <xref ref-type="bibr" rid="b0155">[31]</xref> using the same criteria (<italic>i.e.</italic>, <inline-formula><mml:math altimg="si1.svg" id="M20"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math altimg="si13.svg" id="M21"><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> for positive examples and <inline-formula><mml:math altimg="si7.svg" id="M22"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>30</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> for negative examples) to label compound–protein pairs. The compound–protein pairs derived from ChEMBL and BindingDB were employed as the training and test data, respectively. Compound–protein pairs from BindingDB exhibiting a compound chemical structure similarity score of <inline-formula><mml:math altimg="si12.svg" id="M23"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.55</mml:mn></mml:mrow></mml:math></inline-formula> and a protein sequence identity score of <inline-formula><mml:math altimg="si11.svg" id="M24"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.40</mml:mn></mml:mrow></mml:math></inline-formula> compared with any compound–protein pair from ChEMBL were regarded as overlaps and removed from the test data. The evaluation results on the BindingDB dataset demonstrated that DeepCPI outperformed all of the baseline methods (<xref ref-type="fig" rid="f0010">Figure 2</xref>E and F; <xref ref-type="sec" rid="s0165">Figure S4</xref>). Collectively, these data support the strong generalization ability of DeepCPI.</p>
<p id="p0055">We subsequently investigated the extraction of high-level feature abstractions from the input data using the DNN. We applied T-distributed stochastic neighbor embedding (t-SNE) <xref ref-type="bibr" rid="b0180">[36]</xref> to visualize and compare the distributions of positive and negative examples with their original 300-dimensional input features and the latent features represented by the last hidden layer in DNN. In this study, DNN was trained on ChEMBL, and a combination of 5000 positive and 5000 negative examples randomly selected from BindingDB was used as the test data. Visualization (<xref ref-type="sec" rid="s0165">Figure S5</xref>) showed that the test data were better organized using DNN. Consequently, the final output layer (which was simply a logistic regression classifier) can more easily exploit hidden features to yield better classification results.</p>
<p id="p0060">Finally, we compared the performance of DeepCPI with those of the following two DL-based models: AtomNet (a structure-based DL approach for predicting compound–protein binding potencies) <xref ref-type="bibr" rid="b0115">[23]</xref> and DeepDTI <xref ref-type="bibr" rid="b0120">[24]</xref> (a deep belief network-based model with molecule fingerprints and protein k-mer frequencies as input features) (Materials and methods). Specifically, we compared DeepCPI with AtomNet in terms of the directory of useful decoys from DUD-E <xref ref-type="bibr" rid="b0185">[37]</xref>. DUD-E is a widely used benchmark dataset for evaluating molecular docking programs and contains active compounds against 102 targets (<xref ref-type="sec" rid="s0165">Table S3</xref>). Each active compound in DUD-E is also paired with several decoys that share similar physicochemical properties but have dissimilar two-dimensional topologies, under the assumption that such dissimilarity in the compound structure results in different pharmacological activities with high probability.</p>
<p id="p0065">We adopted two test settings as reported previously <xref ref-type="bibr" rid="b0115">[23]</xref> to evaluate the performance of different prediction approaches using DUD-E. In the first setting, cross validation was performed on 102 proteins, <italic>i.e.</italic>, the data were separated according to proteins. In the second setting, cross validation was performed for all pairs, <italic>i.e.</italic>, all compound–protein pairs were divided into three groups for validation. In addition to AtomNet, we also compared our method with a random forest model.</p>
<p id="p0070">The tests on DUD-E showed that DeepCPI outperformed both the random forest model and AtomNet in the aforementioned settings (<xref ref-type="sec" rid="s0165">Table S4</xref>). In addition, in the second setting, our protein structure-free feature extraction schemes with the random forest model also greatly outperformed AtomNet, which requires protein structures and uses a convolutional neural network classifier. These observations further demonstrate the superiority of our feature extraction schemes. DeepCPI achieved a significantly larger area under receiver operating characteristic curve (AUROC) than the random forest model in the first test setting. The first setting was generally more stringent than the second one as protein information was not visible to classifiers during cross validation. Thus, this result indicates that DeepCPI has better generalization ability than the random forest model.</p>
<p id="p0075">DeepDTI <xref ref-type="bibr" rid="b0120">[24]</xref> requires high-dimensional features (14,564 features) as the input data; therefore, it can only be used for analyzing small-scale datasets. Thus, we mainly compared DeepCPI with DeepDTI using the 6262 DTIs provided by the original DeepDTI article <xref ref-type="bibr" rid="b0120">[24]</xref>. We applied the same evaluation strategy as that applied in DeepDTI by randomly sampling the same number (6262) of unknown DTIs as negative examples and splitting the data into the training (60%), validation (20%), and test (20%) data. Our comparison showed that even on this small-scale dataset, DeepCPI continued to achieve a larger mean AUROC (0.9220) than DeepDTI (0.9158) (<xref ref-type="sec" rid="s0165">Table S5</xref>). Therefore, we believe that DeepCPI is superior to DeepDTI in terms of both predictive performance and scalability to large-scale compound affinity data.</p>
</sec>
<sec id="s0025">
<title>Novel interaction prediction</title>
<p id="p0080">All known DTI data obtained from DrugBank <xref ref-type="bibr" rid="b0160">[32]</xref> (released on November 11, 2015) were used to train DeepCPI. The novel prediction results on the missing interactions (<italic>i.e.</italic>, the drug–target pairs that did not have an established interaction record in DrugBank) were then examined. Most of the top predictions with the highest scores could be supported by the evidence available in the literature. For example, among the list of the top 100 predictions, 71 novel DTIs were consistent with those reported in previous studies (<xref ref-type="sec" rid="s0165">Table S6</xref>). <xref ref-type="fig" rid="f0015">Figure 3</xref> presents the visualization of a DTI network comprising the top 200 predictions using DeepCPI as well as the network of the 71 aforementioned novel DTIs.<fig id="f0015"><label>Figure 3</label><caption><p><bold>Network visualization of the novel DTIs predicted using DeepCPI</bold></p><p>DeepCPI was trained on DrugBank data and used to predict novel DTIs (not recorded in DrugBank). <bold>A.</bold> A network comprising the top 200 novel DTI predictions. <bold>B.</bold> A network comprising 71 novel DTIs with literature support among the top 100 DTI predictions. Purple and yellow circles represent targets and drugs, respectively. Gray lines represent the known interactions derived from DrugBank, and red dashed lines represent the novel interactions predicted using DeepCPI. DTI, drug–target interaction.</p></caption><graphic xlink:href="gr3"></graphic></fig></p>
<p id="p0085">We describe several examples of these novel predictions supported by the literature below (<xref ref-type="sec" rid="s0165">Table S6</xref>). Specifically, in addition to the known DTIs recorded in DrugBank, DeepCPI revealed several novel interactions in neural pharmacology. These interactions may provide new direction to further decipher the complex biological processes in the treatment of neural disorders. For instance, dopamine, a catecholamine neurotransmitter with a high binding affinity for dopamine receptors (DRs), was predicted by DeepCPI to also interact with the α2 adrenergic receptor (ADRA2A). Such a prediction representing a crosstalk within the evolutionally related catecholaminergic systems is supported by the known function of dopamine acting as a weak agonist of ADRA2A <xref ref-type="bibr" rid="b0190">[38]</xref> as well as the evidence from multiple previous animal studies <xref ref-type="bibr" rid="b0195">[39]</xref>, <xref ref-type="bibr" rid="b0200">[40]</xref>. Besides the intrinsic neurotransmitters, our prediction results involved various interesting interactions between other types of drugs and their novel binding partners. For instance, amitriptyline, a dual inhibitor of norepinephrine and serotonin reuptake, is commonly used to treat major depression and anxiety. Our predictions indicated that amitriptyline can also interact with three DR isoforms, including DRD1, DRD2, and DRD3. This result is supported by previous evidence, suggesting that amitriptyline displays binding to all three DR isoforms at sub-micromolar potencies <xref ref-type="bibr" rid="b0205">[41]</xref>.</p>
<p id="p0090">While these antagonist potencies are relatively weak compared with those of other targets (<italic>e.g.</italic>, solute carrier family 6 member 2 <xref ref-type="bibr" rid="b0070">[14]</xref> and histamine H1 receptor <xref ref-type="bibr" rid="b0210">[42]</xref>), this new predicted interaction may offer expansion in the chemical space of antipsychotics <xref ref-type="bibr" rid="b0205">[41]</xref> and the treatment of autism <xref ref-type="bibr" rid="b0215">[43]</xref>. Moreover, DeepCPI predicted that oxazepam, an intermediate-acting benzodiazepine widely used in the control of alcohol withdrawal symptoms, can also act on the translocator protein, an important factor involved in intromitochondrial cholesterol transfer. This prediction is also supported by previous data from radioligand binding assays <xref ref-type="bibr" rid="b0220">[44]</xref> as well as by the observation that translocator protein is responsible for the oxazepam-induced reduction of methamphetamine in rats <xref ref-type="bibr" rid="b0225">[45]</xref>.</p>
<p id="p0095">In addition to providing novel indications in neural pharmacology, our predictions showed that polythiazide, a commonly used diuretic, can act on carbonic anhydrases. This predicted interaction, which may be related to the antihypertensive function of polythiazide <xref ref-type="bibr" rid="b0190">[38]</xref>, is supported by the evidence that polythiazide serves as a carbonic anhydrase inhibitor <italic>in vivo</italic>
<xref ref-type="bibr" rid="b0230">[46]</xref>. Another important branch of novel interaction predictions exemplified by an enzyme–substrate interaction between desipramine and cytochrome CYP2D6 highlighted a potential novel indication predicted by DeepCPI from a pharmacokinetics perspective. Indeed, the predicted interaction between desipramine and CYP2D6 is supported by their established metabolic association <xref ref-type="bibr" rid="b0235">[47]</xref>, <xref ref-type="bibr" rid="b0240">[48]</xref>, thus offering important clinical implications in drug–drug interactions <xref ref-type="bibr" rid="b0245">[49]</xref>. Overall, the novel DTIs predicted by DeepCPI and supported by experimental or clinical evidence in the literature further demonstrate the strong predictive performance of DeepCPI.</p>
</sec>
<sec id="s0030">
<title>Validation by experimentation</title>
<p id="p0100">As 30%–40% of the marketed drugs target GPCRs <xref ref-type="bibr" rid="b0250">[50]</xref>, <xref ref-type="bibr" rid="b0255">[51]</xref>, we applied DeepCPI to identify compounds acting on this class of drug targets. In this experiment, we used positive and negative examples from ChEMBL and BindingDB as well as the compound–protein pairs with <inline-formula><mml:math altimg="si14.svg" id="M25"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> affinities in ZINC15 <xref ref-type="bibr" rid="b0260">[52]</xref>, <xref ref-type="bibr" rid="b0265">[53]</xref>, <xref ref-type="bibr" rid="b0270">[54]</xref> as the training data for DeepCPI. Briefly, we predicted potential interacting compounds using a dataset obtained from the Chinese National Compound Library (CNCL; <ext-link ext-link-type="uri" id="ir010" xlink:href="http://www.cncl.org.cn/">http://www.cncl.org.cn/</ext-link>, containing 758,723 small molecules) with three class B GPCRs (GLP-1R, GCGR, and VIPR) involved in metabolic disorders and hypertension <xref ref-type="bibr" rid="b0275">[55]</xref>, <xref ref-type="bibr" rid="b0280">[56]</xref>. These proteins are challenging drug targets, particularly for the development of small-molecule modulators. For each GPCR target, we ran the trained DeepCPI model on the CNCL dataset and selected the top 100 predictions with the highest confidence scores for experimental validation as detailed below.</p>
<sec id="s0035">
<title>Pilot screening</title>
<p id="p0105">We first conducted several pilot screening assays as an initial experimental validation step to verify the top 100 compounds that were predicted by DeepCPI to act on the aforementioned three GPCRs. For GLP-1R, a whole-cell competitive binding assay was used to examine the effects of potential positive allosteric modulators (PAMs) (<xref ref-type="fig" rid="f0020">Figure 4</xref>A; Materials and methods). For GCGR and VIPR, a cAMP accumulation assay was conducted to evaluate the agonistic and antagonistic activities of the predicted compounds (<xref ref-type="fig" rid="f0020">Figure 4</xref>B–E). A total of six putative ligands showed a significant augmentation of radiolabeled GLP-1 binding compared with DMSO control, <italic>i.e.</italic>, within the top 25% quantile of the maximum response (<xref ref-type="fig" rid="f0020">Figure 4</xref>A). Moreover, we discovered putative small-molecule ligands acting on GCGR and VIPR (<xref ref-type="fig" rid="f0020">Figure 4</xref>B–E). Among these, nine compounds exhibited significant antagonistic effects against GCGR (with 7% cAMP inhibition; <xref ref-type="fig" rid="f0020">Figure 4</xref>C), while one compound exhibited an obvious agonistic effect on VIPR (with 20% cAMP increase; <xref ref-type="fig" rid="f0020">Figure 4</xref>D). Thus, these hits were selected for further validation.<fig id="f0020"><label>Figure 4</label><caption><p><bold>Pilot screening of the predicted compounds acting on GLP-1R, GCGR, and VIPR</bold></p><p><bold>A.</bold> A whole-cell competitive ligand binding assay was used to assess the effects of potential PAMs on GLP-1R. GLP-1R was stably expressed in FlpIn-CHO cells, and the effects of the top 100 predicted compounds (10 µM) were studied using the binding assay. Compared to DMSO, six compounds (indicated by arrows) showed obvious enhancement of radiolabeled GLP-1 binding to GLP-1R (within the top 25% quantile of the maximum response). <bold>B.</bold> Agonist validation for GCGR. GCGR-expressing cells were exposed to 20 µM compounds, and data were normalized with 10 nM glucagon (100%). <bold>C.</bold> Antagonist validation for GCGR. GCGR-expressing cells were treated with 0.01 nM glucagon and 20 µM compounds, and data were normalized to the maximal response elicited by 0.01 nM glucagon. Nine compounds (indicated by arrows) showed antagonist effects on cAMP accumulation (&gt;7% inhibition). <bold>D.</bold> Agonist validation for VIPR. CHO cells transfected with VIPR were incubated with 20 µM compounds for 40 min. Data were normalized to maximal response elicited by 10 µM VIP. CD3349-F005 showed a visible agonist effect on cAMP accumulation (&gt;20% increase). <bold>E.</bold> Antagonist validation for VIPR. VIP and compounds were added at the concentration of 10 nM and 20 µM, respectively, and data were normalized to maximal response elicited by 10 nM VIP. For all panels, the compounds selected for further validation are labeled with CNCL identification numbers. GLP-1R, glucagon-like peptide-1 receptor; GCGR, glucagon receptor; VIP, vasoactive intestinal peptide receptor; VIPR, VIP receptor; PAM, positive allosteric modulator; CNCL, Chinese National Compound Library.</p></caption><graphic xlink:href="gr4"></graphic></fig></p>
</sec>
<sec id="s0040">
<title>Confirmation of PAMs of GLP-1R</title>
<p id="p0110">The six putative hits were examined for their binding to GLP-1R. Of these, three (JK0580-H009, CD3293-E005, and CD3848-F005; <xref ref-type="sec" rid="s0165">Figure S6</xref>) showed significant enhancement of GLP-1 binding to GLP-1R (<xref ref-type="fig" rid="f0025">Figure 5</xref>A). Their corresponding dose–response curves exhibited obvious positive allosteric effects, with half maximal effective concentration (<inline-formula><mml:math altimg="si15.svg" id="M26"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values within the low micromolar range (<inline-formula><mml:math altimg="si16.svg" id="M27"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>10</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula>; <xref ref-type="fig" rid="f0025">Figure 5</xref>B). To test the specificity of the three compounds, we investigated their binding ability to GCGR, a homolog of GLP-1R. These compounds did not cross-react with GCGR (<xref ref-type="fig" rid="f0025">Figure 5</xref>C) but substantially promoted intracellular cAMP accumulation in the presence of GLP-1 (<xref ref-type="fig" rid="f0025">Figure 5</xref>D). Collectively, these results suggest that JK0580-H009, CD3293-E005, and CD3848-F005 are PAMs of GLP-1R.<fig id="f0025"><label>Figure 5</label><caption><p><bold>Experimental validation of the putative PAMs of GLP-1R</bold></p><p><bold>A.</bold> A receptor binding assay was used to validate the six hit compounds shown in <xref ref-type="fig" rid="f0020">Figure 4</xref>A. JK0580-H009, CD3878-F005, and CD3293-E005 displayed stable and significant enhancement of <sup>125</sup>I-GLP-1 binding to GLP-1R (<italic>P</italic> &lt; 0.0001). <bold>B.</bold> Dose–response characteristics of JK0580-H009, CD3878-F005, and CD3293-E005 in terms of the binding of <sup>125</sup>I labelled GLP-1 (40 pM) to GLP-1R. <bold>C.</bold> Validation of specificity for compounds JK0580-H009, CD3878-F005, and CD3293-E005 using cells stably expressing GCGR. Compared to DMSO, there was no significant effect of the putative ligands on <sup>125</sup>I-glucagon binding to GCGR. <bold>D.</bold> Effects of compounds JK0580-H009, CD3878-F005, and CD3293-E005 on cAMP accumulation. All compounds showed dose-dependent augmentation of intracellular cAMP levels in the presence of GLP-1. All measurements were performed with at least three independent experiments, and data are shown as mean ± SEM. ***, <italic>P</italic> &lt; 0.0001 (one-way ANOVA followed by Dunnett’s post-test; in comparison to DMSO treatment).</p></caption><graphic xlink:href="gr5"></graphic></fig></p>
<p id="p0115">To explore possible binding modes of these new PAMs, we conducted molecular docking studies using AutoDock Vina <xref ref-type="bibr" rid="b0285">[57]</xref> based on the high-resolution three-dimensional active structure of GLP-1R <xref ref-type="bibr" rid="b0290">[58]</xref> (<xref ref-type="fig" rid="f0030">Figures 6</xref> and <xref ref-type="sec" rid="s0165">Figure S6</xref>). We first used NNC0640 (<xref ref-type="sec" rid="s0165">Figure S6</xref>), a negative allosteric modulator of GLP-1R <xref ref-type="bibr" rid="b0295">[59]</xref>, as a control to demonstrate that our docking approach could recover the experimentally solved complex structure (<xref ref-type="sec" rid="s0165">Figure S7</xref>). Interestingly, our docking results indicated that the binding pocket for the predicted PAMs are located between transmembrane helix 5 (TM5) and TM6 of GLP-1R, which are distinct from that of NNC0640 <bold>(</bold><xref ref-type="fig" rid="f0030">Figure 6</xref>) and consistent with the enlarged cavity in the active form of GLP-1R (<xref ref-type="sec" rid="s0165">Figure S8</xref>). Additionally, the docking results suggest that the binding sites of our predicted PAMs are located deeper inside the transmembrane domain of GLP-1R than that of the known covalently bound PAMs, including Compound 2 <xref ref-type="bibr" rid="b0295">[59]</xref> and 4-(3-(benzyloxy)phenyl)-2-ethylsulfinyl-6-(trifluoromethyl)pyrimidine (BETP) <xref ref-type="bibr" rid="b0300">[60]</xref>, <xref ref-type="bibr" rid="b0305">[61]</xref> (<xref ref-type="fig" rid="f0030">Figures 6</xref>A and S7). These findings reveal a novel route for discovering and designing new PAMs of GLP-1R. To further analyze these docking results, we produced four stable cell lines expressing mutant GLP-1Rs (C347F, T149A, T355A, and I328N). As a control, we first measured the activity of BETP, which is covalently bonded to C347 in GLP-1R. Consistent with the previous findings <xref ref-type="bibr" rid="b0295">[59]</xref>, T149A mutation diminished the binding between <sup>125</sup>I-GLP-1 and GLP-1R, which could be restored by BETP treatment (<xref ref-type="fig" rid="f0035">Figure 7</xref>). Meanwhile, C347F mutation eliminated the covalent anchor of BETP and reduced its efficacy compared with that of wild-type GLP-1R (<xref ref-type="fig" rid="f0035">Figure 7</xref>).<fig id="f0030"><label>Figure 6</label><caption><p><bold>Molecular docking results of the predicted PAMs of GLP-1R</bold></p><p><bold>A.</bold> Schematic display of receptor binding sites of different PAMs including BETP and the compounds predicted by DeepCPI in the current study. T149, a key residue for GLP-1R allosteric modulation, is located on TM1 but not shown in this diagram. <bold>B.</bold> Docked poses for the predicted interaction between GLP-1R and JK0580-H009 (shown in yellow). <bold>C.</bold> Docked poses for the predicted interaction between GLP-1R and CD3878-F005 (shown in cyan) <bold>D.</bold> Docked poses for the predicted interaction between GLP-1R and CD3293-E005 (shown in orange). GLP-1R is shown in its active form (pink, PDB ID: 5NX2). The binding modes of the three PAMs are compared with that of NAM NNC0640 (light purple) adopted from PDB ID: 5VEX. Key residues for PAM and NAM binding, namely I328 and T355, are shown in stick and colored by lemon. TM, transmembrane; NAM, negative allosteric modulator; BETP, 4-(3-(benzyloxy)phenyl)-2-ethylsulfinyl-6-(trifluoromethyl)pyrimidine.</p></caption><graphic xlink:href="gr6"></graphic></fig><fig id="f0035"><label>Figure 7</label><caption><p><bold>Mutations in GLP-1R affect the activities of predicted PAMs</bold></p><p>According to our docking results and previous reported binding sites for allosteric modulators, we established stable cell lines expressing T149A, I328N, C347F or T355A to identify the binding pocket for predicted PAMs. <bold>A.</bold> PAM activities of the predicted compounds on wild-type and mutant GLP-1 receptors, measured using a whole-cell competitive ligand binding assay at different concentrations. <bold>B.</bold> PAM activities of the predicted compounds on the wild-type and mutant GLP-1 receptors, measured by cAMP accumulation assay. All measurements were performed with at least three independent experiments. Data are shown as means ± SEM and fitted to a four-parameter logistic regression model.</p></caption><graphic xlink:href="gr7"></graphic></fig></p>
<p id="p0120">However, none of the three predicted compounds exhibited binding to the T149A mutant, and their modulation behavior on C347F mutant generally aligned with that on wild-type, supporting its non-covalent binding nature (<xref ref-type="fig" rid="f0035">Figure 7</xref>, <xref ref-type="sec" rid="s0165">Table S7</xref>). These observations point to a divergent binding mode of the predicted PAMs different from that of BETP. Intriguingly, I328N mutation principally abolished the allosteric effects of the compounds (<xref ref-type="fig" rid="f0035">Figure 7</xref>), probably due to a large steric crash, as predicted by the docking study. In contrast, T355A mutation located at the other side of TM6 (<xref ref-type="fig" rid="f0030">Figure 6</xref>) showed a negligible impact on the PAM activities of the predicted compounds (<xref ref-type="fig" rid="f0035">Figure 7</xref> and <xref ref-type="sec" rid="s0165">Table S7</xref>). Collectively, our mutagenesis results support the docking data, indicating that DeepCPI can discover potential PAMs of GLP-1R.</p>
</sec>
<sec id="s0045">
<title>Validation of GCGR and VIPR modulators</title>
<p id="p0125">Nine hits with antagonistic effects against GCGR (<xref ref-type="fig" rid="f0020">Figure 4</xref>C) were identified in the pilot screening. Among these, CD3400-G008 (<xref ref-type="sec" rid="s0165">Figure S6</xref>) was confirmed to stably decrease glucagon-induced cAMP accumulation (<xref ref-type="fig" rid="f0040">Figure 8</xref>A). Subsequently, its dose dependency and estimated <inline-formula><mml:math altimg="si1.svg" id="M28"><mml:msub><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> value of antagonism (<inline-formula><mml:math altimg="si17.svg" id="M29"><mml:mrow><mml:mn>22.6</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula>) were determined (<xref ref-type="fig" rid="f0040">Figure 8</xref>B). In addition, this compound led to a rightward shift of the glucagon dose–response curve, as measured by the cAMP accumulation assay (<xref ref-type="fig" rid="f0040">Figure 8</xref>C). This shift corresponded to an increase in the <inline-formula><mml:math altimg="si18.svg" id="M30"><mml:msub><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mn>50</mml:mn></mml:msub></mml:math></inline-formula> value of glucagon from <inline-formula><mml:math altimg="si19.svg" id="M31"><mml:mrow><mml:mn>23.9</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math altimg="si20.svg" id="M32"><mml:mrow><mml:mn>5.56</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula>, although it did not affect forskolin-induced cAMP accumulation (<xref ref-type="fig" rid="f0040">Figure 8</xref>D), ruling out the possibility that CD3400-G008 decreases cAMP accumulation in a non-specific manner. Similarly, the agonistic effect of the putative VIPR agonist CD3349-F005 (Figures 4 and S6) was dose-dependent (<xref ref-type="fig" rid="f0040">Figure 8</xref>E), while its agonism specificity was confirmed using a phosphodiesterase (PDE) inhibitor exclusion assay (<xref ref-type="fig" rid="f0040">Figure 8</xref>F). The results showed that neither <inline-formula><mml:math altimg="si21.svg" id="M33"><mml:mrow><mml:mn>25</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> nor <inline-formula><mml:math altimg="si22.svg" id="M34"><mml:mrow><mml:mn>50</mml:mn><mml:mspace width="0.333333em"></mml:mspace><mml:mi mathvariant="normal">μ</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> of CD3349-F005 affected forskolin-induced cAMP accumulation.<fig id="f0040"><label><bold>Figure 8</bold></label><caption><p><bold>Experimental validation of the hit compounds acting on GCGR and VIPR</bold></p><p><bold>A.</bold> Validation of the nine putative modulators identified in <xref ref-type="fig" rid="f0020">Figure 4</xref>C using cAMP accumulation assay. CD3400-G008 displayed a stable and remarkable antagonist effect on GCGR. <bold>B.</bold> Dose-dependent effect of CD3400-G008 on GCGR with respect to the inhibition of cAMP accumulation. GCGR-expressing cells were incubated with 0.01 nM glucagon and treated with different concentrations of CD3400-G008 to generate the dose–response curve (IC<sub>50</sub> = 22.6 µM). <bold>C.</bold> Glucagon dose–response curves, in which its concentration was gradually increased from 0.01 pM to 10 nM, whereas that of CD3400-G008 was set to 20 µM; DMSO was used as negative control. <bold>D.</bold> Effect of CD3400-G008 on forskolin-induced cAMP accumulation. <bold>E.</bold> Dose–response curves of CD3349-F005 in CHO-K1 cells overexpressing VIPR and in the parental CHO-K1 cells. CD3349-F005 concentration was gradually decreased from 100 µM to 0.78 µM. <bold>F.</bold> PDE inhibitor exclusion assay was conducted using CHO-K1 cells to detect the agonist effects of CD3349-F005 on forskolin-induced cAMP accumulation; a potent PDE inhibitor IBMX was used as positive control. All measurements were performed with at least three independent experiments in quadruplicate. Data are shown as means ± SEM and fitted to a four-parameter logistic regression model. Max, maximum response; PDE, phosphodiesterase; IBMX, 3-isobutyl-1-methylxanthine.</p></caption><graphic xlink:href="gr8"></graphic></fig></p>
<p id="p0130">Collectively, these data support the notion that DeepCPI prediction can offer a promising starting point for small-molecule drug discovery targeting GPCRs.</p>
</sec>
</sec>
</sec>
<sec id="s0050">
<title>Discussion</title>
<p id="p0135">In this article, we propose DeepCPI as a novel and scalable framework that combines data-driven representation learning with DL to predict novel CPIs (DTIs). By exploiting the large-scale unlabeled data of compounds and proteins, the employed representation learning schemes effectively extract low-dimensional expressive features from raw data without the requirement for information on protein structure or known interactions.</p>
<p id="p0140">The combination of the effective feature embedding strategies and the powerful DL model is particularly useful for fully exploiting the massive amount of compound–protein binding data available from large-scale databases, such as PubChem and ChEMBL. The effectiveness of our method was fully validated using several large-scale compound–protein binding datasets as well as the known interactions between Food and Drug Administration (FDA)-approved drugs and targets. Moreover, we experimentally validated several compounds that were predicted to interact with GPCRs, which represent the largest transmembrane receptor family and probably the most important drug targets. This family constitutes &gt;800 annotated and 150 “orphan” receptors. The latter are without known endogenous ligands and/or functions. Target-based drug discovery has been a focal point of research for decades. However, the inefficiency of mass random bioactivity screening promotes the application of <italic>in silico</italic> prediction and discovery of small-molecule ligands. Our DeepCPI model establishes a new framework that effectively combines feature embedding with DL for the prediction of CPIs at a large scale.</p>
<p id="p0145">We conducted several functional assays to validate our prediction results regarding the identification of small-molecule modulators targeting several class B (<italic>i.e.</italic>, the secretin-like family) GPCRs. GLP-1R is an established drug target for type 2 diabetes and obesity, and several peptidic therapeutic agents have been developed and marketed with combined annual sales of billions of dollars. As most therapeutic peptides require non-oral administration routes, discovery of orally available small-molecule surrogates is highly desirable. To the best of our knowledge, since the discovery of Boc5—the first non-peptidic GLP-1R agonist—more than a decade ago <xref ref-type="bibr" rid="b0310">[62]</xref>, <xref ref-type="bibr" rid="b0315">[63]</xref>, <xref ref-type="bibr" rid="b0320">[64]</xref>, little progress has been made in identifying “druggable” small-molecule mimetics for GLP-1. In this study, we identified three PAMs that were computationally predicted by DeepCPI and experimentally confirmed with bioassays to be specific to GLP-1R, thereby providing an alternative to discover non-peptidic modulators of GLP-1R.</p>
<p id="p0150">The docking results of our predicted hits demonstrated that they could be fitted to similar sites corresponding to the binding pockets for previously known PAMs at GLP-1R in its active form. The experimental data generated by binding and cAMP accumulation assays confirmed the positive allosteric action of these hits. Overall, our modeling data, in conjunction with those obtained from mutagenesis studies, revealed the binding poses of the predicted interactions between these compounds and GLP-1R. These results offer new insights into the structural basis and underlying mechanisms of drug action.</p>
<p id="p0155">Cross validation through different databases, supporting evidence from the known DTIs in the literature, and laboratory experimentation indicate that DeepCPI can serve as a useful tool for drug discovery and repositioning. In our follow-up studies, we intend to combine DeepCPI with additional validation experiments for the discovery of drug leads against a wide range of targets. Better prediction results may be achieved by incorporating other available data, such as gene expression and protein structures, into our DL model.</p>
</sec>
<sec id="s0055">
<title>Materials and methods</title>
<sec id="s0060">
<title>DeepCPI</title>
<p id="p0160">DeepCPI is an extension of our previously developed CPI prediction model <xref ref-type="bibr" rid="b0325">[65]</xref>. We describe the building blocks of DeepCPI in the following three subsections.</p>
</sec>
<sec id="s0065">
<title>Compound feature extraction</title>
<p id="p0165">To learn good embeddings (<italic>i.e.</italic>, low-dimensional feature representations) of compounds, we used the latent semantic analysis (also termed latent semantic indexing) technique <xref ref-type="bibr" rid="b0135">[27]</xref>, which is probably one of the most effective methods for document similarity analyses in the field of NLP. In latent semantic analysis, each document is represented by a vector storing the term frequency or term frequency-inverse document frequency information (tf-idf). This is a numerical statistic widely used in information retrieval to describe the importance of a word in a document. Subsequently, a corpus (<italic>i.e</italic>., a collection of documents) can be represented by a matrix, in which each column stores the tf-idf scores of individual terms in a document. Subsequently, singular value decomposition (SVD) is applied to obtain low-dimensional representations of features in documents.</p>
<p id="p0170">In the context of compound feature extraction (<xref ref-type="sec" rid="s0165">Figure S1</xref>), a compound and its substructures can be viewed as a document and corresponding terms, respectively. Given a compound set <inline-formula><mml:math altimg="si23.svg" id="M35"><mml:mi>N</mml:mi></mml:math></inline-formula>, we use the Morgan fingerprints <xref ref-type="bibr" rid="b0175">[35]</xref> with a radius of one to scan every atom of each compound in <inline-formula><mml:math altimg="si23.svg" id="M36"><mml:mi>N</mml:mi></mml:math></inline-formula> and then generate the corresponding substructures. Let <inline-formula><mml:math altimg="si24.svg" id="M37"><mml:mi>D</mml:mi></mml:math></inline-formula> denote the set of substructures generated from all compounds in <inline-formula><mml:math altimg="si23.svg" id="M38"><mml:mi>N</mml:mi></mml:math></inline-formula>. We then employ a matrix <inline-formula><mml:math altimg="si25.svg" id="M39"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to store the word count information for these compounds, where <inline-formula><mml:math altimg="si26.svg" id="M40"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the tf-idf value of the <italic>i</italic><sup>th</sup> substructure in the <inline-formula><mml:math altimg="si27.svg" id="M41"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula><sup>th</sup> compound. More specifically, <inline-formula><mml:math altimg="si26.svg" id="M42"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as <inline-formula><mml:math altimg="si28.svg" id="M43"><mml:mrow><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mi>·</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si29.svg" id="M44"><mml:mrow><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> stands for the number of occurrences of the <italic>i</italic><sup>th</sup> substructure in the <italic>j</italic><sup>th</sup> compound, and <inline-formula><mml:math altimg="si30.svg" id="M45"><mml:mrow><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math altimg="si31.svg" id="M46"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of documents containing the <inline-formula><mml:math altimg="si32.svg" id="M47"><mml:mi>i</mml:mi></mml:math></inline-formula><sup>th</sup> substructure. Basically, <inline-formula><mml:math altimg="si33.svg" id="M48"><mml:mrow><mml:mi mathvariant="italic">idf</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> reweighs <inline-formula><mml:math altimg="si29.svg" id="M49"><mml:mrow><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, resulting in lower weights for more common substructures and higher weights for less common substructures. This is consistent with an observation in the information theory that rarer events generally have higher entropy and are thus more informative.</p>
<p id="p0175">After <inline-formula><mml:math altimg="si34.svg" id="M50"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> is constructed, it is then decomposed by SVD into three matrices, <inline-formula><mml:math altimg="si35.svg" id="M51"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, such that <inline-formula><mml:math altimg="si36.svg" id="M52"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo mathvariant="bold">∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math altimg="si37.svg" id="M53"><mml:mrow><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math altimg="si38.svg" id="M54"><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> diagonal matrix with the eigenvalues of <inline-formula><mml:math altimg="si34.svg" id="M55"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> on the diagonal matrix, and <inline-formula><mml:math altimg="si39.svg" id="M56"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math altimg="si40.svg" id="M57"><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> matrix in which each column <inline-formula><mml:math altimg="si41.svg" id="M58"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is an eigenvector of <inline-formula><mml:math altimg="si34.svg" id="M59"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> corresponding to the <inline-formula><mml:math altimg="si32.svg" id="M60"><mml:mi>i</mml:mi></mml:math></inline-formula><sup>th</sup> eigenvalue <inline-formula><mml:math altimg="si42.svg" id="M61"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ii</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
<p id="p0180">To embed the compounds into a low-dimensional space <inline-formula><mml:math altimg="si43.svg" id="M62"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si44.svg" id="M63"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, we select the first <inline-formula><mml:math altimg="si45.svg" id="M64"><mml:mi>d</mml:mi></mml:math></inline-formula> columns of <inline-formula><mml:math altimg="si39.svg" id="M65"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:math></inline-formula>, which correspond to the largest eigenvalues in <inline-formula><mml:math altimg="si37.svg" id="M66"><mml:mrow><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math altimg="si46.svg" id="M67"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> denote the matrix with columns corresponding to these selected eigenvectors. Subsequently, a low-dimensional embedding of <inline-formula><mml:math altimg="si34.svg" id="M68"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> can be obtained by <inline-formula><mml:math altimg="si47.svg" id="M69"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math></inline-formula><bold> =</bold> <inline-formula><mml:math altimg="si48.svg" id="M70"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si49.svg" id="M71"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is a <inline-formula><mml:math altimg="si50.svg" id="M72"><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula> matrix, in which the <italic>i</italic><sup>th</sup> column corresponds to a <inline-formula><mml:math altimg="si45.svg" id="M73"><mml:mi>d</mml:mi></mml:math></inline-formula>-dimensional embedding (or embedded feature vector) of the <italic>i</italic><sup>th</sup> compound.<inline-formula><mml:math altimg="si46.svg" id="M74"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> can be precomputed and fixed after being trained from a compound corpus. Given any new compound, its embedded low-dimensional feature vector can be obtained by left multiplying its tf-idf by <inline-formula><mml:math altimg="si51.svg" id="M75"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> (<xref ref-type="sec" rid="s0165">Figure S1</xref>).</p>
<p id="p0185">Our compound feature embedding framework used the compounds retrieved from multiple sources, including all compounds labeled as active in bioassays on PubChem <xref ref-type="bibr" rid="b0070">[14]</xref>, all FDA-approved drugs in DrugBank <xref ref-type="bibr" rid="b0160">[32]</xref>, and all compounds stored in ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref>. Duplicate compounds were removed according to their International Chemical Identifiers (InChIs). The total number of final compounds in our compound feature extraction module used to construct matrix <inline-formula><mml:math altimg="si34.svg" id="M76"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> was 1,795,801. The total number of different substructures generated from the Morgan fingerprints with a radius of one was 18,868. We set <inline-formula><mml:math altimg="si52.svg" id="M77"><mml:mrow><mml:mi>d</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>, which is a recommended value in latent semantic analysis <xref ref-type="bibr" rid="b0330">[66]</xref>.</p>
</sec>
<sec id="s0070">
<title>Protein feature extraction</title>
<p id="p0190">We applied Word2vec, a successful word-embedding technique widely used in various NLP tasks <xref ref-type="bibr" rid="b0080">[16]</xref>, <xref ref-type="bibr" rid="b0140">[28]</xref>, to learn the low-dimensional representations of protein features. In particular, we use the Skip-gram with a negative sampling method <xref ref-type="bibr" rid="b0140">[28]</xref> to train the word-embedding model and learn the context relations between words in sentences. We first introduce some necessary notations. Suppose that we are given a set of sentences <inline-formula><mml:math altimg="si53.svg" id="M78"><mml:mi>S</mml:mi></mml:math></inline-formula> and a context window of size <inline-formula><mml:math altimg="si54.svg" id="M79"><mml:mi>b</mml:mi></mml:math></inline-formula>. Given a sentence <inline-formula><mml:math altimg="si55.svg" id="M80"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> that is represented by a sequence of words <inline-formula><mml:math altimg="si56.svg" id="M81"><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si57.svg" id="M82"><mml:mi>L</mml:mi></mml:math></inline-formula> is the length of <inline-formula><mml:math altimg="si58.svg" id="M83"><mml:mi>s</mml:mi></mml:math></inline-formula>, the contexts of a word<inline-formula><mml:math altimg="si59.svg" id="M84"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are defined as <inline-formula><mml:math altimg="si60.svg" id="M85"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. That is, all the words appearing within the context window of size <inline-formula><mml:math altimg="si54.svg" id="M86"><mml:mi>b</mml:mi></mml:math></inline-formula> and centered at word <inline-formula><mml:math altimg="si61.svg" id="M87"><mml:mi>w</mml:mi></mml:math></inline-formula> in the sentence are regarded as the contexts of <inline-formula><mml:math altimg="si61.svg" id="M88"><mml:mi>w</mml:mi></mml:math></inline-formula>. We further use <inline-formula><mml:math altimg="si62.svg" id="M89"><mml:mi>W</mml:mi></mml:math></inline-formula> to denote the set of all words appearing in <inline-formula><mml:math altimg="si53.svg" id="M90"><mml:mi>S</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math altimg="si63.svg" id="M91"><mml:mrow><mml:mo>#</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> to denote the total number of occurrences of word <inline-formula><mml:math altimg="si64.svg" id="M92"><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> appearing in the context window of the word <inline-formula><mml:math altimg="si65.svg" id="M93"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math altimg="si53.svg" id="M94"><mml:mi>S</mml:mi></mml:math></inline-formula>. Since each word can play two roles (<italic>i.e.</italic>, center word and context word), the Skip-gram method equips every word <inline-formula><mml:math altimg="si65.svg" id="M95"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> with two <inline-formula><mml:math altimg="si45.svg" id="M96"><mml:mi>d</mml:mi></mml:math></inline-formula>-dimensional vector representations <inline-formula><mml:math altimg="si66.svg" id="M97"><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si67.svg" id="M98"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> is used when <inline-formula><mml:math altimg="si61.svg" id="M99"><mml:mi>w</mml:mi></mml:math></inline-formula> is a center word and <inline-formula><mml:math altimg="si68.svg" id="M100"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> is used otherwise (both vectors are randomly initialized). Here, <inline-formula><mml:math altimg="si67.svg" id="M101"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math altimg="si69.svg" id="M102"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> basically represents the coordinates of <inline-formula><mml:math altimg="si61.svg" id="M103"><mml:mi>w</mml:mi></mml:math></inline-formula> in the lower dimensional (<italic>i.e.</italic>, <inline-formula><mml:math altimg="si70.svg" id="M104"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>-dimensional) space after embedding. Subsequently, our goal is to maximize the following objective function:<disp-formula id="e0005"><label>(1)</label><mml:math altimg="si71.svg" id="M105"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:mo>#</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">log</mml:mi><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math altimg="si72.svg" id="M106"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the sigmoid function. Since the range of the sigmoid function is <inline-formula><mml:math altimg="si73.svg" id="M107"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math altimg="si74.svg" id="M108"><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> can be interpreted as the probability of word <inline-formula><mml:math altimg="si75.svg" id="M109"><mml:mi>c</mml:mi></mml:math></inline-formula> being a context of word <inline-formula><mml:math altimg="si61.svg" id="M110"><mml:mi>w</mml:mi></mml:math></inline-formula>, and Equation <xref ref-type="disp-formula" rid="e0005">1</xref> can be viewed as the log-likelihood of a given sentence set <inline-formula><mml:math altimg="si53.svg" id="M111"><mml:mi>S</mml:mi></mml:math></inline-formula>.</p>
<p id="p0195">One problem in this objective function (<italic>i.e.</italic>, Equation <xref ref-type="disp-formula" rid="e0005">1</xref>) is that it does not take into account any negative example. If we arbitrarily assign any large positive values to <inline-formula><mml:math altimg="si76.svg" id="M112"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si77.svg" id="M113"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math altimg="si78.svg" id="M114"><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would invariably be predicted as 1. In this case, although Equation <xref ref-type="disp-formula" rid="e0005">1</xref> is maximized, such embeddings are surely useless. To tackle this problem, a Skip-gram model with negative sampling <xref ref-type="bibr" rid="b0140">[28]</xref> has been proposed, in which “negative examples” <inline-formula><mml:math altimg="si79.svg" id="M115"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> ∈ <italic>W</italic> and <inline-formula><mml:math altimg="si80.svg" id="M116"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>) are drawn from a data distribution <inline-formula><mml:math altimg="si81.svg" id="M117"><mml:mrow><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>M</italic> represents the total number of words in <inline-formula><mml:math altimg="si53.svg" id="M118"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math altimg="si82.svg" id="M119"><mml:mrow><mml:mo>#</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the total number of occurrences of word <inline-formula><mml:math altimg="si83.svg" id="M120"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in <inline-formula><mml:math altimg="si84.svg" id="M121"><mml:mrow><mml:mi>S</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Then, the new objective function can be written a<inline-formula><mml:math altimg="si85.svg" id="M122"><mml:mi mathvariant="normal">s</mml:mi></mml:math></inline-formula> follows:<disp-formula id="e0015"><label>(2)</label><mml:math altimg="si87.svg" id="M123"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:mo>#</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">log</mml:mi><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced close="]" open="["><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:mo>#</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">log</mml:mi><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mfenced close="]" open="["><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mi>·</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math altimg="si88.svg" id="M124"><mml:mi>k</mml:mi></mml:math></inline-formula> is the number of “negative examples” to be sampled for each observed word–context pair <inline-formula><mml:math altimg="si89.svg" id="M125"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> during training. Maximizing this objective function can be performed using the stochastic gradient descent technique <xref ref-type="bibr" rid="b0080">[16]</xref>.</p>
<p id="p0200">For each observed word–context pair <inline-formula><mml:math altimg="si89.svg" id="M126"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, Equation <xref ref-type="disp-formula" rid="e0015">2</xref> aims to maximize its log-likelihood, while minimizing the log-likelihood of <inline-formula><mml:math altimg="si88.svg" id="M127"><mml:mi>k</mml:mi></mml:math></inline-formula> random pairs <inline-formula><mml:math altimg="si90.svg" id="M128"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> under the assumption that such random selections can well reflect the unobserved word–context pairs (<italic>i.e.</italic>, negative examples) representing the background. In other words, the goal of this task is to distinguish the observed word–context pairs from the background distribution.</p>
<p id="p0205">As in other existing schemes for encoding the features of genomic sequences <xref ref-type="bibr" rid="b0145">[29]</xref>, each protein sequence in our framework is regarded as a “sentence” reading from its N-terminus to C-terminus and every three non-overlapping amino acid residues are viewed as a “word” (<xref ref-type="sec" rid="s0165">Figure S2</xref>). For each protein sequence, we start from the first, second, and third amino acid residues from the N-terminus sequentially and then consider all possible “words” while discarding those residues that cannot form a “word”.</p>
<p id="p0210">After converting protein sequences to “sentences” and all three non-overlapping amino acid residues to “words”, Skip-gram with negative sampling is employed to learn the low-dimensional embeddings of these “words”. Subsequently, the learnt embeddings of “words” are fixed, and an embedding of a new protein sequence is obtained by summing and averaging the embeddings of all “words” in all three possible encoded “sentences” (<xref ref-type="sec" rid="s0165">Figure S2</xref>). Of note, a similar approach has been successfully used to extract useful features for text classification using Word2vec <xref ref-type="bibr" rid="b0335">[67]</xref>.</p>
<p id="p0215">In our study, the protein sequences used for learning the low-dimensional embeddings of protein features were retrieved from several databases, including PubChem <xref ref-type="bibr" rid="b0070">[14]</xref>, DrugBank <xref ref-type="bibr" rid="b0160">[32]</xref>, ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref>, Protein Data Bank <xref ref-type="bibr" rid="b0340">[68]</xref> (<ext-link ext-link-type="uri" id="ir015" xlink:href="http://www.rcsb.org">www.rcsb.org</ext-link>), and UniProt <xref ref-type="bibr" rid="b0190">[38]</xref>. All duplicate sequences were removed, and the final number of sequences for learning the protein features during the embedding process was 464,122. We followed the previously described principles <xref ref-type="bibr" rid="b0145">[29]</xref> to select the hyper parameters of Skip-gram. More specifically, the embedding dimension was set to <inline-formula><mml:math altimg="si91.svg" id="M129"><mml:mrow><mml:mi>d</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>, the size of the context window was set to <inline-formula><mml:math altimg="si92.svg" id="M130"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>, and the number of negative examples was set to <inline-formula><mml:math altimg="si93.svg" id="M131"><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
</sec>
<sec id="s0075">
<title>Multimodal DNN</title>
<p id="p0220">Suppose that we are given a training dataset of compound–protein pairs <inline-formula><mml:math altimg="si94.svg" id="M132"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula> and a corresponding label set <inline-formula><mml:math altimg="si95.svg" id="M133"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si96.svg" id="M134"><mml:mi>n</mml:mi></mml:math></inline-formula> stands for the total number of compound–protein pairs, <inline-formula><mml:math altimg="si97.svg" id="M135"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> indicates that compound <inline-formula><mml:math altimg="si98.svg" id="M136"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and protein <inline-formula><mml:math altimg="si99.svg" id="M137"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> interact with each other, and <inline-formula><mml:math altimg="si100.svg" id="M138"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. We first use the feature extraction schemes described earlier to derive the feature embeddings of individual compounds and proteins, and then feed these two embeddings to a multimodal DNN to determine whether the given compound–protein pair exhibits a true interaction.</p>
<p id="p0225">We first introduce a vanilla DNN and then describe its multimodal variant. The basic DNN architecture comprises an input layer <inline-formula><mml:math altimg="si101.svg" id="M139"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, an output layer <inline-formula><mml:math altimg="si102.svg" id="M140"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math altimg="si103.svg" id="M141"><mml:mi>H</mml:mi></mml:math></inline-formula> hidden layers <inline-formula><mml:math altimg="si104.svg" id="M142"><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math altimg="si105.svg" id="M143"><mml:mrow><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>) between input and output layers. Each hidden layer <inline-formula><mml:math altimg="si104.svg" id="M144"><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> contains a set of units that can be represented by a vector <inline-formula><mml:math altimg="si106.svg" id="M145"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si107.svg" id="M146"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for the number of units in <inline-formula><mml:math altimg="si104.svg" id="M147"><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula>. Subsequently, each hidden layer <inline-formula><mml:math altimg="si104.svg" id="M148"><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> can be parameterized by a weight matrix <inline-formula><mml:math altimg="si108.svg" id="M149"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a bias vector <inline-formula><mml:math altimg="si109.svg" id="M150"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and an activation function <inline-formula><mml:math altimg="si110.svg" id="M151"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>·</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>. More specifically, the units in <inline-formula><mml:math altimg="si104.svg" id="M152"><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> can be calculated by <inline-formula><mml:math altimg="si111.svg" id="M153"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si112.svg" id="M154"><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula>, and the units <inline-formula><mml:math altimg="si113.svg" id="M155"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> in the input layer <inline-formula><mml:math altimg="si114.svg" id="M156"><mml:msub><mml:mi>L</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> are the input features. We use the rectified linear unit function <inline-formula><mml:math altimg="si115.svg" id="M157"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is a common choice of activation function in DL, perhaps due to its sparsity property, high computational efficiency, and absence of the gradient-vanishing effect during the back-propagation training process <xref ref-type="bibr" rid="b0345">[69]</xref>.</p>
<p id="p0230">The multimodal DNN differs from the vanilla DNN in terms of the use of local hidden layers to distinguish different input modalities (<xref ref-type="fig" rid="f0005">Figure 1</xref>). In our case, the low-dimensional compound and protein embeddings are considered distinct input modalities and separately fed to two different local hidden layers. Subsequently, these two types of local hidden layers are concatenated and fed to joint hidden layers (<xref ref-type="fig" rid="f0005">Figure 1</xref>). Here, the explicit partition of local hidden layers for distinct input channels can better exploit the statistical properties of different modalities <xref ref-type="bibr" rid="b0350">[70]</xref>.</p>
<p id="p0235">After we calculate the <inline-formula><mml:math altimg="si116.svg" id="M158"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> for the final (joint) hidden layer, the output layer <inline-formula><mml:math altimg="si102.svg" id="M159"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is simply a logistic regression model that takes <inline-formula><mml:math altimg="si117.svg" id="M160"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> as its input and computes <inline-formula><mml:math altimg="si118.svg" id="M161"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the output <inline-formula><mml:math altimg="si119.svg" id="M162"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math></inline-formula> is the confidence score for classification, <inline-formula><mml:math altimg="si120.svg" id="M163"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> is the sigmoid function, <inline-formula><mml:math altimg="si121.svg" id="M164"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math altimg="si122.svg" id="M165"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> are the parameters of the output layer <inline-formula><mml:math altimg="si102.svg" id="M166"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Since the sigmoid function has a range <inline-formula><mml:math altimg="si123.svg" id="M167"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math altimg="si119.svg" id="M168"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math></inline-formula> can also be interpreted as the interacting probability of the given compound–protein pair.</p>
<p id="p0240">To learn <inline-formula><mml:math altimg="si124.svg" id="M169"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math altimg="si125.svg" id="M170"><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and all parameters <inline-formula><mml:math altimg="si126.svg" id="M171"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math altimg="si127.svg" id="M172"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> in the hidden layers from the training data set and the corresponding label set, we need to minimize the following cross-entropy loss:<disp-formula id="e0020"><mml:math altimg="si128.svg" id="M173"><mml:mrow><mml:mi>J</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="badbreak">-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
<p id="p0245">The aforementioned minimization problem can be solved using the stochastic gradient descent and back-propagation techniques <xref ref-type="bibr" rid="b0095">[19]</xref>. In addition, we apply two popular strategies in DL communities—dropout <xref ref-type="bibr" rid="b0355">[71]</xref> and batch normalization <xref ref-type="bibr" rid="b0360">[72]</xref>—to further enhance the classification performance of our DL model. In particular, dropout sets the hidden units to zero with a certain probability, which can effectively alleviate the potential overfitting problem in DL <xref ref-type="bibr" rid="b0355">[71]</xref>. The batch normalization scheme normalizes the outputs of hidden units to zero mean and unit standard deviation, which can accelerate the training process and act as a regularizer <xref ref-type="bibr" rid="b0360">[72]</xref>.</p>
<p id="p0250">Since positive and negative examples are possibly imbalanced, our classifier may learn a “lazy” solution. That is, in such a skewed data distribution case, the classifier can relatively easily predict the dominant class given any input. To alleviate this problem, we downsample the examples from the majority class in order for the numbers of positive and negative examples to be comparable during training. In our computational tests, we implement an ensemble version of the previously described DL model and use the average prediction over 20 models to obtain relatively more stable classification results.</p>
</sec>
<sec id="s0080">
<title>Time and space complexities</title>
<sec id="s0085">
<title>Training</title>
<p id="p0255">For compound feature extraction, given a compound, let <inline-formula><mml:math altimg="si129.svg" id="M174"><mml:mi>g</mml:mi></mml:math></inline-formula> denote the running time of generating its Morgan fingerprints with a radius of one. The time complexity for extracting the low-dimensional representations of compound features is <inline-formula><mml:math altimg="si130.svg" id="M175"><mml:mrow><mml:mi>O</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mi>g</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si131.svg" id="M176"><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> stands for the number of compounds and <inline-formula><mml:math altimg="si132.svg" id="M177"><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> stands for the total number of substructures in all compounds. Here, <inline-formula><mml:math altimg="si133.svg" id="M178"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mi>g</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> is required for generating the substructures of all compounds and <inline-formula><mml:math altimg="si134.svg" id="M179"><mml:mrow><mml:mi>O</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> is required for running SVD. The space complexity for the compound feature extraction module is <inline-formula><mml:math altimg="si135.svg" id="M180"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>. After training, we need <inline-formula><mml:math altimg="si136.svg" id="M181"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> space to store the selected eigenvectors of <inline-formula><mml:math altimg="si34.svg" id="M182"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:math></inline-formula> for future inference, where <inline-formula><mml:math altimg="si137.svg" id="M183"><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> stands for the dimension of compound embedding.</p>
<p id="p0260">For the protein feature extraction process, given a protein corpus <inline-formula><mml:math altimg="si53.svg" id="M184"><mml:mi>S</mml:mi></mml:math></inline-formula>, it takes <inline-formula><mml:math altimg="si138.svg" id="M185"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>b</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time and <inline-formula><mml:math altimg="si139.svg" id="M186"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> space to scan and calculate the context information, where <inline-formula><mml:math altimg="si62.svg" id="M187"><mml:mi>W</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math altimg="si54.svg" id="M188"><mml:mi>b</mml:mi></mml:math></inline-formula> stand for the set of all possible three non-overlapping amino acid residues and the context window size, respectively. Given a word–context pair <inline-formula><mml:math altimg="si140.svg" id="M189"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula>, it takes <inline-formula><mml:math altimg="si141.svg" id="M190"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time to compute the objective function in Equation <xref ref-type="disp-formula" rid="e0015">2</xref>, where <inline-formula><mml:math altimg="si142.svg" id="M191"><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> stands for the dimension of a protein embedding and <inline-formula><mml:math altimg="si88.svg" id="M192"><mml:mi>k</mml:mi></mml:math></inline-formula> stands for the number of negative samples for each word–context pair. Suppose that we perform <inline-formula><mml:math altimg="si143.svg" id="M193"><mml:mi>q</mml:mi></mml:math></inline-formula> iterations of gradient descent, then, the time complexity of the protein feature extraction module is <inline-formula><mml:math altimg="si144.svg" id="M194"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>q</mml:mi><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>. After training, we need <inline-formula><mml:math altimg="si145.svg" id="M195"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> space to store the protein embeddings for future inference.</p>
<p id="p0265">For a neural network, let <inline-formula><mml:math altimg="si146.svg" id="M196"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si107.svg" id="M197"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the numbers of units in layers <inline-formula><mml:math altimg="si147.svg" id="M198"><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si148.svg" id="M199"><mml:mi>h</mml:mi></mml:math></inline-formula>, respectively. Suppose that <inline-formula><mml:math altimg="si149.svg" id="M200"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the largest value among all layers, then, the time and space complexities for training our DL model are bounded by <inline-formula><mml:math altimg="si150.svg" id="M201"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>q</mml:mi><mml:mi>n</mml:mi><mml:mi>H</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si151.svg" id="M202"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>H</mml:mi><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo linebreak="badbreak">-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, respectively, where <inline-formula><mml:math altimg="si96.svg" id="M203"><mml:mi>n</mml:mi></mml:math></inline-formula> stands for the total number of training samples, <inline-formula><mml:math altimg="si103.svg" id="M204"><mml:mi>H</mml:mi></mml:math></inline-formula> stands for the number of hidden layers in the neural network, and <inline-formula><mml:math altimg="si143.svg" id="M205"><mml:mi>q</mml:mi></mml:math></inline-formula> stands for the number of training iterations.</p>
</sec>
<sec id="s0090">
<title>Prediction</title>
<p id="p0270">During the prediction stage, given a compound–protein pair, we first compute the low-dimensional vector representations of their features. This operation takes <inline-formula><mml:math altimg="si152.svg" id="M206"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>g</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time for the compound and <inline-formula><mml:math altimg="si153.svg" id="M207"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>r</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time for the protein, where <inline-formula><mml:math altimg="si154.svg" id="M208"><mml:mi>r</mml:mi></mml:math></inline-formula> stands for the length of the protein sequence. Then, these two low-dimensional vector representations are fed to the deep multimodal neural network to make the prediction, which takes <inline-formula><mml:math altimg="si151.svg" id="M209"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>H</mml:mi><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo linebreak="badbreak">-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time. In our framework, we set <inline-formula><mml:math altimg="si155.svg" id="M210"><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> = 1024 and <inline-formula><mml:math altimg="si156.svg" id="M211"><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> = 256, which are small and can be considered constant (also see “Implementation of DeepCPI” below).</p>
</sec>
</sec>
<sec id="s0095">
<title>Mapping DrugBank data to ChEMBL</title>
<p id="p0275">A known drug–target pair from DrugBank <xref ref-type="bibr" rid="b0160">[32]</xref> is considered to be in ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref> if compound–target pairs with identical InChIs for compounds or drugs and sequence identity scores <inline-formula><mml:math altimg="si157.svg" id="M212"><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.40</mml:mn></mml:mrow></mml:math></inline-formula> for proteins are present in the latter dataset. Given two protein sequences <inline-formula><mml:math altimg="si58.svg" id="M213"><mml:mi>s</mml:mi></mml:math></inline-formula> and <italic>s'</italic>, their sequence identity score <inline-formula><mml:math altimg="si158.svg" id="M214"><mml:mi>t</mml:mi></mml:math></inline-formula> is defined as <inline-formula><mml:math altimg="si158.svg" id="M215"><mml:mi>t</mml:mi></mml:math></inline-formula> = <inline-formula><mml:math altimg="si159.svg" id="M216"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mtext>'</mml:mtext></mml:mrow></mml:mfenced></mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mtext>'</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mtext>'</mml:mtext><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si160.svg" id="M217"><mml:mrow><mml:mi mathvariant="italic">SW</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>·</mml:mi><mml:mo>,</mml:mo><mml:mi>·</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> stands for the Smith–Waterman score <xref ref-type="bibr" rid="b0365">[73]</xref>.</p>
</sec>
<sec id="s0100">
<title>Definition of unique compounds and proteins</title>
<p id="p0280">Given a dataset, a compound is considered unique if its chemical structure similarity score with any other compound is &lt;0.55, where the chemical structure similarity score is defined as the Jaccard similarity between two Morgan fingerprints with a radius of three of the corresponding compounds <xref ref-type="bibr" rid="b0175">[35]</xref>. Similarly, a protein is considered unique if its sequence similarity score with any other protein is &lt; 0.40.</p>
</sec>
<sec id="s0105">
<title>DeepCPI implementation</title>
<p id="p0285">The Morgan fingerprints of compounds were generated by RDKit (<ext-link ext-link-type="uri" id="ir020" xlink:href="https://github.com/rdkit/rdkit">https://github.com/rdkit/rdkit</ext-link>). Latent semantic analysis and Word2vec (Skip-gram with negative sampling) were performed using Gensim <xref ref-type="bibr" rid="b0370">[74]</xref>, a Python library designed to automatically extract semantic topics from documents. Our DNN implementation was based on Keras (<ext-link ext-link-type="uri" id="ir025" xlink:href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</ext-link>)—a highly modular DL library. For all computational experiments, we used two local hidden layers with 1024 and 256 units, respectively, for both compound and protein input channels. The two local layers were then connected to three joint hidden layers with 512, 128, and 32 units, respectively. We set the dropout rate at 0.2. Batch normalization was added to all hidden layers. During training, we selected 20% of the training data as validation set to select the optimal training epoch.</p>
</sec>
<sec id="s0110">
<title>Baseline methods</title>
<p id="p0290">When testing on ChEMBL <xref ref-type="bibr" rid="b0150">[30]</xref> and BindingDB <xref ref-type="bibr" rid="b0155">[31]</xref>, we compared our method with two network-based DTI prediction methods, including DTINet <xref ref-type="bibr" rid="b0060">[12]</xref> and NetLapRLS <xref ref-type="bibr" rid="b0050">[10]</xref>, as well as with three constructed baseline methods as shown below.</p>
<p id="p0295">For DTINet and NetLapRLS, Jaccard similarity between the Morgan fingerprints of the corresponding compounds with a radius of three and pairwise Smith–Waterman scores were used to construct compound and protein similarity matrices as required in both methods. The default hyperparameters of both methods were used.</p>
<p id="p0300">For random forest with our feature extraction schemes, we set the tree number to 128 in all computational tests as previously recommended <xref ref-type="bibr" rid="b0375">[75]</xref>. We randomly selected 20% of the training data as validation set to select the optimal tree depth from 1 to 30.</p>
<p id="p0305">For SLNN with our feature extraction schemes, we used a local hidden layer with 1024 units for both compound and protein input channels. The two local layers were then connected and fed to a logistic layer to make the CPI prediction. We set the dropout rate to 0.2 and batch normalization was added to the hidden layer as in DeepCPI. We selected 20% of the training data as validation set to select the optimal training epoch.</p>
<p id="p0310">For DNN with conventional features, instead of using our feature extraction schemes, Morgan fingerprints with a radius of three and pairwise Smith–Waterman scores were used as compound and protein features, respectively. These features were subsequently fed into the same multimodal neural network as in DeepCPI. We selected 20% of the training data as validation set to select the optimal training epoch.</p>
<p id="p0315">We also compared our method DeepCPI with two other DL-based models, namely AtomNet <xref ref-type="bibr" rid="b0115">[23]</xref> and DeepDTI <xref ref-type="bibr" rid="b0120">[24]</xref>.</p>
<p id="p0320">When comparing with AtomNet, we experienced difficulty in reimplementing AtomNet. Therefore, we mainly compared the performance of DeepCPI with that of AtomNet on the same DUD-E dataset. For a fair comparison, we only used a single DeepCPI model instead of an ensemble version.</p>
<p id="p0325">We also compared the performance of DeepCPI to that of DeepDTI on 6262 DTIs provided by the original DeepDTI article <xref ref-type="bibr" rid="b0120">[24]</xref>. DeepDTI conducted a grid search to determine the hyper parameters of the model. Hence, for a fair comparison, we followed the same strategy to determine the hyper parameters of DeepCPI. Here, we reported the hyper parameter space that we searched. In particular, we selected a batch size from <inline-formula><mml:math altimg="si161.svg" id="M218"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>512</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>, dimensions of compound and protein features from <inline-formula><mml:math altimg="si162.svg" id="M219"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn><mml:mo>,</mml:mo><mml:mn>70</mml:mn><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo>,</mml:mo><mml:mn>90</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>300</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>, dropout rate from<inline-formula><mml:math altimg="si163.svg" id="M220"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>, and joint hidden layers with sizes from <inline-formula><mml:math altimg="si164.svg" id="M221"><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mn>512</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo stretchy="true">}</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mn>512</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo stretchy="true">}</mml:mo><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:math></inline-formula>. We only used a single DeeCPI model instead of an ensemble version for a fair comparison.</p>
</sec>
<sec id="s0115">
<title>Molecular docking</title>
<p id="p0330">Compounds were docked using AutoDock Vina <xref ref-type="bibr" rid="b0285">[57]</xref>. The GLP-1R model in its active form was extracted from a co-crystal structure of full-length GLP-1R and a truncated peptide agonist (PDB: 5NX2) <xref ref-type="bibr" rid="b0290">[58]</xref>. The best docked poses were selected based on the Vina-predicted energy values.</p>
</sec>
<sec id="s0120">
<title>Experimental validation</title>
<sec id="s0125">
<title>Cell culture</title>
<p id="p0335">Stable cell lines were established using FlpIn Chinese hamster ovary (CHO) cells (Invitrogen, Carlsbad, CA) expressing either GLP-1R or GCGR and cultured in Ham’s F12 nutrient medium (F12) with 10% fetal bovine serum (FBS) and 800 μg/ml hygromycin-B at 37 °C and 5% carbon dioxide (CO<sub>2</sub>). Desired mutations were introduced to GLP-1R construct using the Muta-direct™ kit (Catalog No. SDM-15; Beijing SBS Genetech, Beijing, China) and integrated into FlpIn-CHO cells. VIPR overexpression was achieved through transient transfection using Lipofectamine 2000 (Invitrogen) in F12 medium with 10% FBS. Cells were cultured for 24 h before being seeded into microtiter plates.</p>
</sec>
<sec id="s0130">
<title>Whole-cell competitive ligand binding assay</title>
<p id="p0340">CHO cells stably expressing GLP-1R or GCGR were seeded into 96-well plates at a density of 3 × 10<sup>4</sup> cells/well and incubated overnight at 37 °C and 5% CO<sub>2</sub>. The radioligand binding assay was performed 24 h thereafter. For homogeneous binding, cells were incubated in binding buffer with a constant concentration of <sup>125</sup>I-GLP-1 (40 pM, PerkinElmer, Boston, MA) or <sup>125</sup>I-glucagon (40 pM, PerkinElmer) and unlabeled compounds at 4 °C overnight. Cells were washed three times with ice-cold PBS and lysed using 50  μl lysis buffer (PBS supplemented with 20 <inline-formula><mml:math altimg="si165.svg" id="M222"><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula> Tris–HCl and 1% Triton X-100, pH 7.4). Subsequently, the plates were counted for radioactivity (counts per minute, CPM) in a scintillation counter (MicroBeta2 Plate Counter, PerkinElmer) using a scintillation cocktail (OptiPhase SuperMix; PerkinElmer).</p>
</sec>
<sec id="s0135">
<title>cAMP accumulation assay</title>
<p id="p0345">All cells were seeded into 384-well culture plates (4000 cells/well) and incubated for 24 h at 37 °C and 5% CO<sub>2</sub>. For the agonist assay, after 24 h, the culture medium was discarded and 5 µl cAMP stimulation buffer [calcium- and magnesium-free Hanks’ balanced salt solution (HBSS) buffer with 5 mM 4-(2-hydroxyethyl)-1-piperazineethanesulfonic acid (HEPES), 0.1% bovine serum albumin (BSA), and 0.5 mM 3-isobutyl-1-methylxanthine (IBMX)] was added to the cells. Subsequently, 5 µl compounds were introduced to simulate the cAMP reaction. For the PAM or antagonist assay, each well contained 2.5 µl cAMP stimulation buffer, 5 µl endogenous ligand (GLP-1, glucagon, or VIP) at various concentrations, and 2.5 µl testing compounds diluted in the cAMP assay buffer. After 40-min incubation at room temperature, cAMP levels were determined using the LANCE cAMP kit (Catalog No. TRF0264; PerkinElmer).</p>
</sec>
<sec id="s0140">
<title>Specificity verification and PDE inhibitor exclusion assay</title>
<p id="p0350">Two experiments were performed using the cAMP accumulation assay (antagonist mode) to study the specificity of the hit compounds acting on GCGR or VIPR. In the case of GCGR, glucagon was replaced by forskolin to investigate whether CD3400-G008 affects forskolin-induced cAMP accumulation. Forskolin concentration was gradually increased from 1.28 nM to 100 µM, whereas CD3400-G008 concentration was kept unchanged (20 µM). The PDE inhibitor exclusion assay was performed in CHO-K1 cells, in which IBMX-free stimulation buffer (calcium- and magnesium-free HBSS buffer with 5 mM HEPES and 0.1% BSA) was used. Concentrations of both IBMX (PDE inhibitor, positive control) and forskolin were gradually increased from 1.28 nM to 100 µM, and the agonistic effect of CD3349-F005 was examined at concentrations of 25 µM and 50 µM, respectively.</p>
</sec>
</sec>
</sec>
<sec id="s0145">
<title>Availability</title>
<p id="p0355">The source code of DeepCPI can be downloaded from <ext-link ext-link-type="uri" id="ir030" xlink:href="https://github.com/FangpingWan/DeepCPI">https://github.com/FangpingWan/DeepCPI</ext-link>.</p>
</sec>
<sec id="s0150">
<title>Authors’ contributions</title>
<p id="p0360">FW, DY, MWW, and JZ conceived the project. JZ, DY, and MWW supervised the project. FW and JZ designed the computational pipeline. FW implemented DeepCPI, and performed the model training and prediction validation tasks. HH and JZ analyzed the novel prediction results. YZ, AD, XC, and DY performed the experimental validation. DY and MWW analyzed the validation results. HH and HG performed the computational docking and data analysis. TX and LC helped analyze the prediction results. FW, HH, MWW, and JZ wrote the manuscript with input from all co-authors. All authors read and approved the final manuscript.</p>
</sec>
<sec id="s0155">
<title>Competing interests</title>
<p id="p0365">The authors have declared no competing interests.</p>
</sec>
</body>
<back>
<ref-list id="bi005">
<title>References</title>
<ref id="b0005">
<label>1</label>
<element-citation id="h0005" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keiser</surname>
<given-names>M.J.</given-names>
</name>
<name>
<surname>Setola</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Laggner</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Abbas</surname>
<given-names>A.I.</given-names>
</name>
<name>
<surname>Hufeisen</surname>
<given-names>S.J.</given-names>
</name>
</person-group>
<article-title>Predicting new molecular targets for known drugs</article-title>
<source/>Nature
          <volume>462</volume>
<year>2009</year>
<fpage>175</fpage>
<lpage>181</lpage>
<pub-id pub-id-type="pmid">19881490</pub-id>
</element-citation>
</ref>
<ref id="b0010">
<label>2</label>
<element-citation id="h0010" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lounkine</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Keiser</surname>
<given-names>M.J.</given-names>
</name>
<name>
<surname>Whitebread</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Mikhailov</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Hamon</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Jenkins</surname>
<given-names>J.L.</given-names>
</name>
</person-group>
<article-title>Large-scale prediction and testing of drug activity on side-effect targets</article-title>
<source/>Nature
          <volume>486</volume>
<year>2012</year>
<fpage>361</fpage>
<lpage>367</lpage>
<pub-id pub-id-type="pmid">22722194</pub-id>
</element-citation>
</ref>
<ref id="b0015">
<label>3</label>
<element-citation id="h0015" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Medina-Franco</surname>
<given-names>J.L.</given-names>
</name>
<name>
<surname>Giulianotti</surname>
<given-names>M.A.</given-names>
</name>
<name>
<surname>Welmaker</surname>
<given-names>G.S.</given-names>
</name>
<name>
<surname>Houghten</surname>
<given-names>R.A.</given-names>
</name>
</person-group>
<article-title>Shifting from the single to the multitarget paradigm in drug discovery</article-title>
<source/>Drug Discov Today
          <volume>18</volume>
<year>2013</year>
<fpage>495</fpage>
<lpage>501</lpage>
<pub-id pub-id-type="pmid">23340113</pub-id>
</element-citation>
</ref>
<ref id="b0020">
<label>4</label>
<element-citation id="h0020" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Walsh</surname>
<given-names>C.T.</given-names>
</name>
<name>
<surname>Fischbach</surname>
<given-names>M.A.</given-names>
</name>
</person-group>
<article-title>Repurposing libraries of eukaryotic protein kinase inhibitors for antibiotic discovery</article-title>
<source/>Proc Natl Acad Sci U S A
          <volume>106</volume>
<year>2009</year>
<fpage>1689</fpage>
<lpage>1690</lpage>
<pub-id pub-id-type="pmid">19193851</pub-id>
</element-citation>
</ref>
<ref id="b0025">
<label>5</label>
<element-citation id="h0025" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scannell</surname>
<given-names>J.W.</given-names>
</name>
<name>
<surname>Blanckley</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Boldon</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Warrington</surname>
<given-names>B.</given-names>
</name>
</person-group>
<article-title>Diagnosing the decline in pharmaceutical R&amp;D efficiency</article-title>
<source/>Nat Rev Drug Discov
          <volume>11</volume>
<year>2012</year>
<fpage>191</fpage>
<lpage>200</lpage>
<pub-id pub-id-type="pmid">22378269</pub-id>
</element-citation>
</ref>
<ref id="b0030">
<label>6</label>
<element-citation id="h0030" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keiser</surname>
<given-names>M.J.</given-names>
</name>
<name>
<surname>Roth</surname>
<given-names>B.L.</given-names>
</name>
<name>
<surname>Armbruster</surname>
<given-names>B.N.</given-names>
</name>
<name>
<surname>Ernsberger</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Shoichet</surname>
<given-names>B.K.</given-names>
</name>
</person-group>
<article-title>Relating protein pharmacology by ligand chemistry</article-title>
<source/>Nat Biotechnol
          <volume>25</volume>
<year>2007</year>
<fpage>197</fpage>
<lpage>206</lpage>
<pub-id pub-id-type="pmid">17287757</pub-id>
</element-citation>
</ref>
<ref id="b0035">
<label>7</label>
<element-citation id="h0035" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Martínez-Jiménez</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Marti-Renom</surname>
<given-names>M.A.</given-names>
</name>
</person-group>
<article-title>Ligand-target prediction by structural network biology using nAnnoLyze</article-title>
<source/>PLoS Comput Biol
          <volume>11</volume>
<year>2015</year>
<fpage>e1004157</fpage>
<pub-id pub-id-type="pmid">25816344</pub-id>
</element-citation>
</ref>
<ref id="b0040">
<label>8</label>
<element-citation id="h0040" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>van Laarhoven</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Nabuurs</surname>
<given-names>S.B.</given-names>
</name>
<name>
<surname>Marchiori</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Gaussian interaction profile kernels for predicting drug–target interaction</article-title>
<source/>Bioinformatics
          <volume>27</volume>
<year>2011</year>
<fpage>3036</fpage>
<lpage>3043</lpage>
<pub-id pub-id-type="pmid">21893517</pub-id>
</element-citation>
</ref>
<ref id="b0045">
<label>9</label>
<element-citation id="h0045" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bleakley</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Yamanishi</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Supervised prediction of drug–target interactions using bipartite local models</article-title>
<source/>Bioinformatics
          <volume>25</volume>
<year>2009</year>
<fpage>2397</fpage>
<lpage>2403</lpage>
<pub-id pub-id-type="pmid">19605421</pub-id>
</element-citation>
</ref>
<ref id="b0050">
<label>10</label>
<element-citation id="h0050" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xia</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>L.Y.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Wong</surname>
<given-names>S.T.</given-names>
</name>
</person-group>
<article-title>Semi-supervised drug-protein interaction prediction from heterogeneous biological spaces</article-title>
<source/>BMC Syst Biol
          <volume>4</volume>
<year>2010</year>
<fpage>S6</fpage>
</element-citation>
</ref>
<ref id="b0055">
<label>11</label>
<element-citation id="h0055" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Zeng</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Predicting drug-target interactions using restricted Boltzmann machines</article-title>
<source/>Bioinformatics
          <volume>29</volume>
<year>2013</year>
<fpage>i126</fpage>
<lpage>i134</lpage>
<pub-id pub-id-type="pmid">23812976</pub-id>
</element-citation>
</ref>
<ref id="b0060">
<label>12</label>
<element-citation id="h0060" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Luo</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Kuang</surname>
<given-names>W.</given-names>
</name>
</person-group>
<article-title>A network integration approach for drug-target interaction prediction and computational drug repositioning from heterogeneous information</article-title>
<source/>Nat Commun
          <volume>8</volume>
<year>2017</year>
<fpage>573</fpage>
<pub-id pub-id-type="pmid">28924171</pub-id>
</element-citation>
</ref>
<ref id="b0065">
<label>13</label>
<element-citation id="h0065" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hattori</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Okuno</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Goto</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Kanehisa</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Development of a chemical structure comparison method for integrated analysis of chemical and genomic information in the metabolic pathways</article-title>
<source/>J Am Chem Soc
          <volume>125</volume>
<year>2003</year>
<fpage>11853</fpage>
<lpage>11865</lpage>
<pub-id pub-id-type="pmid">14505407</pub-id>
</element-citation>
</ref>
<ref id="b0070">
<label>14</label>
<element-citation id="h0070" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Suzek</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>He</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Cheng</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>PubChem BioAssay: 2014 update</article-title>
<source/>Nucleic Acids Res
          <volume>42</volume>
<year>2014</year>
<fpage>D1075</fpage>
<lpage>D1082</lpage>
<pub-id pub-id-type="pmid">24198245</pub-id>
</element-citation>
</ref>
<ref id="b0075">
<label>15</label>
<element-citation id="h0075" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bengio</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Courville</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Vincent</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>Representation learning: a review and new perspectives</article-title>
<source/>IEEE Trans Pattern Anal Mach Intell
          <volume>35</volume>
<year>2013</year>
<fpage>1798</fpage>
<lpage>1828</lpage>
<pub-id pub-id-type="pmid">23787338</pub-id>
</element-citation>
</ref>
<ref id="b0080">
<label>16</label>
<mixed-citation id="h0080" publication-type="other">Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv 2013;1301.3781.</mixed-citation>
</ref>
<ref id="b0085">
<label>17</label>
<element-citation id="h0085" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Liang</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>Elastic restricted Boltzmann machines for cancer data analysis</article-title>
<source/>Quant Biol
          <volume>5</volume>
<year>2017</year>
<fpage>159</fpage>
<lpage>172</lpage>
</element-citation>
</ref>
<ref id="b0090">
<label>18</label>
<element-citation id="h0090" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hu</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Xiao</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Shi</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Jiang</surname>
<given-names>T.</given-names>
</name>
</person-group>
<article-title>DeepHINT: understanding HIV-1 integration via deep learning with attention</article-title>
<source/>Bioinformatics
          <volume>35</volume>
<year>2019</year>
<fpage>1660</fpage>
<lpage>1667</lpage>
<pub-id pub-id-type="pmid">30295703</pub-id>
</element-citation>
</ref>
<ref id="b0095">
<label>19</label>
<element-citation id="h0095" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>LeCun</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Bengio</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Hinton</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Deep learning</article-title>
<source/>Nature
          <volume>521</volume>
<year>2015</year>
<fpage>436</fpage>
<lpage>444</lpage>
<pub-id pub-id-type="pmid">26017442</pub-id>
</element-citation>
</ref>
<ref id="b0100">
<label>20</label>
<element-citation id="h0100" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Unterthiner</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Mayr</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Klambauer</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Steijaert</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Wegner</surname>
<given-names>J.K.</given-names>
</name>
<name>
<surname>Ceulemans</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Deep learning as an opportunity in virtual screening</article-title>
<source/>Workshop Deep Learn Represent Learn
          <volume>27</volume>
<year>2014</year>
<fpage>1</fpage>
<lpage>9</lpage>
</element-citation>
</ref>
<ref id="b0105">
<label>21</label>
<mixed-citation id="h0105" publication-type="other">Ramsundar B, Kearnes S, Riley P, Webster D, Konerding D, Pande V. Massively multitask networks for drug discovery. arXiv 2015;1502.02072.</mixed-citation>
</ref>
<ref id="b0110">
<label>22</label>
<element-citation id="h0110" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wan</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Hong</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Xiao</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Jiang</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Zeng</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>NeoDTI: neural integration of neighbor information from a heterogeneous network for discovering new drug–target interactions</article-title>
<source/>Bioinformatics
          <volume>35</volume>
<year>2019</year>
<fpage>104</fpage>
<lpage>111</lpage>
<pub-id pub-id-type="pmid">30561548</pub-id>
</element-citation>
</ref>
<ref id="b0115">
<label>23</label>
<mixed-citation id="h0115" publication-type="other">Wallach I, Dzamba M, Heifets A. AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery. arXiv 2015;1510.02855.</mixed-citation>
</ref>
<ref id="b0120">
<label>24</label>
<element-citation id="h0120" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wen</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Niu</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Sha</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Yun</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Deep-learning-based drug–target interaction prediction</article-title>
<source/>J Proteome Res
          <volume>16</volume>
<year>2017</year>
<fpage>1401</fpage>
<lpage>1409</lpage>
<pub-id pub-id-type="pmid">28264154</pub-id>
</element-citation>
</ref>
<ref id="b0125">
<label>25</label>
<element-citation id="h0125" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Öztürk</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Özgür</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Ozkirimli</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>DeepDTA: deep drug–target binding affinity prediction</article-title>
<source/>Bioinformatics
          <volume>34</volume>
<year>2018</year>
<fpage>i821</fpage>
<lpage>i829</lpage>
<pub-id pub-id-type="pmid">30423097</pub-id>
</element-citation>
</ref>
<ref id="b0130">
<label>26</label>
<element-citation id="h0130" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tsubaki</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Tomii</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Sese</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Compound–protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</article-title>
<source/>Bioinformatics
          <volume>35</volume>
<year>2019</year>
<fpage>309</fpage>
<lpage>318</lpage>
<pub-id pub-id-type="pmid">29982330</pub-id>
</element-citation>
</ref>
<ref id="b0135">
<label>27</label>
<element-citation id="h0135" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Deerwester</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Dumais</surname>
<given-names>S.T.</given-names>
</name>
<name>
<surname>Furnas</surname>
<given-names>G.W.</given-names>
</name>
<name>
<surname>Landauer</surname>
<given-names>T.K.</given-names>
</name>
<name>
<surname>Harshman</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Indexing by latent semantic analysis</article-title>
<source/>J Am Soc Inf Sci
          <volume>41</volume>
<year>1990</year>
<fpage>391</fpage>
<lpage>407</lpage>
</element-citation>
</ref>
<ref id="b0140">
<label>28</label>
<element-citation id="h0140" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mikolov</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Sutskever</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Corrado</surname>
<given-names>G.S.</given-names>
</name>
<name>
<surname>Dean</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Distributed representations of words and phrases and their compositionality</article-title>
<source/>Adv Neural Inf Process Syst
          <volume>26</volume>
<year>2013</year>
<fpage>3111</fpage>
<lpage>3119</lpage>
</element-citation>
</ref>
<ref id="b0145">
<label>29</label>
<element-citation id="h0145" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Asgari</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Mofrad</surname>
<given-names>M.R.</given-names>
</name>
</person-group>
<article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>
<source/>PLoS One
          <volume>10</volume>
<year>2015</year>
<fpage>e0141287</fpage>
<pub-id pub-id-type="pmid">26555596</pub-id>
</element-citation>
</ref>
<ref id="b0150">
<label>30</label>
<element-citation id="h0150" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bento</surname>
<given-names>A.P.</given-names>
</name>
<name>
<surname>Gaulton</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Hersey</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Bellis</surname>
<given-names>L.J.</given-names>
</name>
<name>
<surname>Chambers</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Davies</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>The ChEMBL bioactivity database: an update</article-title>
<source/>Nucleic Acids Res
          <volume>42</volume>
<year>2014</year>
<fpage>D1083</fpage>
<lpage>D1090</lpage>
<pub-id pub-id-type="pmid">24214965</pub-id>
</element-citation>
</ref>
<ref id="b0155">
<label>31</label>
<element-citation id="h0155" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Lin</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Wen</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Jorissen</surname>
<given-names>R.N.</given-names>
</name>
<name>
<surname>Gilson</surname>
<given-names>M.K.</given-names>
</name>
</person-group>
<article-title>BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</article-title>
<source/>Nucleic Acids Res
          <volume>35</volume>
<year>2006</year>
<fpage>D198</fpage>
<lpage>D201</lpage>
<pub-id pub-id-type="pmid">17145705</pub-id>
</element-citation>
</ref>
<ref id="b0160">
<label>32</label>
<element-citation id="h0160" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wishart</surname>
<given-names>D.S.</given-names>
</name>
<name>
<surname>Knox</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Guo</surname>
<given-names>A.C.</given-names>
</name>
<name>
<surname>Shrivastava</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Hassanali</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Stothard</surname>
<given-names>P.</given-names>
</name>
</person-group>
<article-title>DrugBank: a comprehensive resource for in silico drug discovery and exploration</article-title>
<source/>Nucleic Acids Res
          <volume>34</volume>
<year>2006</year>
<fpage>D668</fpage>
<lpage>D672</lpage>
<pub-id pub-id-type="pmid">16381955</pub-id>
</element-citation>
</ref>
<ref id="b0165">
<label>33</label>
<element-citation id="h0165" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Salvat</surname>
<given-names>R.S.</given-names>
</name>
<name>
<surname>Parker</surname>
<given-names>A.S.</given-names>
</name>
<name>
<surname>Choi</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Bailey-Kellogg</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Griswold</surname>
<given-names>K.E.</given-names>
</name>
</person-group>
<article-title>Mapping the Pareto optimal design space for a functionally deimmunized biotherapeutic candidate</article-title>
<source/>PLoS Comput Biol
          <volume>11</volume>
<year>2015</year>
<fpage>e1003988</fpage>
<pub-id pub-id-type="pmid">25568954</pub-id>
</element-citation>
</ref>
<ref id="b0170">
<label>34</label>
<element-citation id="h0170" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>van Laarhoven</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Marchiori</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Biases of drug–target interaction network data</article-title>
<source/>IAPR Inter Conf Pattern Recogn Bioinformatics
          <year>2014</year>
<fpage>23</fpage>
<lpage>33</lpage>
</element-citation>
</ref>
<ref id="b0175">
<label>35</label>
<element-citation id="h0175" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogers</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Hahn</surname>
<given-names>M.</given-names>
</name>
</person-group>
<article-title>Extended-connectivity fingerprints</article-title>
<source/>J Chem Inf Model
          <volume>50</volume>
<year>2010</year>
<fpage>742</fpage>
<lpage>754</lpage>
<pub-id pub-id-type="pmid">20426451</pub-id>
</element-citation>
</ref>
<ref id="b0180">
<label>36</label>
<element-citation id="h0180" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lvd</surname>
<given-names>Maaten</given-names>
</name>
<name>
<surname>Hinton</surname>
<given-names>G.</given-names>
</name>
</person-group>
<article-title>Visualizing data using t-SNE</article-title>
<source/>J Mach Learn Res
          <volume>9</volume>
<year>2008</year>
<fpage>2579</fpage>
<lpage>2625</lpage>
</element-citation>
</ref>
<ref id="b0185">
<label>37</label>
<element-citation id="h0185" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mysinger</surname>
<given-names>M.M.</given-names>
</name>
<name>
<surname>Carchia</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Shoichet</surname>
<given-names>B.K.</given-names>
</name>
</person-group>
<article-title>Directory of useful decoys, enhanced (DUD-E): better ligands and decoys for better benchmarking</article-title>
<source/>J Med Chem
          <volume>55</volume>
<year>2012</year>
<fpage>6582</fpage>
<lpage>6594</lpage>
<pub-id pub-id-type="pmid">22716043</pub-id>
</element-citation>
</ref>
<ref id="b0190">
<label>38</label>
<element-citation id="h0190" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>UniProt Consortium</surname>
</name>
</person-group>
<article-title>UniProt: a hub for protein information</article-title>
<source/>Nucleic Acids Res
          <volume>43</volume>
<year>2015</year>
<fpage>D204</fpage>
<lpage>D212</lpage>
<pub-id pub-id-type="pmid">25348405</pub-id>
</element-citation>
</ref>
<ref id="b0195">
<label>39</label>
<element-citation id="h0195" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cornil</surname>
<given-names>C.A.</given-names>
</name>
<name>
<surname>Ball</surname>
<given-names>G.F.</given-names>
</name>
</person-group>
<article-title>Interplay among catecholamine systems: dopamine binds to α2-adrenergic receptors in birds and mammals</article-title>
<source/>J Comp Neurol
          <volume>511</volume>
<year>2008</year>
<fpage>610</fpage>
<lpage>627</lpage>
<pub-id pub-id-type="pmid">18924139</pub-id>
</element-citation>
</ref>
<ref id="b0200">
<label>40</label>
<element-citation id="h0200" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cornil</surname>
<given-names>C.A.</given-names>
</name>
<name>
<surname>Castelino</surname>
<given-names>C.B.</given-names>
</name>
<name>
<surname>Ball</surname>
<given-names>G.F.</given-names>
</name>
</person-group>
<article-title>Dopamine binds to α2-adrenergic receptors in the song control system of zebra finches (<italic>Taeniopygia guttata</italic>)</article-title>
<source/>J Chem Neuroanat
          <volume>35</volume>
<year>2008</year>
<fpage>202</fpage>
<lpage>215</lpage>
<pub-id pub-id-type="pmid">18155403</pub-id>
</element-citation>
</ref>
<ref id="b0205">
<label>41</label>
<element-citation id="h0205" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Von Coburg</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Kottke</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Weizel</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Ligneau</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Stark</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>Potential utility of histamine H3 receptor antagonist pharmacophore in antipsychotics</article-title>
<source/>Bioorg Med Chem Lett
          <volume>19</volume>
<year>2009</year>
<fpage>538</fpage>
<lpage>542</lpage>
<pub-id pub-id-type="pmid">19091563</pub-id>
</element-citation>
</ref>
<ref id="b0210">
<label>42</label>
<element-citation id="h0210" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taylor</surname>
<given-names>J.E.</given-names>
</name>
<name>
<surname>Richelson</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>High affinity binding of tricyclic antidepressants to histamine H1-receptors: fact and artifact</article-title>
<source/>Eur J Pharmacol
          <volume>67</volume>
<year>1980</year>
<fpage>41</fpage>
<lpage>46</lpage>
<pub-id pub-id-type="pmid">6106553</pub-id>
</element-citation>
</ref>
<ref id="b0215">
<label>43</label>
<element-citation id="h0215" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hellings</surname>
<given-names>J.A.</given-names>
</name>
<name>
<surname>Arnold</surname>
<given-names>L.E.</given-names>
</name>
<name>
<surname>Han</surname>
<given-names>J.C.</given-names>
</name>
</person-group>
<article-title>Dopamine antagonists for treatment resistance in autism spectrum disorders: review and focus on BDNF stimulators loxapine and amitriptyline</article-title>
<source/>Expert Opin Pharmacother
          <volume>18</volume>
<year>2017</year>
<fpage>581</fpage>
<lpage>588</lpage>
<pub-id pub-id-type="pmid">28335658</pub-id>
</element-citation>
</ref>
<ref id="b0220">
<label>44</label>
<element-citation id="h0220" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schmoutz</surname>
<given-names>C.D.</given-names>
</name>
<name>
<surname>Guerin</surname>
<given-names>G.F.</given-names>
</name>
<name>
<surname>Goeders</surname>
<given-names>N.E.</given-names>
</name>
</person-group>
<article-title>Role of GABA-active neurosteroids in the efficacy of metyrapone against cocaine addiction</article-title>
<source/>Behav Brain Res
          <volume>271</volume>
<year>2014</year>
<fpage>269</fpage>
<lpage>276</lpage>
<pub-id pub-id-type="pmid">24959859</pub-id>
</element-citation>
</ref>
<ref id="b0225">
<label>45</label>
<element-citation id="h0225" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Spence</surname>
<given-names>A.L.</given-names>
</name>
<name>
<surname>Guerin</surname>
<given-names>G.F.</given-names>
</name>
<name>
<surname>Goeders</surname>
<given-names>N.E.</given-names>
</name>
</person-group>
<article-title>The differential effects of alprazolam and oxazepam on methamphetamine self-administration in rats</article-title>
<source/>Drug Alcohol Depend
          <volume>166</volume>
<year>2016</year>
<fpage>209</fpage>
<lpage>217</lpage>
<pub-id pub-id-type="pmid">27485488</pub-id>
</element-citation>
</ref>
<ref id="b0230">
<label>46</label>
<element-citation id="h0230" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scriabine</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Korol</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Kondratas</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Yu</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>P'an</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Schneider</surname>
<given-names>J.</given-names>
</name>
</person-group>
<article-title>Pharmacological studies with polythiazide, a new diuretic and antihypertensive agent</article-title>
<source/>Proc Soc Exp Biol Med
          <volume>107</volume>
<year>1961</year>
<fpage>864</fpage>
<lpage>872</lpage>
<pub-id pub-id-type="pmid">13910047</pub-id>
</element-citation>
</ref>
<ref id="b0235">
<label>47</label>
<element-citation id="h0235" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gueorguieva</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Jackson</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Wrighton</surname>
<given-names>S.A.</given-names>
</name>
<name>
<surname>Sinha</surname>
<given-names>V.P.</given-names>
</name>
<name>
<surname>Chien</surname>
<given-names>J.Y.</given-names>
</name>
</person-group>
<article-title>Desipramine, substrate for CYP2D6 activity: population pharmacokinetic model and design elements of drug–drug interaction trials</article-title>
<source/>Br J Clin Pharmacol
          <volume>70</volume>
<year>2010</year>
<fpage>523</fpage>
<lpage>536</lpage>
<pub-id pub-id-type="pmid">20840444</pub-id>
</element-citation>
</ref>
<ref id="b0240">
<label>48</label>
<element-citation id="h0240" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Spina</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Gitto</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Avenoso</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Campo</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Caputi</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Perucca</surname>
<given-names>E.</given-names>
</name>
</person-group>
<article-title>Relationship between plasma desipramine levels, CYP2D6 phenotype and clinical response to desipramine: a prospective study</article-title>
<source/>Eur J Clin Pharmacol
          <volume>51</volume>
<year>1997</year>
<fpage>395</fpage>
<lpage>398</lpage>
<pub-id pub-id-type="pmid">9049581</pub-id>
</element-citation>
</ref>
<ref id="b0245">
<label>49</label>
<element-citation id="h0245" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Reese</surname>
<given-names>M.J.</given-names>
</name>
<name>
<surname>Wurm</surname>
<given-names>R.M.</given-names>
</name>
<name>
<surname>Muir</surname>
<given-names>K.T.</given-names>
</name>
<name>
<surname>Generaux</surname>
<given-names>G.T.</given-names>
</name>
<name>
<surname>John-Williams</surname>
<given-names>L.S.</given-names>
</name>
<name>
<surname>Mcconn</surname>
<given-names>D.J.</given-names>
</name>
</person-group>
<article-title>An in vitro mechanistic study to elucidate the desipramine/bupropion clinical drug-drug interaction</article-title>
<source/>Drug Metab Dispos
          <volume>36</volume>
<year>2008</year>
<fpage>1198</fpage>
<lpage>1201</lpage>
<pub-id pub-id-type="pmid">18420781</pub-id>
</element-citation>
</ref>
<ref id="b0250">
<label>50</label>
<element-citation id="h0250" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stevens</surname>
<given-names>R.C.</given-names>
</name>
<name>
<surname>Cherezov</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Katritch</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Abagyan</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Kuhn</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Rosen</surname>
<given-names>H.</given-names>
</name>
</person-group>
<article-title>The GPCR Network: a large-scale collaboration to determine human GPCR structure and function</article-title>
<source/>Nat Rev Drug Discov
          <volume>12</volume>
<year>2013</year>
<fpage>25</fpage>
<lpage>34</lpage>
<pub-id pub-id-type="pmid">23237917</pub-id>
</element-citation>
</ref>
<ref id="b0255">
<label>51</label>
<element-citation id="h0255" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Filmore</surname>
<given-names>D.</given-names>
</name>
</person-group>
<article-title>It's a GPCR world</article-title>
<source/>Mod Drug Discovery
          <volume>7</volume>
<year>2004</year>
<fpage>24</fpage>
<lpage>28</lpage>
</element-citation>
</ref>
<ref id="b0260">
<label>52</label>
<element-citation id="h0260" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sterling</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
</person-group>
<article-title>ZINC 15–ligand discovery for everyone</article-title>
<source/>J Chem Inf Model
          <volume>55</volume>
<year>2015</year>
<fpage>2324</fpage>
<lpage>2337</lpage>
<pub-id pub-id-type="pmid">26479676</pub-id>
</element-citation>
</ref>
<ref id="b0265">
<label>53</label>
<element-citation id="h0265" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Sterling</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Mysinger</surname>
<given-names>M.M.</given-names>
</name>
<name>
<surname>Bolstad</surname>
<given-names>E.S.</given-names>
</name>
<name>
<surname>Coleman</surname>
<given-names>R.G.</given-names>
</name>
</person-group>
<article-title>ZINC: a free tool to discover chemistry for biology</article-title>
<source/>J Chem Inf Model
          <volume>52</volume>
<year>2012</year>
<fpage>1757</fpage>
<lpage>1768</lpage>
<pub-id pub-id-type="pmid">22587354</pub-id>
</element-citation>
</ref>
<ref id="b0270">
<label>54</label>
<element-citation id="h0270" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Irwin</surname>
<given-names>J.J.</given-names>
</name>
<name>
<surname>Shoichet</surname>
<given-names>B.K.</given-names>
</name>
</person-group>
<article-title>ZINC-a free database of commercially available compounds for virtual screening</article-title>
<source/>J Chem Inf Model
          <volume>45</volume>
<year>2005</year>
<fpage>177</fpage>
<lpage>182</lpage>
<pub-id pub-id-type="pmid">15667143</pub-id>
</element-citation>
</ref>
<ref id="b0275">
<label>55</label>
<element-citation id="h0275" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roth</surname>
<given-names>J.D.</given-names>
</name>
<name>
<surname>Erickson</surname>
<given-names>M.R.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Parkes</surname>
<given-names>D.G.</given-names>
</name>
</person-group>
<article-title>GLP-1R and amylin agonism in metabolic disease: complementary mechanisms and future opportunities</article-title>
<source/>Br J Pharmacol
          <volume>166</volume>
<year>2012</year>
<fpage>121</fpage>
<lpage>136</lpage>
<pub-id pub-id-type="pmid">21671898</pub-id>
</element-citation>
</ref>
<ref id="b0280">
<label>56</label>
<element-citation id="h0280" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Munro</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Skrobot</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Sanyoura</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Kay</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Susce</surname>
<given-names>M.T.</given-names>
</name>
<name>
<surname>Glaser</surname>
<given-names>P.E.</given-names>
</name>
</person-group>
<article-title>Relaxin polymorphisms associated with metabolic disturbance in patients treated with antipsychotics</article-title>
<source/>J Psychopharmacol
          <volume>26</volume>
<year>2012</year>
<fpage>374</fpage>
<lpage>379</lpage>
<pub-id pub-id-type="pmid">21693553</pub-id>
</element-citation>
</ref>
<ref id="b0285">
<label>57</label>
<element-citation id="h0285" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Trott</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>A.J.</given-names>
</name>
</person-group>
<article-title>AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading</article-title>
<source/>J Comput Chem
          <volume>31</volume>
<year>2010</year>
<fpage>455</fpage>
<lpage>461</lpage>
<pub-id pub-id-type="pmid">19499576</pub-id>
</element-citation>
</ref>
<ref id="b0290">
<label>58</label>
<element-citation id="h0290" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jazayeri</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Rappas</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Brown</surname>
<given-names>A.J.</given-names>
</name>
<name>
<surname>Kean</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Errey</surname>
<given-names>J.C.</given-names>
</name>
<name>
<surname>Robertson</surname>
<given-names>N.J.</given-names>
</name>
</person-group>
<article-title>Crystal structure of the GLP-1 receptor bound to a peptide agonist</article-title>
<source/>Nature
          <volume>546</volume>
<year>2017</year>
<fpage>254</fpage>
<lpage>258</lpage>
<pub-id pub-id-type="pmid">28562585</pub-id>
</element-citation>
</ref>
<ref id="b0295">
<label>59</label>
<element-citation id="h0295" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Song</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>de Graaf</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Jiang</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>Human GLP-1 receptor transmembrane domain structure in complex with allosteric modulators</article-title>
<source/>Nature
          <volume>546</volume>
<year>2017</year>
<fpage>312</fpage>
<lpage>315</lpage>
<pub-id pub-id-type="pmid">28514449</pub-id>
</element-citation>
</ref>
<ref id="b0300">
<label>60</label>
<element-citation id="h0300" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sloop</surname>
<given-names>K.W.</given-names>
</name>
<name>
<surname>Willard</surname>
<given-names>F.S.</given-names>
</name>
<name>
<surname>Brenner</surname>
<given-names>M.B.</given-names>
</name>
<name>
<surname>Ficorilli</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Valasek</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Showalter</surname>
<given-names>A.D.</given-names>
</name>
</person-group>
<article-title>Novel small molecule glucagon-like peptide-1 receptor agonist stimulates insulin secretion in rodents and from human islets</article-title>
<source/>Diabetes
          <volume>59</volume>
<year>2010</year>
<fpage>3099</fpage>
<lpage>3107</lpage>
<pub-id pub-id-type="pmid">20823098</pub-id>
</element-citation>
</ref>
<ref id="b0305">
<label>61</label>
<element-citation id="h0305" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nolte</surname>
<given-names>W.M.</given-names>
</name>
<name>
<surname>Fortin</surname>
<given-names>J.-P.</given-names>
</name>
<name>
<surname>Stevens</surname>
<given-names>B.D.</given-names>
</name>
<name>
<surname>Aspnes</surname>
<given-names>G.E.</given-names>
</name>
<name>
<surname>Griffith</surname>
<given-names>D.A.</given-names>
</name>
<name>
<surname>Hoth</surname>
<given-names>L.R.</given-names>
</name>
</person-group>
<article-title>A potentiator of orthosteric ligand activity at GLP-1R acts via covalent modification</article-title>
<source/>Nat Chem Biol
          <volume>10</volume>
<year>2014</year>
<fpage>629</fpage>
<lpage>631</lpage>
<pub-id pub-id-type="pmid">24997604</pub-id>
</element-citation>
</ref>
<ref id="b0310">
<label>62</label>
<element-citation id="h0310" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Su</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>He</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Boc5, a non-peptidic glucagon-like peptide-1 receptor agonist, invokes sustained glycemic control and weight loss in diabetic mice</article-title>
<source/>PLoS One
          <volume>3</volume>
<year>2008</year>
<fpage>e2892</fpage>
<pub-id pub-id-type="pmid">18682834</pub-id>
</element-citation>
</ref>
<ref id="b0315">
<label>63</label>
<element-citation id="h0315" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>He</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Su</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Gao</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Johansson</surname>
<given-names>S.M.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>X.</given-names>
</name>
</person-group>
<article-title>Reversal of obesity and insulin resistance by a non-peptidic glucagon-like peptide-1 receptor agonist in diet-induced obese mice</article-title>
<source/>PLoS One
          <volume>5</volume>
<year>2010</year>
<fpage>e14205</fpage>
<pub-id pub-id-type="pmid">21151924</pub-id>
</element-citation>
</ref>
<ref id="b0320">
<label>64</label>
<element-citation id="h0320" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>He</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Guan</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Gao</surname>
<given-names>W.W.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Q.</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>X.Y.</given-names>
</name>
<name>
<surname>Ma</surname>
<given-names>D.W.</given-names>
</name>
</person-group>
<article-title>A continued saga of Boc5, the first non-peptidic glucagon-like peptide-1 receptor agonist with in vivo activities</article-title>
<source/>Acta Pharmacol Sin
          <volume>33</volume>
<year>2012</year>
<fpage>148</fpage>
<lpage>154</lpage>
<pub-id pub-id-type="pmid">22301855</pub-id>
</element-citation>
</ref>
<ref id="b0325">
<label>65</label>
<mixed-citation id="h0325" publication-type="other">Wan F, Zeng J. Deep learning with feature embedding for compound-protein interaction prediction. bioRxiv 2016;086033.</mixed-citation>
</ref>
<ref id="b0330">
<label>66</label>
<element-citation id="h0330" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bradford</surname>
<given-names>R.B.</given-names>
</name>
</person-group>
<article-title>An empirical study of required dimensionality for large-scale latent semantic indexing applications</article-title>
<source/>Proc ACM Int Conf Inf Knowl Manag
          <year>2008</year>
<fpage>153</fpage>
<lpage>162</lpage>
</element-citation>
</ref>
<ref id="b0335">
<label>67</label>
<element-citation id="h0335" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Iyyer</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Manjunatha</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Boyd-Graber</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Daumé</surname>
<given-names>H.</given-names>
<suffix>III</suffix>
</name>
</person-group>
<article-title>Deep unordered composition rivals syntactic methods for text classification</article-title>
<source/>Proc Conf Assoc Comput Linguist Meet
          <volume>1</volume>
<year>2015</year>
<fpage>1681</fpage>
<lpage>1691</lpage>
</element-citation>
</ref>
<ref id="b0340">
<label>68</label>
<element-citation id="h0340" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rose</surname>
<given-names>P.W.</given-names>
</name>
<name>
<surname>Prlić</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Bi</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Bluhm</surname>
<given-names>W.F.</given-names>
</name>
<name>
<surname>Christie</surname>
<given-names>C.H.</given-names>
</name>
<name>
<surname>Dutta</surname>
<given-names>S.</given-names>
</name>
</person-group>
<article-title>The RCSB Protein Data Bank: views of structural biology for basic and applied research and education</article-title>
<source/>Nucleic Acids Res
          <volume>43</volume>
<year>2015</year>
<fpage>D345</fpage>
<lpage>D356</lpage>
<pub-id pub-id-type="pmid">25428375</pub-id>
</element-citation>
</ref>
<ref id="b0345">
<label>69</label>
<element-citation id="h0345" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Glorot</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Bordes</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Bengio</surname>
<given-names>Y.</given-names>
</name>
</person-group>
<article-title>Deep sparse rectifier neural networks</article-title>
<source/>Proc 14th Int Conf Artif Intell Stat
          <volume>15</volume>
<year>2011</year>
<fpage>315</fpage>
<lpage>323</lpage>
</element-citation>
</ref>
<ref id="b0350">
<label>70</label>
<element-citation id="h0350" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Srivastava</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Salakhutdinov</surname>
<given-names>R.R.</given-names>
</name>
</person-group>
<article-title>Multimodal learning with deep boltzmann machines</article-title>
<source/>Adv Neural Inf Process Syst
          <volume>2</volume>
<year>2012</year>
<fpage>2222</fpage>
<lpage>2230</lpage>
</element-citation>
</ref>
<ref id="b0355">
<label>71</label>
<element-citation id="h0355" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Srivastava</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Hinton</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Krizhevsky</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Sutskever</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Salakhutdinov</surname>
<given-names>R.</given-names>
</name>
</person-group>
<article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
<source/>J Mach Learn Res
          <volume>15</volume>
<year>2014</year>
<fpage>1929</fpage>
<lpage>1958</lpage>
</element-citation>
</ref>
<ref id="b0360">
<label>72</label>
<mixed-citation id="h0360" publication-type="other">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv 2015;1502.03167.</mixed-citation>
</ref>
<ref id="b0365">
<label>73</label>
<element-citation id="h0365" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>T.F.</given-names>
</name>
<name>
<surname>Waterman</surname>
<given-names>M.S.</given-names>
</name>
</person-group>
<article-title>Identification of common molecular subsequences</article-title>
<source/>J Mol Biol
          <volume>147</volume>
<year>1981</year>
<fpage>195</fpage>
<lpage>197</lpage>
<pub-id pub-id-type="pmid">7265238</pub-id>
</element-citation>
</ref>
<ref id="b0370">
<label>74</label>
<mixed-citation id="h0370" publication-type="other">Rehurek R, Sojka P. Software framework for topic modelling with large corpora. Proc LREC 2010 Workshop New Challenges NLP Frameworks 2010.</mixed-citation>
</ref>
<ref id="b0375">
<label>75</label>
<element-citation id="h0375" publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oshiro</surname>
<given-names>T.M.</given-names>
</name>
<name>
<surname>Perez</surname>
<given-names>P.S.</given-names>
</name>
<name>
<surname>Baranauskas</surname>
<given-names>J.A.</given-names>
</name>
</person-group>
<article-title>How many trees in a random forest?</article-title>
<source/>Int Workshop Mach Learn Data Mining Pattern Recogn
          <year>2012</year>
<fpage>154</fpage>
<lpage>168</lpage>
</element-citation>
</ref>
</ref-list>
<sec id="s0165" sec-type="supplementary-material">
<title>Supplementary material</title>
<p id="p0380">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0070"><caption><title>Supplementary Figure S1</title><p><bold>Schematic of the compound feature embedding module (extraction of compound features)</bold> The substructures of each compound from a compound corpus are generated by Morgan fingerprints with a radius of one. The term (i.e., substructure) frequency-inverse document (i.e., compound) frequency information of all compounds in the corpus forms a <inline-formula><mml:math altimg="si166.svg" id="M223"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula> matrix, where <inline-formula><mml:math altimg="si167.svg" id="M224"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math altimg="si131.svg" id="M225"><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>are the total numbers of substructures and compounds in the corpus, respectively. Subsequently, SVD is used to decompose this <inline-formula><mml:math altimg="si166.svg" id="M226"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula> matrix, and all eigenvectors corresponding to the largest eigenvalues,<inline-formula><mml:math altimg="si70.svg" id="M227"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>, are selected to construct a<inline-formula><mml:math altimg="si168.svg" id="M228"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math altimg="si169.svg" id="M229"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math></inline-formula>, where <inline-formula><mml:math altimg="si45.svg" id="M230"><mml:mi>d</mml:mi></mml:math></inline-formula> is smaller than <inline-formula><mml:math altimg="si167.svg" id="M231"><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:math></inline-formula>. This matrix <inline-formula><mml:math altimg="si169.svg" id="M232"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:math></inline-formula> can be precomputed and fixed after training. Then, for any new compound, its low-dimensional embedded feature vector can be easily obtained by simply left multiplying its tf-idf by<inline-formula><mml:math altimg="si170.svg" id="M233"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo mathvariant="bold-italic">⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Details can be found in the Materials and methods section. tf-idf, term frequency-inverse document frequency; SVD, singular value decomposition.</p></caption><media xlink:href="mmc1.pdf"></media></supplementary-material></p>
<p id="p0385">
<supplementary-material content-type="local-data" id="m0065">
<caption>
<title>Supplementary Figure S2</title>
<p><bold>Schematic of the protein feature embedding module (extraction of protein features)</bold> We first build a “word”-embedding set. In particular, we split each protein sequence from a corpus and represent it with three “sentences,” starting from the first, second, and third residue positions. Every three non-overlapping residues in a “sentence” are regarded as a “word.” For each “word” <inline-formula><mml:math altimg="si61.svg" id="M234"><mml:mi>w</mml:mi></mml:math></inline-formula>, all other “words” within a context window of size <inline-formula><mml:math altimg="si54.svg" id="M235"><mml:mi>b</mml:mi></mml:math></inline-formula> and centered at <inline-formula><mml:math altimg="si61.svg" id="M236"><mml:mi>w</mml:mi></mml:math></inline-formula> in any “sentence” are termed “the contexts” (or neighbors) of <inline-formula><mml:math altimg="si61.svg" id="M237"><mml:mi>w</mml:mi></mml:math></inline-formula>. The embeddings (i.e., the low-dimensional feature vectors) of “words” are learnt by maximizing the likelihood of their context words, while minimizing the likelihood of random “words” representing the background (i.e., maximizing Equation 2 in the main text). All “word”-embedding pairs are stored in an embedding set (implemented by a hash table). For a given protein sequence, its embedding is constructed by summing and averaging the embedding of all “words” in all three possible encoded “sentences.” The embedding of “words” can be precomputed and fixed after training. Details can be found in the Materials and methods section.</p>
</caption>
<media xlink:href="mmc2.pdf"></media>
</supplementary-material>
</p>
<p id="p0390">
<supplementary-material content-type="local-data" id="m0060">
<caption>
<title>Supplementary Figure S3</title>
<p><bold>Statistics for mapped affinities of drug-target pairs from DrugBank</bold> Mapping the known interacting drug–target pairs obtained from DrugBank to the corresponding compound–protein pairs obtained from ChEMBL. Ranges of the measured values of IC50 and K<sub>i</sub> in ChEMBL and their corresponding numbers of known interacting drug–target pairs mapped from DrugBank are plotted.</p>
</caption>
<media xlink:href="mmc3.pdf"></media>
</supplementary-material>
</p>
<p id="p0395">
<supplementary-material content-type="local-data" id="m0055">
<caption>
<title>Supplementary Figure S4</title>
<p><bold>Additional performance evaluation of DeepCPI</bold> Supplementary results to Figure 2 on the performance comparison between different prediction methods. Unlike in Figure 2, the redundant proteins and compounds were not removed. All results are summarized over 10 trials and expressed as mean ± SD. SLNN, single-layer neural network; DNN, deep neural network; AUROC, area under receiver operating characteristic curve; AUPRC, area under precision-recall curve.</p>
</caption>
<media xlink:href="mmc4.pdf"></media>
</supplementary-material>
</p>
<p id="p0400">
<supplementary-material content-type="local-data" id="m0050">
<caption>
<title>Supplementary Figure S5</title>
<p><bold>The t-SNE visualization of input features versus hidden features of the DNN</bold> Visualization of the distribution of positive and negative examples using t-SNE on the original 300-dimensional input features of the DNN (left) and the features represented by the last hidden layer of the DNN (right). DNN was trained on the ChEMBL dataset, and a combination of 5000 positive and 5000 negative examples randomly selected from BindingDB served as the test data and was also used for the visualization.</p>
</caption>
<media xlink:href="mmc5.pdf"></media>
</supplementary-material>
</p>
<p id="p0405">
<fig id="f0045" position="anchor">
<label>Supplementary Figure S6</label>
<caption>
<p>Chemical structures of the compounds used in this study</p>
</caption>
<graphic xlink:href="fx1"></graphic>
</fig>
</p>
<p id="p0410">
<supplementary-material content-type="local-data" id="m0045">
<caption>
<title>Supplementary Figure S7</title>
<p><bold>The docking result of the NNC0640-GLP-1R complex</bold> Recovery of the experimentally solved complex structure between NNC0640 and GLP-1R (PDB ID: 5VEX) using our docking method. The experimentally determined and docked positions of NNC0640 are shown in magenta and yellow, respectively. Blue and red balls represent nitrogen and oxygen atoms, respectively.</p>
</caption>
<media xlink:href="mmc6.pdf"></media>
</supplementary-material>
</p>
<p id="p0415">
<supplementary-material content-type="local-data" id="m0040">
<caption>
<title>Supplementary Figure S8</title>
<p><bold>The docking results of three predicted PAMs binding to GLP-1R</bold> Docked poses of the predicted interactions between three PAMs and GLP-1R (PDB ID: 5NX2). Local views of the overall docked structure of JK0580-H009 (<bold>A</bold>), CD3878-F005 (<bold>B</bold>), and CD3293-E005 (<bold>C</bold>). For all panels, blue and red balls represent nitrogen and oxygen atoms, respectively, whereas blue and red threads represent nitrogen and carbon atoms from GLP-1R, respectively. Yellow (<bold>A</bold>), turquoise (<bold>B</bold>), and brown (<bold>C</bold>) balls represent carbon atoms, whereas the ball in white (<bold>B</bold>) represents a hydrogen atom and the balls in azure (<bold>C</bold>) represent fluorine atoms.</p>
</caption>
<media xlink:href="mmc7.pdf"></media>
</supplementary-material>
</p>
<p id="p0420">
<supplementary-material content-type="local-data" id="m0035">
<caption>
<title>Supplementary Table S1</title>
</caption>
<media xlink:href="mmc8.rtf"></media>
</supplementary-material>
</p>
<p id="p0425">
<supplementary-material content-type="local-data" id="m0030">
<caption>
<title>Supplementary Table S2</title>
</caption>
<media xlink:href="mmc9.rtf"></media>
</supplementary-material>
</p>
<p id="p0430">
<supplementary-material content-type="local-data" id="m0025">
<caption>
<title>Supplementary Table S3</title>
</caption>
<media xlink:href="mmc10.rtf"></media>
</supplementary-material>
</p>
<p id="p0435">
<supplementary-material content-type="local-data" id="m0020">
<caption>
<title>Supplementary Table S4</title>
</caption>
<media xlink:href="mmc11.rtf"></media>
</supplementary-material>
</p>
<p id="p0440">
<supplementary-material content-type="local-data" id="m0015">
<caption>
<title>Supplementary Table S5</title>
</caption>
<media xlink:href="mmc12.rtf"></media>
</supplementary-material>
</p>
<p id="p0445">
<supplementary-material content-type="local-data" id="m0010">
<caption>
<title>Supplementary Table S6</title>
</caption>
<media xlink:href="mmc13.rtf"></media>
</supplementary-material>
</p>
<p id="p0450">
<supplementary-material content-type="local-data" id="m0005">
<caption>
<title>Supplementary Table S7</title>
</caption>
<media xlink:href="mmc14.rtf"></media>
</supplementary-material>
</p>
</sec>
<ack id="ak005">
<title>Acknowledgments</title>
<p>This work was supported in part by the <funding-source id="gp005">National Natural Science Foundation of China</funding-source> (Grant Nos. 61872216 and 81630103 to JZ, 81872915 to MWW, 81573479 and 81773792 to DY), the <funding-source id="gp015">National Science and Technology Major Project</funding-source> (Grant No. 2018ZX09711003-004-002 to LC), the National Science and Technology Major Project <funding-source id="gp020">Key New Drug Creation and Manufacturing Program</funding-source> of China (Grant Nos. 2018ZX09735-001 to MWW, 2018ZX09711002-002-005 to DY), and <funding-source id="gp025">Shanghai Science and Technology Development Fund</funding-source> (Grant Nos. 15DZ2291600 to MWW, 16ZR1407100 to AD). We acknowledge the support of the NVIDIA Corporation for the donation of the Titan X GPU used in this study.</p>
</ack>
<fn-group>
<fn id="d31e206">
<p id="np005">Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.</p>
</fn>
<fn fn-type="supplementary-material" id="s0160">
<p id="p0375">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" id="ir035" xlink:href="https://doi.org/10.1016/j.gpb.2019.04.003">https://doi.org/10.1016/j.gpb.2019.04.003</ext-link>.</p>
</fn>
</fn-group>
</back>
</article>
</pmc-articleset>