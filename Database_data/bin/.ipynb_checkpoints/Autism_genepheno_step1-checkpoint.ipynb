{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dirs():\n",
    "    ASDPTO_dir = '../source/ASDPTO.csv'\n",
    "    UMLS_dir = '../source/UMLS.txt'\n",
    "    allGene_dir = '../source/export_latest.tsv'\n",
    "    papers_dir = '../source/XML_datasets_5year/'\n",
    "    \n",
    "    out_dir = '../Autism_genepheno_results/'\n",
    "    \n",
    "    HPOtreeview_dir = 'HPO_treeview.txt'   # From https://raw.githubusercontent.com/obophenotype/human-phenotype-ontology/master/hp.obo'\n",
    "    return(ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_HPO_treeview():\n",
    "    import copy\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "    f = open('./HPO_treeview.txt', encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    HPO = text.split('\\n\\n[Term]\\n')[2:]\n",
    "    for i in range(len(HPO)):\n",
    "        HPO[i] = HPO[i].split('\\n')\n",
    "    HP_dic = {}\n",
    "    for item in HPO:\n",
    "        HPid = item[0][4:]\n",
    "        for line in item[1:]:\n",
    "            if 'name:' in line:\n",
    "                name = line[6:]\n",
    "                HP_dic[name] = HPid\n",
    "            elif 'synonym:' in line:\n",
    "                b = line.find('\"')\n",
    "                e= line[b+1:].find('\"')\n",
    "                syn = line[b+1:b+e+1]\n",
    "                HP_dic[syn] = HPid\n",
    "                \n",
    "    dic = {}\n",
    "    for item in HPO:\n",
    "        HPid = item[0][4:]\n",
    "        xref = 'NULL'\n",
    "        is_a_ls = []\n",
    "        for line in item[1:]:\n",
    "            if 'name:' in line:\n",
    "                name = [line[6:]]\n",
    "            elif 'xref: UMLS:' in line:\n",
    "                xref = line[11:]\n",
    "            elif 'is_a:' in line:\n",
    "                is_a_ls.append(line[6:16])\n",
    "            elif 'synonym:' in line:\n",
    "                b = line.find('\"')\n",
    "                e= line[b+1:].find('\"')\n",
    "                name.append(line[b+1:b+e+1])\n",
    "        name_ = list(set(name))\n",
    "        name_.sort(key = name.index)\n",
    "        dic[HPid] = {'name':name_, 'xref':xref, 'is_a':is_a_ls}\n",
    "        \n",
    "    upper_dic = {}\n",
    "    del_hierarchy_ls = ['All']\n",
    "\n",
    "    for key in dic:\n",
    "\n",
    "        ls = []\n",
    "        HP_ls = copy.deepcopy(dic[key]['is_a'])\n",
    "\n",
    "        if 'HP:0000001' in HP_ls:\n",
    "            HP_ls.remove('HP:0000001')\n",
    "        if 'HP:0000118' in HP_ls:\n",
    "            ls.append(dic[key]['name'][0])\n",
    "\n",
    "        while HP_ls:\n",
    "            HP = HP_ls.pop()\n",
    "            if 'HP:0000118' in dic[HP]['is_a']:\n",
    "                ls.append(dic[HP]['name'][0])\n",
    "            for HP_ in dic[HP]['is_a']:\n",
    "                if HP_ != 'HP:0000001':\n",
    "                    HP_ls.append(HP_)\n",
    "\n",
    "        if ls:\n",
    "            for name in dic[key]['name']:\n",
    "                upper_dic[name] = list(set(ls))\n",
    "        else:\n",
    "            for name in dic[key]['name']:\n",
    "                del_hierarchy_ls.append(name)\n",
    "                \n",
    "    return(upper_dic, HP_dic, del_hierarchy_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def XML_reader(f_path):\n",
    "    \n",
    "    ab_mark = 0\n",
    "\n",
    "    s_list = []\n",
    "    f_name = open(f_path, encoding=\"utf8\")\n",
    "    f = f_name.read()\n",
    "    f_name.close()\n",
    "    \n",
    "    #To get the pub date\n",
    "    h = f.find('<pub-date pub-type=\"epub\">')\n",
    "    if h == -1:\n",
    "        date = 'NULL'\n",
    "    else:\n",
    "        f_date = f[h:h+80]\n",
    "        h = f_date.find('<day>')\n",
    "        e = f_date.find('</day>')\n",
    "        date = '/' + f_date[h+5:e] + '/'\n",
    "        h = f_date.find('<month>')\n",
    "        e = f_date.find('</month>')\n",
    "        date = f_date[h+7:e] + date\n",
    "        h = f_date.find('<year>')\n",
    "        e = f_date.find('</year>')\n",
    "        date += f_date[h+6:e]\n",
    "    \n",
    "    #To get the title\n",
    "    h = f.find('<article-title>')\n",
    "    e = f.find('</article-title>')\n",
    "    title = f[h+15:e]\n",
    "    title = re.sub('<.*?>', '', title)\n",
    "    title += ' (Published on ' + date + ')'\n",
    "    \n",
    "    ab = ''\n",
    "    h = f.find('<abstract')\n",
    "    e = f.find('</abstract>')\n",
    "    ab = f[h:e]\n",
    "    \n",
    "    #To get the body\n",
    "    h = f.find('<body>')\n",
    "    e = f.find('</body>')\n",
    "    \n",
    "    if h == -1:\n",
    "        title += ' [Only abstract]'\n",
    "        \n",
    "    f = ab + ' ' + f[h:e]\n",
    "    \n",
    "    #Replacements for more accurate sentence breaks\n",
    "    f = f.replace('\\n', ' ')\n",
    "    f = f.replace('Fig. ', 'Fig.')\n",
    "    f = f.replace('i.e. ', 'i.e.')\n",
    "    \n",
    "    #To eliminate reference, table, alternatives\n",
    "    f = re.sub('<ref.*?/ref>', '', f)    \n",
    "    f = re.sub('<table.*?/table>', ' ', f)  \n",
    "    f = re.sub('<alternatives.*?/alternatives>', ' ', f) \n",
    "    \n",
    "    #To eliminate all '<*>'s\n",
    "    f = re.sub('<.*?>', '', f)\n",
    "    \n",
    "    #Sentence breaks using NLTK package\n",
    "    s_list = sent_tokenize(f)\n",
    "    \n",
    "    #To eliminate table/figure names\n",
    "    for i in range(len(s_list)):\n",
    "        sp = s_list[i].rfind('   ')\n",
    "        if sp != -1:\n",
    "            s_list[i] = s_list[i][sp:]\n",
    "        s_list[i] = s_list[i].strip()\n",
    "        \n",
    "    return(title,s_list)\n",
    "\n",
    "#The following function is to extract genotype and phenotype in a given sentence.\n",
    "import copy\n",
    "\n",
    "def gp_extraction(s, g_ls, p_dic):\n",
    "    \n",
    "    #s = re.sub('\\(.*?\\)', '', s)\n",
    "    s_tokens = re.findall(\"[A-Za-z0-9\\-]+\", s)\n",
    "    s_tokens_origin = copy.deepcopy(s_tokens)\n",
    "    \n",
    "    eg_list = []\n",
    "    for item in s_tokens:\n",
    "        if item in g_ls:\n",
    "            eg_list.append(item)\n",
    "    \n",
    "    if len(eg_list) == 0:\n",
    "        return(1,1,1)\n",
    "    \n",
    "    ep_list = []\n",
    "    op_list = []\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(s_tokens)):\n",
    "            if not s_tokens[i].isupper():\n",
    "                s_tokens[i] = lemmatizer.lemmatize(s_tokens[i].lower())\n",
    "    \n",
    "    s_tokens_str = str(s_tokens)\n",
    "\n",
    "    for key in p_dic:\n",
    "        if p_dic[key] in s_tokens_str:\n",
    "            ep_list.append(key)\n",
    "            \n",
    "            b = 0\n",
    "            e = 0\n",
    "            for i in range(4, s_tokens_str.find(p_dic[key])+2):\n",
    "                if s_tokens_str[i-4:i] == '\\', \\'':\n",
    "                    b += 1\n",
    "            for i in range(4, s_tokens_str.find(p_dic[key])+len(p_dic[key])):\n",
    "                if s_tokens_str[i-4:i] == '\\', \\'':\n",
    "                    e += 1\n",
    "            op = ''\n",
    "            for i in range(b, e+1):\n",
    "                op = op + s_tokens_origin[i] + ' '\n",
    "            op = op[:-1]\n",
    "            op_list.append(op)\n",
    "    \n",
    "    if len(ep_list) != 0:\n",
    "        eg_list = sorted(set(eg_list), key=eg_list.index)\n",
    "        op_list = sorted(set(op_list), key=op_list.index)\n",
    "\n",
    "        return(eg_list, ep_list, op_list)\n",
    "    \n",
    "    return(1,1,1)\n",
    "\n",
    "def get_phenotype_lists():\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "    \n",
    "    #Files reading(ASDPTO.csv).\n",
    "    import csv\n",
    "    ASDPTO_ls = []\n",
    "\n",
    "    with open(ASDPTO_dir, 'r') as f:\n",
    "        ASDPTO_file = csv.reader(f)\n",
    "\n",
    "        mk = 0\n",
    "        for row in ASDPTO_file:\n",
    "            if mk == 1:\n",
    "                ASDPTO_ls.append(row[1])\n",
    "            mk = 1\n",
    "            \n",
    "    # To read phenotype list generated from UMLS\n",
    "    UMLS = UMLS_dir\n",
    "    UMLS_ls = []\n",
    "    source_dic = {}\n",
    "    cid2c = {}\n",
    "    c2cid = {}\n",
    "    sSTT = {}\n",
    "\n",
    "    f_name = open(UMLS, encoding=\"utf8\")\n",
    "    f = f_name.read()\n",
    "    f_name.close()\n",
    "\n",
    "    f = f.split('\\n')\n",
    "    f = f[:-1]\n",
    "\n",
    "    all_upper = []\n",
    "\n",
    "    for line in f:\n",
    "        cid = line[1:9]\n",
    "        line = line[12:]\n",
    "        m0 = line.find('\"|')\n",
    "\n",
    "        concept = line[:m0]\n",
    "        if concept.isupper():\n",
    "            all_upper.append(concept)\n",
    "            continue\n",
    "\n",
    "        line = line[m0+3:]\n",
    "        m1 = line.find('\"|')\n",
    "        m2 = line.rfind('|\"')\n",
    "        source = line[:m1]\n",
    "        STT = line[m2+2:-1]\n",
    "\n",
    "        if concept in c2cid:\n",
    "            if cid not in c2cid[concept]:\n",
    "                c2cid[concept].append(cid)\n",
    "        else:\n",
    "            c2cid[concept] = []\n",
    "            c2cid[concept].append(cid)\n",
    "\n",
    "\n",
    "        if cid in cid2c:\n",
    "            cid2c[cid].append(concept)\n",
    "        else:\n",
    "            cid2c[cid] = []\n",
    "            cid2c[cid].append(concept)\n",
    "\n",
    "        if concept not in UMLS_ls:\n",
    "            UMLS_ls.append(concept)\n",
    "\n",
    "        if concept not in source_dic:\n",
    "            source_dic[concept] = source + ', '\n",
    "        else:\n",
    "            if (source + ', ') not in source_dic[concept]:\n",
    "                source_dic[concept] = source_dic[concept] + source + ', '\n",
    "\n",
    "        inf = cid+' from '+source+'('+STT+')'\n",
    "        if concept not in sSTT:\n",
    "            sSTT[concept] = []\n",
    "            sSTT[concept].append(inf)\n",
    "        else:\n",
    "            if (inf) not in sSTT[concept]:\n",
    "                sSTT[concept].append(inf)\n",
    "                \n",
    "    \n",
    "    pri = ['HPO(PF)', 'HPO', 'PF','']\n",
    "    u_c2cid = {}\n",
    "    c_best_source = {}\n",
    "    for c in sSTT:\n",
    "        for s in pri:\n",
    "            for item in sSTT[c]:\n",
    "                if s in item:\n",
    "                    u_c2cid[c] = item[:8]\n",
    "                    c_best_source[c] = item[14:]\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break  \n",
    "\n",
    "    #To combine UMLS and ASDPTO\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    merged_ls = UMLS_ls\n",
    "    for concept in ASDPTO_ls:\n",
    "        if concept not in merged_ls:\n",
    "            merged_ls.append(concept)\n",
    "            source_dic[concept] = 'ASDPTO' + ', '\n",
    "        else:\n",
    "            source_dic[concept] = source_dic[concept] + 'ASDPTO' + ', '\n",
    "\n",
    "    merged_ls = list(set(merged_ls))\n",
    "\n",
    "    '''\n",
    "    merged_ls = ['UMLS concept 1', 'UMLS concept 2', ..., 'ASDPTO concept 1', ...]\n",
    "    source_dic = {'Concept 1': 'Source A',\n",
    "                  'Concept 2': 'Source B, source C, ...',\n",
    "                   ...}\n",
    "    c2cid = {'Concept 1': ['CID 1'],\n",
    "             'Concept 2': ['CID 2', 'CID 3'],\n",
    "             ...}\n",
    "    cid2c = {'CID 1': ['Concept 1', 'Concept 2', ...],\n",
    "             'CID 2': ['Concept 5', 'Concept 6', ...],\n",
    "             ...}\n",
    "    '''\n",
    "\n",
    "    #To delete selected phenotypes and their synonyms\n",
    "    #del_ls: manually eliminated. del_ls_ASDPTO: high level words of ASDPTO. del_ls_HPO: top 2 level words of HPO\n",
    "\n",
    "    del_ls = ['Fitting', 'Sharing', 'Sharp', 'Smoking', 'Phenotypic variability', 'Pain', 'Spots', 'Planning', 'Syncope', 'Family history of cancer', \n",
    "              'Medical History', 'Social Interest', 'Illness', 'Falls', 'Fit', 'Painful', 'Blood spots', 'Autism Spectrum Disorders', 'Affect', 'Imbalance', \n",
    "              'Childhood autism', 'Severe', 'Signs and Symptoms', 'Blocking', 'Autoimmunity', 'Beta-EEG', 'Dependence', 'Imbalance', 'Diagnosis', 'Syncope', \n",
    "              'Social Interest', 'Autism', 'Exposures', 'Medications', 'Sporadic', 'High-functioning autism', 'Dissociation', 'Nonverbal', 'Ritual', \n",
    "              'Mannerism', 'Compulsion', 'Autistic behavior', 'Shock', 'Mood', 'Vocalizations', 'Behavioral Symptoms', 'Autism Phenotype', 'Eye Contact', \n",
    "              'Circling', 'Executive Function', 'Perinatal Exposures', 'Rigidity', 'Somatic mosaicism', 'Cognitive Ability', 'Self-Care', 'Family history', \n",
    "              'Withdrawal', 'Working Memory', 'Autism spectrum disorder', 'Clinical disease AND/OR syndrome', 'Syndrome', 'Nervous tension', \n",
    "              'Acquired deficiency', 'Symptom', 'Overlying', 'Ache', 'Adult onset', 'Does not worsen', 'IQ', 'Heterogeneity', 'Oxidative stress', \n",
    "              'Infection', 'Weakness']\n",
    "\n",
    "    del_ls_ASDPTO = ['Medical History', 'Comorbidities',  \n",
    "                    'Complaints and Indications', 'Neurologic Indications', 'Psychologic Indications', 'Diagnosis', \n",
    "                    'Primary Diagnosis', 'Exposures', 'Substance Abuse', 'Perinatal History', 'Perinatal Exposures', 'Personal Traits', 'Cognitive Ability', \n",
    "                    'Analytic Capability', 'Reasoning', 'Visual Perception', 'Emotional Traits', 'Affect', 'Mood', 'Executive Function', \n",
    "                    'Emotional Regulation and Control', 'Control of Emotional Reactions', 'Impulse Control and Regulation', 'Mental Flexibility', 'Planning', \n",
    "                    'Working Memory', 'Task Performance', 'Language Ability', 'Development or Regression of Language Skills', 'Expressive Language', \n",
    "                    'Expressive Phonology', 'Non-Verbal Communication', 'Receptive Language', 'Motor Skills', 'Stereotyped, Restricted, and Repetitive Behavior', \n",
    "                    'Adherence to Rituals and Routines', 'Restricted and Unusual Interests', \n",
    "                    'Social Competence', 'Adaptive Life Skills', 'Community Life Skills', 'Engagement in Social Activities', 'Home Life Skills', \n",
    "                    'Performance of Household Tasks', 'Self-Care', 'Interpersonal Interactions', 'Interpersonal Awareness', 'Reciprocal Social Interaction', \n",
    "                    'Social Interest', 'Initiating and Responding to Social Overtures', 'Interactions with Friends and Family', 'Recognition of Social Norms', \n",
    "                    'Awareness of Social Cues', 'Conversational Skills', 'Ability to Converse in Social Settings', 'Ability to Convey Information', \n",
    "                    'Understanding Context']\n",
    "\n",
    "    c2upper, HP_dic, del_ls_HPO = handle_HPO_treeview()\n",
    "\n",
    "    del_ls.extend(del_ls_ASDPTO)\n",
    "    del_ls.extend(del_ls_HPO)\n",
    "\n",
    "    for item in del_ls:\n",
    "\n",
    "        if item in c2cid:\n",
    "            for cid in c2cid[item]:\n",
    "                for c in cid2c[cid]:\n",
    "                    if c in merged_ls:\n",
    "                        merged_ls.remove(c) \n",
    "        else:\n",
    "            if item in merged_ls:\n",
    "                merged_ls.remove(item)\n",
    "\n",
    "    for key in source_dic:\n",
    "        source_dic[key] = source_dic[key][:-2]\n",
    "\n",
    "    #lemmatize the concepts \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_dic = {}\n",
    "    for concept in merged_ls:\n",
    "        concept_tokens = re.findall(\"[A-Za-z0-9\\-]+\", concept)\n",
    "        for i in range(len(concept_tokens)):\n",
    "            if not concept_tokens[i].isupper():\n",
    "                concept_tokens[i] = lemmatizer.lemmatize(concept_tokens[i].lower())\n",
    "        lemmatized_dic[concept] = str(concept_tokens)[1:-1]\n",
    "\n",
    "    '''\n",
    "    lemmatized_dic = {'Concept 1': \"Processed string 1\",\n",
    "                      'Concept 2': \"Processed string 2\", \n",
    "                      ...}\n",
    "    '''\n",
    "\n",
    "    #Phenotype normalization\n",
    "    inverted_lem_dic = {}\n",
    "    ulc_dic = {}\n",
    "    nc_dic = {}\n",
    "\n",
    "    for key in lemmatized_dic:\n",
    "        if lemmatized_dic[key] not in inverted_lem_dic:\n",
    "            inverted_lem_dic[lemmatized_dic[key]] = []\n",
    "        inverted_lem_dic[lemmatized_dic[key]].append(key)\n",
    "\n",
    "    # To build the dictionary for unique lemmatized concepts\n",
    "    for key in inverted_lem_dic:\n",
    "        for s in pri:\n",
    "            for c in sorted(inverted_lem_dic[key]):\n",
    "                if c in c_best_source:\n",
    "                    best_source_here = c_best_source[c]\n",
    "                else:\n",
    "                    best_source_here = 'ASDPTO'\n",
    "                if s in best_source_here:\n",
    "                    ulc_dic[c] = key\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # To build the dictionary for normalized concepts\n",
    "    for cid in cid2c:\n",
    "        if cid in cid2c:\n",
    "            for s in pri:\n",
    "                for c in sorted(cid2c[cid]):\n",
    "                    if s in c_best_source[c]:\n",
    "                        nc_dic[cid] = c\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break  \n",
    "            \n",
    "    return (ulc_dic, u_c2cid, nc_dic, HP_dic, source_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is to extract gene and phenotype from the given dataset (eg. papers in the folder 'XML_datasets_5year' here).\n",
    "\n",
    "def get_results(f_ls):\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "\n",
    "    #Files reading(export_latest.tsv). To generate genotype list.\n",
    "    import csv\n",
    "\n",
    "    g_ls = []\n",
    "    csv.register_dialect('tsv',delimiter='\\t',quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    with open(allGene_dir, 'r') as f:\n",
    "        g_file = csv.reader(f, 'tsv')    \n",
    "        m0 = 0\n",
    "        for row in g_file:\n",
    "            if m0 == 1:\n",
    "                g_ls.append(row[13])\n",
    "            m0 = 1\n",
    "\n",
    "    g_ls = list(set(g_ls))\n",
    "\n",
    "    # g_ls = ['genotype 1', 'genotype 2', 'genotype 3', ...]\n",
    "\n",
    "    ulc_dic, u_c2cid, nc_dic, HP_dic, source_dic = get_phenotype_lists()\n",
    "\n",
    "    import json\n",
    "    import ast\n",
    "\n",
    "    pid = 0\n",
    "\n",
    "\n",
    "    c2upper, HP_dic, del_ls_HPO = handle_HPO_treeview()\n",
    "\n",
    "    for f_path in f_ls:\n",
    "        PMCid = 'PMC' + f_path[-11:-4]\n",
    "        title, s_list = XML_reader(f_path)\n",
    "\n",
    "        n = 0 \n",
    "        dic = {}\n",
    "        dic_ = {}\n",
    "\n",
    "        for s in s_list:\n",
    "            eg_list, ep_list, op_list = gp_extraction(s, g_ls, ulc_dic)\n",
    "\n",
    "            if eg_list != 1:\n",
    "\n",
    "                his = set()\n",
    "                np_list = []\n",
    "                upper_list = []\n",
    "                for p in ep_list:\n",
    "                    if p in u_c2cid:\n",
    "                        cid = u_c2cid[p]\n",
    "                        nc = nc_dic[cid]\n",
    "                        if nc not in his:\n",
    "                            if nc in c2upper:\n",
    "                                upper_list.extend(c2upper[nc])\n",
    "                            HPid = 'NULL'\n",
    "                            if nc in HP_dic:\n",
    "                                HPid = HP_dic[nc]\n",
    "                            np_list.append([cid, nc, source_dic[nc_dic[cid]], HPid])\n",
    "                            his.add(nc)\n",
    "                    else:\n",
    "                        if p not in his:\n",
    "                            np_list.append(['ASDPTO', p, source_dic[p], 'NULL'])\n",
    "                            his.add(p)\n",
    "\n",
    "            #    upper_list = []\n",
    "            #    for p in ep_list:\n",
    "               #     if p in c2cid:\n",
    "                #        for cid in c2cid[p]:\n",
    "                  #          for c in cid2c[cid]:\n",
    "                    #            if c in c2upper:\n",
    "                     #               upper_list.extend(c2upper[c])\n",
    "\n",
    "\n",
    "            #    np_list = sorted(set(np_list), key=np_list.index)\n",
    "                upper_list = sorted(set(upper_list), key=upper_list.index)\n",
    "\n",
    "                ss = 'Sentence' + str(n)\n",
    "\n",
    "                if ss not in dic_:\n",
    "                    dic_[ss] = {}\n",
    "\n",
    "                dic_[ss]['Content'] = s\n",
    "                dic_[ss]['Gene'] = eg_list\n",
    "                dic_[ss]['Original phenotype'] = op_list\n",
    "                dic_[ss]['Normolized phenotype'] = np_list\n",
    "                dic_[ss]['Upper level concepts (HPO only)'] = upper_list\n",
    "\n",
    "                n += 1\n",
    "\n",
    "        if len(dic_) != 0:\n",
    "\n",
    "            dic['PMCid'] = PMCid\n",
    "            dic['Title'] = title\n",
    "            dic['Sentences'] = dic_\n",
    "\n",
    "            doc = open(out_dir + 'Extraced_results/'+ PMCid + '.json', 'w', encoding=\"utf8\")\n",
    "            print(json.dumps(dic, sort_keys=False, indent=4, separators=(', ', ': '), ensure_ascii=False), file = doc)\n",
    "            doc.close()\n",
    "\n",
    "        pid += 1\n",
    "        print('Doing extraction... Paper id:', pid, '; PMC id:', PMCid, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_for_each_paper(f_ls):\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "\n",
    "    import os\n",
    "\n",
    "    f_ls = []\n",
    "    path = papers_dir\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if '.xml' in f:\n",
    "                f_ls.append(f)\n",
    "\n",
    "    import os\n",
    "    import ast\n",
    "\n",
    "    results_ls = []\n",
    "    path = out_dir + 'Extraced_results/'\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if ('.json' in f) and ('PMC' in f):\n",
    "                results_ls.append(f)\n",
    "\n",
    "    ug_ls = []\n",
    "    up_ls = []\n",
    "    est_num = 0\n",
    "\n",
    "    for f_path in results_ls:\n",
    "        f_name = open(f_path, encoding=\"utf8\")\n",
    "        f = f_name.read()\n",
    "        f_name.close()\n",
    "        f = ast.literal_eval(f)\n",
    "        est_num += len(f['Sentences'])\n",
    "        for s in f['Sentences']:\n",
    "            ug_ls.extend(f['Sentences'][s]['Gene'])\n",
    "            for item in f['Sentences'][s]['Normolized phenotype']:\n",
    "                up_ls.append(item[1])\n",
    "\n",
    "    ug_ls = list(set(ug_ls))\n",
    "    up_ls = list(set(up_ls))\n",
    "\n",
    "    ulc_dic, u_c2cid, nc_dic, HP_dic, source_dic = get_phenotype_lists()\n",
    "\n",
    "    ulc0_dic = {}\n",
    "\n",
    "    for p in ulc_dic:\n",
    "        if p in u_c2cid:\n",
    "            cid = u_c2cid[p]\n",
    "            np = nc_dic[cid]\n",
    "        else:\n",
    "            np = p\n",
    "\n",
    "        if np in up_ls:\n",
    "            ulc0_dic[p] = ulc_dic[p]\n",
    "\n",
    "    import json\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    num = 0\n",
    "\n",
    "    for f_path in f_ls:\n",
    "        dic = {}\n",
    "        g_counts = {}\n",
    "        p_counts = {}\n",
    "        PMCid = 'PMC' + f_path[-11:-4]\n",
    "        title, s_list = XML_reader(f_path)\n",
    "\n",
    "        dic['PMCid'] = PMCid\n",
    "        if '[Only abstract]' in title:\n",
    "            dic['Only abstract?'] = 'Y'\n",
    "        else:\n",
    "            dic['Only abstract?'] = 'N'\n",
    "        dic['Number of Sentences'] = len(s_list)\n",
    "\n",
    "        for s in s_list:\n",
    "            s_tokens = re.findall(\"[A-Za-z0-9\\-]+\", s)\n",
    "\n",
    "            eg_ls = []\n",
    "            for token in s_tokens:\n",
    "                if token in ug_ls:\n",
    "                    eg_ls.append(token)\n",
    "            eg_ls = list(set(eg_ls))        \n",
    "            for g in eg_ls:\n",
    "                if g in g_counts:\n",
    "                    g_counts[g] += 1\n",
    "                else:\n",
    "                    g_counts[g] = 1\n",
    "\n",
    "            for i in range(len(s_tokens)):\n",
    "                    if not s_tokens[i].isupper():\n",
    "                        s_tokens[i] = lemmatizer.lemmatize(s_tokens[i].lower())\n",
    "\n",
    "            s_tokens_str = str(s_tokens)\n",
    "\n",
    "            ep_ls = []\n",
    "\n",
    "            for p in ulc0_dic:\n",
    "                if ulc0_dic[p] in s_tokens_str:\n",
    "                    ep_ls.append(p)\n",
    "            ep_ls = list(set(ep_ls))\n",
    "\n",
    "            for p in ep_ls:\n",
    "                if p in u_c2cid:\n",
    "                    cid = u_c2cid[p]\n",
    "                    nc = nc_dic[cid]\n",
    "                    HPid = 'NULL'\n",
    "                    if nc in HP_dic:\n",
    "                        HPid = HP_dic[nc]\n",
    "                  #  if (cid + ': ' + nc_dic[cid] + '(from ' + source_dic[nc_dic[cid]] + ')' not in up_ls):\n",
    "                  #      print('!!!!!!!!!',cid + ': ' + nc_dic[cid] + '(from ' + source_dic[nc_dic[cid]] + ')')\n",
    "\n",
    "                    if str([cid, nc, source_dic[nc_dic[cid]], HPid]) in p_counts: \n",
    "                        p_counts[str([cid, nc, source_dic[nc_dic[cid]], HPid])] += 1\n",
    "                    else:\n",
    "                        p_counts[str([cid, nc, source_dic[nc_dic[cid]], HPid])] = 1\n",
    "                else:\n",
    "                    if str(['ASDPTO', p, source_dic[p], 'NULL']) in p_counts:\n",
    "                        p_counts[str(['ASDPTO', p, source_dic[p], 'NULL'])] += 1\n",
    "                    else:\n",
    "                     #   if 'ASDPTO: ' + p + '(from ' + source_dic[p] + ')' not in up_ls:\n",
    "                     #       print('!!!','ASDPTO: ' + p + '(from ' + source_dic[p] + ')')\n",
    "                        p_counts[str(['ASDPTO', p, source_dic[p], 'NULL'])] = 1\n",
    "\n",
    "        dic['n_g'] = g_counts\n",
    "        dic['n_p'] = p_counts\n",
    "\n",
    "        doc = open(out_dir + 'Sum_for_each_paper/'+PMCid+'.txt', 'w', encoding=\"utf8\")\n",
    "        print(json.dumps(dic, sort_keys=False, indent=4, separators=(', ', ': '), ensure_ascii=False), file = doc)\n",
    "        doc.close()\n",
    "\n",
    "        num += 1\n",
    "        print('Summarizing each paper... Paper id:', num, '; PMC id:', PMCid, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_all():\n",
    "    \n",
    "    print('Getting the final summary...')\n",
    "    import os\n",
    "    import ast\n",
    "    import json\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "    f_ls = []\n",
    "    path = out_dir + 'Sum_for_each_paper/'\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            f_ls.append(f)\n",
    "\n",
    "    num = 0\n",
    "    ab_num = 0\n",
    "    N_totle = 0\n",
    "    u_g_ls = []\n",
    "    u_p_ls = []\n",
    "    n_g = {}\n",
    "    n_p = {}\n",
    "    for f_path in f_ls:\n",
    "        f_name = open(f_path, encoding=\"utf8\")\n",
    "        f = f_name.read()\n",
    "        f_name.close()\n",
    "        f = ast.literal_eval(f)\n",
    "        if f['Only abstract?'] == 'Y':\n",
    "            ab_num += 1\n",
    "        N_totle += f['Number of Sentences']\n",
    "        for g in f['n_g']:\n",
    "            u_g_ls.append(g)\n",
    "            if g in n_g:\n",
    "                n_g[g] += f['n_g'][g]\n",
    "            else:\n",
    "                n_g[g] = f['n_g'][g]\n",
    "        for p in f['n_p']:\n",
    "            u_p_ls.append(p)\n",
    "            if p in n_p:\n",
    "                n_p[p] += f['n_p'][p]\n",
    "            else:\n",
    "                n_p[p] = f['n_p'][p]\n",
    "        num += 1\n",
    "        print(num, f_path)\n",
    "    u_g_ls = list(set(u_g_ls))\n",
    "    u_p_ls = list(set(u_p_ls))\n",
    "\n",
    "    results_ls = []\n",
    "    path = out_dir + 'Extraced_results/'\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if ('.json' in f) and ('PMC' in f):\n",
    "                results_ls.append(f)\n",
    "\n",
    "    est_num = 0\n",
    "    for f_path in results_ls:\n",
    "        f_name = open(f_path, encoding=\"utf8\")\n",
    "        f = f_name.read()\n",
    "        f_name.close()\n",
    "        f = ast.literal_eval(f)\n",
    "        est_num += len(f['Sentences'])\n",
    "\n",
    "    sum_all_dir = out_dir + 'Sum_all/'\n",
    "    doc = open(sum_all_dir + 'In_Summary.txt', 'w')\n",
    "    print('Number of paper processed:', len(f_ls), file = doc)\n",
    "    print(file = doc)\n",
    "    print('Number of the articles have only abstract:', ab_num, file = doc)\n",
    "    print(file = doc)\n",
    "    print('Number of paper get at least one sentence:', len(results_ls), file = doc)\n",
    "    print(file = doc)\n",
    "    print('Sentences extracted:', est_num, file = doc)\n",
    "    print(file = doc)\n",
    "    print('N_tot = ', N_totle, file = doc)\n",
    "    print(file = doc)\n",
    "    #print('Phenotype source distribution:', counts, file = doc)\n",
    "    #print(file = doc)\n",
    "    print('Unique gene list from all papers:', u_g_ls, file = doc)\n",
    "    print(file = doc)\n",
    "    print('Unique normalized phenotype list from all papers:', u_p_ls, file = doc)\n",
    "    print(file = doc)\n",
    "    #print('Unique phenotype source distribution:', u_counts, file = doc)\n",
    "    #print(file = doc)\n",
    "    doc.close()\n",
    "\n",
    "    doc = open(sum_all_dir + 'n_g.txt', 'w')\n",
    "    print(n_g, file = doc)\n",
    "    doc.close()\n",
    "\n",
    "    doc = open(sum_all_dir + 'n_p.txt', 'w')\n",
    "    print(n_p, file = doc)\n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start/continue the extraction\n",
    "def find_breakpoint():\n",
    "    \n",
    "    ASDPTO_dir, UMLS_dir, HPOtreeview_dir, allGene_dir, papers_dir, out_dir = get_dirs()\n",
    "\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "    if not os.path.exists(out_dir + 'Extraced_results'):\n",
    "        os.mkdir(out_dir + 'Extraced_results')\n",
    "    if not os.path.exists(out_dir + 'Sum_for_each_paper'):\n",
    "        os.mkdir(out_dir + 'Sum_for_each_paper')\n",
    "    if not os.path.exists(out_dir + 'Sum_all'):\n",
    "        os.mkdir(out_dir + 'Sum_all')\n",
    "    \n",
    "    f_ls = []\n",
    "    path = papers_dir\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if '.xml' in f:\n",
    "                f_ls.append(f)\n",
    "\n",
    "    sum_ls = []\n",
    "    path = out_dir + 'Sum_for_each_paper/'\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if ('.json' in f) and ('PMC' in f):\n",
    "                results_ls.append(f)\n",
    "        \n",
    "    results_ls = []\n",
    "    path = out_dir + 'Extraced_results/'\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            f = os.path.join(root,file)\n",
    "            if ('.json' in f) and ('PMC' in f):\n",
    "                results_ls.append(f)\n",
    "                \n",
    "    if len(results_ls) == 0:\n",
    "        get_results(f_ls)\n",
    "        get_sum_for_each_paper(f_ls)\n",
    "        get_sum_all()\n",
    "    elif len(sum_ls) == 0: \n",
    "        x = 0\n",
    "        for item in results_ls:\n",
    "            loc = item.find('PMC') + 3\n",
    "            f = papers_dir + item[loc:-4] + 'xml'\n",
    "            x = max(x, f_ls.index(f))\n",
    "        get_results(f_ls[x:])\n",
    "        get_sum_for_each_paper(f_ls)\n",
    "        get_sum_all()\n",
    "    else:\n",
    "        x = 0\n",
    "        for item in sum_ls:\n",
    "            loc = item.find('PMC') + 3\n",
    "            f = papers_dir + item[loc:-4] + 'xml'\n",
    "            x = max(x, f_ls.index(f))\n",
    "        get_sum_for_each_paper(f_ls[x:])\n",
    "        get_sum_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-278dca43e777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_breakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e40cd8cdd7a4>\u001b[0m in \u001b[0;36mfind_breakpoint\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_ls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mget_sum_for_each_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mget_sum_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3c236737935d>\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(f_ls)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# g_ls = ['genotype 1', 'genotype 2', 'genotype 3', ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mulc_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_c2cid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHP_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_phenotype_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5ea7274af07e>\u001b[0m in \u001b[0;36mget_phenotype_lists\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcid2c\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mUMLS_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mUMLS_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "find_breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wangyasi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
